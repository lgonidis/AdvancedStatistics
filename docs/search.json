[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AdvancedStatistics",
    "section": "",
    "text": "Assignment 1\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nIntroduction to Advanced Statistics\n\n\n\n\n\n\n\n\n\n\n\n\nJan 28, 2025\n\n\nDr Lazaros Gonidis\n\n\n\n\n\n\n  \n\n\n\n\nCross-Lagged Panel Models\n\n\n\n\n\n\n\n\n\n\n\n\nApr 20, 2024\n\n\nDr Lazaros Gonidis\n\n\n\n\n\n\n  \n\n\n\n\nMeta-Analysis II: Subgroup Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2024\n\n\nDr Lazaros Gonidis\n\n\n\n\n\n\n  \n\n\n\n\nMeta-Analysis I\n\n\n\n\n\n\n\n\n\n\n\n\nMar 12, 2024\n\n\nDr Lazaros Gonidis\n\n\n\n\n\n\n  \n\n\n\n\nLatent Growth Models\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2024\n\n\nDr Lazaros Gonidis\n\n\n\n\n\n\n  \n\n\n\n\nConfirmatory Factor Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2024\n\n\nDr Lazaros Gonidis\n\n\n\n\n\n\n  \n\n\n\n\nExploratory Factor Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2024\n\n\nDr Lazaros Gonidis\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to S.E.M\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2024\n\n\nDr Lazaros Gonidis\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Mediation\n\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2024\n\n\nDr Lazaros Gonidis\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#todays-aims",
    "href": "posts/Introduction to Advanced Statistics/index.html#todays-aims",
    "title": "Introduction to Advanced Statistics",
    "section": "Today’s Aims",
    "text": "Today’s Aims\n\nCanvas\nAssessment\nQuizzes\nRstudio/Posit\nGithub"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#who-i-am",
    "href": "posts/Introduction to Advanced Statistics/index.html#who-i-am",
    "title": "Introduction to Advanced Statistics",
    "section": "Who I am",
    "text": "Who I am\n\n\n\n\n\n\n\nThis is me (used to be)\nI love my two boys\nI also love computer games, motorbikes, music, food\nFunnily enough I love maths and statistics, and even more teaching these\nI am also dyslexic so please do tell me when you spot typing mistakes, usually whole words missing."
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#essential-and-supplamentary-materials",
    "href": "posts/Introduction to Advanced Statistics/index.html#essential-and-supplamentary-materials",
    "title": "Introduction to Advanced Statistics",
    "section": "Essential and Supplamentary Materials",
    "text": "Essential and Supplamentary Materials\n\n\n\n\n\n\nPrinciples and Practice of Structural Equation Modeling\nLongitudinal Structural Equation Modeling: A comprehensive Introduction\nOfficial lavaan tutorial from lavaan.org\nPsychometrics in Exercises using R and Rstudio by Prof Anna Brown\nIntroduction to Structural Equation Modeling (SEM) in R with lavaan by Dr Johnny Lin\n** And many other online resources that will be revealed in due time"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#section",
    "href": "posts/Introduction to Advanced Statistics/index.html#section",
    "title": "Introduction to Advanced Statistics",
    "section": "",
    "text": "Multiple regression, lm()\n\n\nCode\nreading2_lm &lt;- lm(reading ~ income + books, data = df)\n\n\nsummary(reading2_lm)\n\n\n\nCall:\nlm(formula = reading ~ income + books, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-50.397 -13.633   0.547  13.698  51.585 \n\nCoefficients:\n             Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 187.28030    6.14063  30.499 &lt; 0.0000000000000002 ***\nincome        0.06160    0.01997   3.085              0.00265 ** \nbooks         6.14601    1.40272   4.382            0.0000298 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.69 on 97 degrees of freedom\nMultiple R-squared:  0.4487,    Adjusted R-squared:  0.4373 \nF-statistic: 39.47 on 2 and 97 DF,  p-value: 0.0000000000002875"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#todays-aims-in-terms-of-the-module",
    "href": "posts/Introduction to Advanced Statistics/index.html#todays-aims-in-terms-of-the-module",
    "title": "Introduction to Advanced Statistics",
    "section": "Today’s Aims in Terms of the Module",
    "text": "Today’s Aims in Terms of the Module\n\nCanvas\nRstudio/Posit\ndiscord\nAssessment\nQuizzes (Badges to be confirmed)\nGithub"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#todays-aims-in-terms-of-sem",
    "href": "posts/Introduction to Advanced Statistics/index.html#todays-aims-in-terms-of-sem",
    "title": "Introduction to Advanced Statistics",
    "section": "Today’s Aims in Terms of SEM",
    "text": "Today’s Aims in Terms of SEM\n\nIntroduce key terminology that we will be using this term\nDiscuss an example of simple regression\nRevisit the same example using lavaan\nDiscuss baximum likelihood vs. least squares"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#key-terminology",
    "href": "posts/Introduction to Advanced Statistics/index.html#key-terminology",
    "title": "Introduction to Advanced Statistics",
    "section": "Key Terminology",
    "text": "Key Terminology\nWe will be using quite a few terms in our module so it is important to define them in advance and try to use them consistently. Most of the them are used as in the field but you could come across some slight variations in terminology. This should not put you off or scare you, as long as you understand the substance of each term. It is also a good idea to try to learn the visual equivalents of these terms"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#key-terminology-2",
    "href": "posts/Introduction to Advanced Statistics/index.html#key-terminology-2",
    "title": "Introduction to Advanced Statistics",
    "section": "Key Terminology 2",
    "text": "Key Terminology 2\n\n\n\n\n\nlatent variable: a variable that is constructed/inferred indirectly by the data and does not exist in the data.\nobserved variable: a variable that has been measured and exists in our data.\nexogenous variable: an independent variable that explains an endogenous variable. Can be either observed \\((x)\\) or latent \\((ξ)\\).\nendogenous variable: a dependent variable that has a causal path leading to it. Can be either observed \\((y)\\) or latent \\((η)\\).\nmeasurement model: a model that links observed variables with latent variables"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#key-terminology-3",
    "href": "posts/Introduction to Advanced Statistics/index.html#key-terminology-3",
    "title": "Introduction to Advanced Statistics",
    "section": "Key Terminology 3",
    "text": "Key Terminology 3\n\n\n\n\n\nindicator: an observed variable in a measurement model (can be exogenous or endogenous).\nfactor: a latent variable defined by its indicators (can be exogenous or endogenous).\nloading: a path between an indicator and a factor.\nstructural model: a model that specifies causal relationships among exogenous variables to endogenous variables (can be observed or latent).\nregression path: a path between exogenous and endogenous variables (can be observed or latent)."
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#are-you-ready-for-the-first-headache-question",
    "href": "posts/Introduction to Advanced Statistics/index.html#are-you-ready-for-the-first-headache-question",
    "title": "Introduction to Advanced Statistics",
    "section": "Are you ready for the first headache question?",
    "text": "Are you ready for the first headache question?\nSo far, in linear regression we have learnt that \\(x\\) is an independent variable and \\(y\\) the dependent variable or outcome. However, in measurement models, the use of \\(x\\) or \\(y\\) depends on the type of factor we are referring to. If an indicator depends on an exogenous factor, the we refer to it as \\(x\\)-side. If an indicator depends on an endogenous factor then we refer to it as \\(y\\)-side."
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#let-us-expand-on-this-headache-a-bit-more",
    "href": "posts/Introduction to Advanced Statistics/index.html#let-us-expand-on-this-headache-a-bit-more",
    "title": "Introduction to Advanced Statistics",
    "section": "Let us expand on this headache a bit more",
    "text": "Let us expand on this headache a bit more"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#simple-regression",
    "href": "posts/Introduction to Advanced Statistics/index.html#simple-regression",
    "title": "Introduction to Advanced Statistics",
    "section": "Simple Regression",
    "text": "Simple Regression\nSo far we have learnt that a simple regression is the linear relation between a predictor (or an independent variable, or an observed exogenous variable) and an outcome (or observed endogenous variable).\n\\[\ny_1 = b_0 +b_1x_1 + ε_1\n\\]\nwhere \\(b_0\\) is the intercept and \\(b_1\\) is the coefficient of \\(x_1\\) (observed predictor) and \\(ε_1\\) is the residual with \\(y_1\\) being the outcome."
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#simple-regression-cont.",
    "href": "posts/Introduction to Advanced Statistics/index.html#simple-regression-cont.",
    "title": "Introduction to Advanced Statistics",
    "section": "Simple Regression cont.",
    "text": "Simple Regression cont.\nI strongly recommend reading Kline chapter 2, pages 25-30 at minimum, Regression Fundamentals.\n\\[\n\\hat{Y} = B_XX + A_X\n\\]\nThe above equation represents predicting Y from X\nAlso referred to as regressing Y on X, with \\(\\hat{Y}\\) representing predicted scores, \\(B_X\\) unstandardised regression coefficient for predictor \\(X\\), also known as slope, and \\(A_X\\) is the intercept.\nGenerally, with linear models we would use ordinary least squares (OLS) so that the least squares criterion is satisfied. In practice, we are trying to minimise the sum of squared residuals, \\(\\sum(Y-\\hat{Y})\\)"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#let-us-see-an-example-with-data",
    "href": "posts/Introduction to Advanced Statistics/index.html#let-us-see-an-example-with-data",
    "title": "Introduction to Advanced Statistics",
    "section": "Let us see an example with data",
    "text": "Let us see an example with data\nWe will work with the randomly generated data included in the random.csv\nThe datafile includes three variables:\nreading: reading ability as assessed in school\nincome: weekly family income in £\nbooks: number of books read in a month (on average)\nWe will create a simple regression model of reading  regressing on income using lm()"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#loading-the-datafile",
    "href": "posts/Introduction to Advanced Statistics/index.html#loading-the-datafile",
    "title": "Introduction to Advanced Statistics",
    "section": "Loading the datafile",
    "text": "Loading the datafile\n\n\nCode\nlibrary(tidyverse)\nlibrary(lavaan)\ndf &lt;- read_csv(\"random.csv\")"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#creating-the-model-and-getting-model-parameters",
    "href": "posts/Introduction to Advanced Statistics/index.html#creating-the-model-and-getting-model-parameters",
    "title": "Introduction to Advanced Statistics",
    "section": "Creating the model and getting model parameters",
    "text": "Creating the model and getting model parameters\n\n\nCode\nreading_lm &lt;- lm(reading ~ income, data = df)\n# the option below instructs R to give us the output in non-exponential notation\noptions(scipen=999)\n\nsummary(reading_lm)\n\n\n\nCall:\nlm(formula = reading ~ income, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-50.590 -16.562  -0.178  13.429  48.207 \n\nCoefficients:\n             Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 188.17787    6.68278  28.159 &lt; 0.0000000000000002 ***\nincome        0.11800    0.01662   7.098       0.000000000201 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.44 on 98 degrees of freedom\nMultiple R-squared:  0.3395,    Adjusted R-squared:  0.3328 \nF-statistic: 50.38 on 1 and 98 DF,  p-value: 0.0000000002013"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#looking-at-the-output",
    "href": "posts/Introduction to Advanced Statistics/index.html#looking-at-the-output",
    "title": "Introduction to Advanced Statistics",
    "section": "Looking at the output",
    "text": "Looking at the output\nOur intercept is 188.18 and our income coefficient is 0.118 (0.12). This means that the reading ability of a student with a family income of £0 will be 188.18 and for every £1 of family income increase the reading ability will increase by 0.12.\nWe also see residual standard error of 21.44\nThe square of that value is 459.67"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#let-us-recreate-the-same-model-using-lavaan",
    "href": "posts/Introduction to Advanced Statistics/index.html#let-us-recreate-the-same-model-using-lavaan",
    "title": "Introduction to Advanced Statistics",
    "section": "Let us recreate the same model using lavaan",
    "text": "Let us recreate the same model using lavaan\n\n\nCode\n# lavaan uses a simple language when specifying the model\n#simple regression using lavaan \nreading_lav &lt;-   '\n  # regressions\n    reading ~ 1 + income\n  # variance (optional)\n    income ~~ income\n'\nreading_lav_sem &lt;- sem(reading_lav, data=df)\nsummary(reading_lav_sem)\n\n\nlavaan 0.6.17 ended normally after 11 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         5\n\n  Number of observations                           100\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate   Std.Err  z-value  P(&gt;|z|)\n  reading ~                                            \n    income             0.118    0.016    7.170    0.000\n\nIntercepts:\n                   Estimate   Std.Err  z-value  P(&gt;|z|)\n   .reading          188.178    6.616   28.445    0.000\n    income           380.752   12.896   29.525    0.000\n\nVariances:\n                   Estimate   Std.Err  z-value  P(&gt;|z|)\n    income         16630.573 2351.918    7.071    0.000\n   .reading          450.401   63.696    7.071    0.000"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#let-us-recreate-the-same-model-using-lavaan-1",
    "href": "posts/Introduction to Advanced Statistics/index.html#let-us-recreate-the-same-model-using-lavaan-1",
    "title": "Introduction to Advanced Statistics",
    "section": "Let us recreate the same model using Lavaan",
    "text": "Let us recreate the same model using Lavaan\n\n\nCode\n# lavaan uses a simple language when specifying the model\n#simple regression using lavaan \nreading_lav &lt;-   '\n  # regressions\n    reading ~ 1 + income\n  # variance (optional)\n    income ~~ income\n'\nreading_lav_sem &lt;- sem(reading_lav, data=df)\nsummary(reading_lav_sem)\n\n\nlavaan 0.6.17 ended normally after 11 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         5\n\n  Number of observations                           100\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate      Std.Err  z-value  P(&gt;|z|)\n  reading ~                                               \n    income                0.002 NA                        \n\nIntercepts:\n                   Estimate      Std.Err  z-value  P(&gt;|z|)\n   .reading             188.180 NA                        \n    income            19037.628 NA                        \n\nVariances:\n                   Estimate      Std.Err  z-value  P(&gt;|z|)\n    income         41576275.335 NA                        \n   .reading             450.395 NA"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#comparing-the-two-outputs",
    "href": "posts/Introduction to Advanced Statistics/index.html#comparing-the-two-outputs",
    "title": "Introduction to Advanced Statistics",
    "section": "Comparing the two outputs",
    "text": "Comparing the two outputs\nWe observe that the estimates of the regression coefficients are the same despite lm() using least squares (LS) and lavaan using maximum likelihood (ML). However the variances are different with 459.67 for lm() and 450.40 for lavaan.\nIn we want to convert from LS variance to ML variance we can use the following formula\n\\[\n\\hat{σ}_{ML}^2 = \\frac{(N-k)}{n}\\hat{σ}_{LS}^2\n\\]\nWhere \\(N\\) and \\(n\\) are the sample sizes and \\(k\\) is the number or parameters to estimate, in this case \\(k\\)=2, one intercept and one regression coefficient\n\\[\n\\hat{σ}_{ML}^2 = \\frac{(100-2)}{100}21.44^2=450.48\n\\]"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#visualising",
    "href": "posts/Introduction to Advanced Statistics/index.html#visualising",
    "title": "Introduction to Advanced Statistics",
    "section": "Visualising",
    "text": "Visualising\n\n\nCode\nlibrary(semPlot)\n\nsemPaths(reading_lav_sem,\n         whatLabels = \"est\",\n         sizeMan = 10,\n         style = \"ram\",\n         layout = \"circle\")"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#multiple-regression",
    "href": "posts/Introduction to Advanced Statistics/index.html#multiple-regression",
    "title": "Introduction to Advanced Statistics",
    "section": "Multiple regression",
    "text": "Multiple regression\nWe can expand the previous example to now include a second predictor, books which represents the number of books read"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#visualising-with-sempaths-from-semplot",
    "href": "posts/Introduction to Advanced Statistics/index.html#visualising-with-sempaths-from-semplot",
    "title": "Introduction to Advanced Statistics",
    "section": "Visualising with semPaths() from semPlot",
    "text": "Visualising with semPaths() from semPlot\n\n\nCode\nlibrary(semPlot)\n\nsemPaths(reading_lav_sem,\n         whatLabels = \"est\",\n         sizeMan = 10,\n         style = \"ram\",\n         layout = \"circle\")"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#multiple-regression-lm",
    "href": "posts/Introduction to Advanced Statistics/index.html#multiple-regression-lm",
    "title": "Introduction to Advanced Statistics",
    "section": "Multiple regression, lm()",
    "text": "Multiple regression, lm()\nWe can expand the previous example to now include a second predictor, books which represents the number of books read in a month (on average)\n\\[\ny_1 = b_0 +b_1x_1 + b_2x_2 + ε_1\n\\]\nYou may also come across the following notation:\n\\[\n\\hat{Y} = B_XX + +B_WW + A_{X,W}\n\\]\nImportant to note here that \\(B_X\\) and \\(B_W\\) are the unstandardized partial regression coefficients, \\(A_{X,W}\\) is the intercept. For more information please read Kline page 30."
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#multiple-regression-lavaan",
    "href": "posts/Introduction to Advanced Statistics/index.html#multiple-regression-lavaan",
    "title": "Introduction to Advanced Statistics",
    "section": "Multiple regression, lavaan",
    "text": "Multiple regression, lavaan\n\n\nCode\nreading2_lav &lt;-   '\n  # regressions\n    reading ~ 1 + income + books\n  # covariance\n    income ~~ books\n'\nreading2_lav_sem &lt;- sem(reading2_lav, data=df)\nsummary(reading2_lav_sem)\n\n\nlavaan 0.6.17 ended normally after 38 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n\n  Number of observations                           100\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate   Std.Err  z-value  P(&gt;|z|)\n  reading ~                                            \n    income             0.062    0.020    3.132    0.002\n    books              6.146    1.382    4.449    0.000\n\nCovariances:\n                   Estimate   Std.Err  z-value  P(&gt;|z|)\n  income ~~                                            \n    books            152.610   28.168    5.418    0.000\n\nIntercepts:\n                   Estimate   Std.Err  z-value  P(&gt;|z|)\n   .reading          187.280    6.048   30.967    0.000\n    income           380.752   12.896   29.525    0.000\n    books              3.640    0.184   19.827    0.000\n\nVariances:\n                   Estimate   Std.Err  z-value  P(&gt;|z|)\n   .reading          375.988   53.173    7.071    0.000\n    income         16630.574 2351.918    7.071    0.000\n    books              3.370    0.477    7.071    0.000"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#visualising-with-sempaths-from-semplot-1",
    "href": "posts/Introduction to Advanced Statistics/index.html#visualising-with-sempaths-from-semplot-1",
    "title": "Introduction to Advanced Statistics",
    "section": "Visualising with semPaths() from semPlot",
    "text": "Visualising with semPaths() from semPlot\n\n\nCode\nsemPaths(reading2_lav_sem,\n         whatLabels = \"est\",\n         sizeMan = 10,\n         style = \"ram\",\n         layout = \"spring\")"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#canvas",
    "href": "posts/Introduction to Advanced Statistics/index.html#canvas",
    "title": "Introduction to Advanced Statistics",
    "section": "Canvas",
    "text": "Canvas\nHopefully you should know where to find our Canvas website, but just in case:\nhttps://canvas.sussex.ac.uk/courses/31432"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#rstudioposit-cloud",
    "href": "posts/Introduction to Advanced Statistics/index.html#rstudioposit-cloud",
    "title": "Introduction to Advanced Statistics",
    "section": "RStudio/Posit Cloud",
    "text": "RStudio/Posit Cloud\nFor our analysis we will be using exclusively R and mainly Posit Cloud. However, I strongly believe that as future scientists you should also have locally installed RStudio/Posit, have it up to date and use that for your everyday work. I would also advise you to attend our workshops bringing your own laptops/tablets/mobile phones.\nYou can join our Posit Cloud Workspace by following this link:\nhttps://posit.cloud/spaces/605745/content/all?sort=name_asc"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#discord-as-our-mean-of-day-to-day-communication",
    "href": "posts/Introduction to Advanced Statistics/index.html#discord-as-our-mean-of-day-to-day-communication",
    "title": "Introduction to Advanced Statistics",
    "section": "Discord as our mean of day to day communication",
    "text": "Discord as our mean of day to day communication\nWe will be using discord as our main communication channel, please use it for any stats related questions. If you have any more sensitive questions please do not hesitate to email me at: L.Gonidis@Sussex.ac.uk\nYou can join us by following this link: https://discord.gg/SxANsDEF"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#visualising-with-sempaths-from-semplot-2",
    "href": "posts/Introduction to Advanced Statistics/index.html#visualising-with-sempaths-from-semplot-2",
    "title": "Introduction to Advanced Statistics",
    "section": "Visualising with semPaths() from semPlot",
    "text": "Visualising with semPaths() from semPlot\n\n\nCode\nsemPaths(reading2_lav_sem,\n         whatLabels = \"est\",\n         sizeMan = 10,          \n         style = \"ram\",          \n         layout = \"tree\",\n         intercepts = FALSE)"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#a-prelude-to-next-week.-mediation-with-lavaan",
    "href": "posts/Introduction to Advanced Statistics/index.html#a-prelude-to-next-week.-mediation-with-lavaan",
    "title": "Introduction to Advanced Statistics",
    "section": "A prelude to next week. Mediation with Lavaan",
    "text": "A prelude to next week. Mediation with Lavaan\nWe will explore whether the number of books read actually mediate the effect of income on reading ability. We will not go in depth today as we will discuss this topic next week in detail. We will just demonstrate how the code would change in lavaan.\n\nreading_med_model &lt;- ' # direct effect\n             reading ~ c*income\n           # mediator\n             books ~ a*income\n             reading ~ b*books\n           # indirect effect (a*b)\n             ab := a*b\n           # total effect\n             total := c + (a*b)\n         '\nreading_med &lt;- sem(reading_med_model, data = df)\nsummary(reading_med )\n\nlavaan 0.6.17 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         5\n\n  Number of observations                           100\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate   Std.Err  z-value  P(&gt;|z|)\n  reading ~                                            \n    income     (c)     0.062    0.020    3.132    0.002\n  books ~                                              \n    income     (a)     0.009    0.001    8.431    0.000\n  reading ~                                            \n    books      (b)     6.146    1.382    4.449    0.000\n\nVariances:\n                   Estimate   Std.Err  z-value  P(&gt;|z|)\n   .reading          375.988   53.173    7.071    0.000\n   .books              1.970    0.279    7.071    0.000\n\nDefined Parameters:\n                   Estimate   Std.Err  z-value  P(&gt;|z|)\n    ab                 0.056    0.014    3.935    0.000\n    total              0.118    0.016    7.170    0.000"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#let-us-visualise-the-above-model",
    "href": "posts/Introduction to Advanced Statistics/index.html#let-us-visualise-the-above-model",
    "title": "Introduction to Advanced Statistics",
    "section": "Let us visualise the above model",
    "text": "Let us visualise the above model\n\nsemPaths(reading_med,\n         whatLabels = \"est\",\n         sizeMan = 10,          \n         style = \"ram\",          \n         layout = \"tree\",\n         intercepts = FALSE,\n         rotation = 2)"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#to-conclude",
    "href": "posts/Introduction to Advanced Statistics/index.html#to-conclude",
    "title": "Introduction to Advanced Statistics",
    "section": "To conclude",
    "text": "To conclude\nHopefully, this was a gentle introduction to the module and lavaan. We will be following a more formal approach in the coming weeks. In the meantime do spend some time this week to do the suggested reading and practice with R and lavaan in our Posit Cloud Workspace."
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#who-you-are",
    "href": "posts/Introduction to Advanced Statistics/index.html#who-you-are",
    "title": "Introduction to Advanced Statistics",
    "section": "Who you are",
    "text": "Who you are\nI have no idea who you are but I hope by the end of this module you will love statistics a bit more and you will appreciate even more why they are paramount for Psychological research."
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#assessment",
    "href": "posts/Introduction to Advanced Statistics/index.html#assessment",
    "title": "Introduction to Advanced Statistics",
    "section": "Assessment",
    "text": "Assessment\nThe module will be assessed exclusively by coursework. This will be in the form of five bi-weekly online canvas quizzes (40% total) and one 1,500 written report. The deadlines are:\n\nWritten Report 08/04/2025, 16:00 (submission on canvas)\nCanvas online quiz, week 2, deadline 14/02/2025, 10am.\nCanvas online quiz, week 4, deadline 28/02/2025, 10am.\nCanvas online quiz, week 6, deadline 14/03/2025, 10am.\nCanvas online quiz, week 8, deadline 28/03/2025, 10am.\nCanvas online quiz, week 10, deadline 11/04/2025, 10am.\n\nYou can find more information on the module assessment and feedback page on Canvas:\nhttps://canvas.sussex.ac.uk/courses/31432/pages/assessments-and-feedback?module_item_id=1371109"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#quizzes-and-badgeshopefully",
    "href": "posts/Introduction to Advanced Statistics/index.html#quizzes-and-badgeshopefully",
    "title": "Introduction to Advanced Statistics",
    "section": "Quizzes and Badges(hopefully)",
    "text": "Quizzes and Badges(hopefully)\nEvery week there will be a Canvas quiz provided so you can practice and enhance your understanding. These are absolutely non-compulsory but do spend some time trying to complete them. You will also be collecting points while you complete these (and other Canvas activities) and at the end of the module the two students with the highest scores will receive a surprise present."
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#github",
    "href": "posts/Introduction to Advanced Statistics/index.html#github",
    "title": "Introduction to Advanced Statistics",
    "section": "Github",
    "text": "Github\nGithub has been increasingly becoming a standard in the world of coding and statistical analysis as it can serve both as a repository and a host of websites related to projects. We will have a very brief demonstration today and we will return next week and start using it as a standard in our module."
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#todays-aims-in-terms-of-structural-equation-modeling-sem",
    "href": "posts/Introduction to Advanced Statistics/index.html#todays-aims-in-terms-of-structural-equation-modeling-sem",
    "title": "Introduction to Advanced Statistics",
    "section": "Today’s Aims in Terms of Structural Equation Modeling (SEM)",
    "text": "Today’s Aims in Terms of Structural Equation Modeling (SEM)\n\nIntroduce key terminology that we will be using this term\nDiscuss examples of simple and multiple regression\nRevisit the same examples using lavaan\nDiscuss maximum likelihood vs. least squares"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#ols-vs-ml",
    "href": "posts/Introduction to Advanced Statistics/index.html#ols-vs-ml",
    "title": "Introduction to Advanced Statistics",
    "section": "OLS vs ML",
    "text": "OLS vs ML\nGenerally in SEM, we use the maximum likelihood estimator (MLE). In this module we will be using the acronyms ML and MLE to denote the maximum likelihood estimator method. This method estimates model parameters by maximising the likelihood function. In other words, maximising the probability of observing our existing data points given specific parameter values. We will be discussing in details what these parameters are in SEM (e.g., coefficients, latent variable variances, etc.). It should also be noted that MLE is not the only estimation methods, other methods can also be successfully implemented, such as generalised least squares. We decided on the appropriate estimator based on our data characteristics and assumptions."
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#todays-aims",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#todays-aims",
    "title": "Introduction to Mediation",
    "section": "Today’s Aims",
    "text": "Today’s Aims\n\nIntroduce the concept of Mediation\nDiscuss a Simple Mediation Model\nUse of basic path analysis for Mediation\nUse of lavaan for mediation\nSerial and parallel mediation"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#linear-models",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#linear-models",
    "title": "Introduction to Mediation",
    "section": "Linear models",
    "text": "Linear models\nSo far we have discussed linear models where a variable Y regresses on X\n\\[\ny = b_0 +b_1x_1 + ε\n\\]\nOr, variable Y regresses on more than one variables, X1, X2, …\n\\[\ny = b_0 +b_1x_1 + b_2x_2 + ε_1\n\\]"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#linear-models-1",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#linear-models-1",
    "title": "Introduction to Mediation",
    "section": "Linear models",
    "text": "Linear models\nThese models allowed us to test hypotheses where exogenous observed variables X1, X2, … would have an effect on an endogenous observed variable Y\nHowever, in this approach we did not examine for potential impact of X1 on X2, and consequently the impact of X2 on Y."
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#the-simple-mediation-model",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#the-simple-mediation-model",
    "title": "Introduction to Mediation",
    "section": "The Simple Mediation Model",
    "text": "The Simple Mediation Model\nToday we will introduce a new concept where two predicting variables X and M predict an endogenous observed variable Y.\nFurthermore, variable X also influences variable M. We will be referring to variable M as our Mediator.\nIt is also worth noting here that different sources and approaches are even stricter on the above by also requesting that these relations are causal.\nIn other words, X causally influences M and Y, and M causally influences Y."
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#the-simple-mediation-model-visually",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#the-simple-mediation-model-visually",
    "title": "Introduction to Mediation",
    "section": "The Simple Mediation Model visually",
    "text": "The Simple Mediation Model visually"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#guidelines-prior-to-carry-out-mediation-analysis",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#guidelines-prior-to-carry-out-mediation-analysis",
    "title": "Introduction to Mediation",
    "section": "Guidelines prior to carry out mediation analysis",
    "text": "Guidelines prior to carry out mediation analysis\n\n\n\n\n\n\nX should significantly predict Y (path c, although some authors argue this is not required)*\nX must significantly predict M (path a)\nM must significantly predict Y (path b)\nNote: Typically the path above is called c path, whereas the direct path in the mediation would be called c’ path. However, you will mostly be seeing the direct path in the mediation models also being called c path as well. Please be aware of this."
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#quizzes-and-badgeshopefully",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#quizzes-and-badgeshopefully",
    "title": "Introduction to Mediation",
    "section": "Quizzes and Badges(hopefully)",
    "text": "Quizzes and Badges(hopefully)\nEvery week there will be a Canvas quiz provided so you can practice and enhance your understanding. These are absolutely non-compulsory but do spend some time trying to complete them. You will also be collecting points while you complete these (and other Canvas activities) and at the end of the module the two students with the highest scores will receive a surprise present."
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#github",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#github",
    "title": "Introduction to Mediation",
    "section": "Github",
    "text": "Github\nGithub has been increasingly becoming a standard in the world of coding and statistical analysis as it can serve both as a repository and a host of websites related to projects. We will have a very brief demonstration today and we will return next week and start using it as a standard in our module."
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#essential-and-supplamentary-materials",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#essential-and-supplamentary-materials",
    "title": "Introduction to Mediation",
    "section": "Essential and Supplamentary Materials",
    "text": "Essential and Supplamentary Materials\n\n\n\n\n\n\nPrinciples and Practice of Structural Equation Modeling\nLongitudinal Structural Equation Modeling: A comprehensive Introduction\nOfficial lavaan tutorial from lavaan.org\nPsychometrics in Exercises using R and Rstudio by Prof Anna Brown\nIntroduction to Structural Equation Modeling (SEM) in R with lavaan by Dr Johnny Lin\n** And many other online resources that will be revealed in due time"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#todays-aims-in-terms-of-structural-equation-modeling-sem",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#todays-aims-in-terms-of-structural-equation-modeling-sem",
    "title": "Introduction to Mediation",
    "section": "Today’s Aims in Terms of Structural Equation Modeling (SEM)",
    "text": "Today’s Aims in Terms of Structural Equation Modeling (SEM)\n\nIntroduce key terminology that we will be using this term\nDiscuss examples of simple and multiple regression\nRevisit the same examples using lavaan\nDiscuss baximum likelihood vs. least squares"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#key-terminology",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#key-terminology",
    "title": "Introduction to Mediation",
    "section": "Key Terminology",
    "text": "Key Terminology\nWe will be using quite a few terms in our module so it is important to define them in advance and try to use them consistently. Most of the them are used as in the field but you could come across some slight variations in terminology. This should not put you off or scare you, as long as you understand the substance of each term. It is also a good idea to try to learn the visual equivalents of these terms"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#key-terminology-2",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#key-terminology-2",
    "title": "Introduction to Mediation",
    "section": "Key Terminology 2",
    "text": "Key Terminology 2\n\n\n\n\n\nlatent variable: a variable that is constructed/inferred indirectly by the data and does not exist in the data.\nobserved variable: a variable that has been measured and exists in our data.\nexogenous variable: an independent variable that explains an endogenous variable. Can be either observed \\((x)\\) or latent \\((ξ)\\).\nendogenous variable: a dependent variable that has a causal path leading to it. Can be either observed \\((y)\\) or latent \\((η)\\).\nmeasurement model: a model that links observed variables with latent variables"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#key-terminology-3",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#key-terminology-3",
    "title": "Introduction to Mediation",
    "section": "Key Terminology 3",
    "text": "Key Terminology 3\n\n\n\n\n\nindicator: an observed variable in a measurement model (can be exogenous or endogenous).\nfactor: a latent variable defined by its indicators (can be exogenous or endogenous).\nloading: a path between an indicator and a factor.\nstructural model: a model that specifies causal relationships among exogenous variables to endogenous variables (can be observed or latent).\nregression path: a path between exogenous and endogenous variables (can be observed or latent)."
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#are-you-ready-for-the-first-headache-question",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#are-you-ready-for-the-first-headache-question",
    "title": "Introduction to Mediation",
    "section": "Are you ready for the first headache question?",
    "text": "Are you ready for the first headache question?\n\n\nCode\nplot_med &lt;- mark_sig(plot_med,model_med_fit)\nplot(plot_med)"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#let-us-expand-on-this-headache-a-bit-more",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#let-us-expand-on-this-headache-a-bit-more",
    "title": "Introduction to Mediation",
    "section": "Let us expand on this headache a bit more",
    "text": "Let us expand on this headache a bit more"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#simple-regression",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#simple-regression",
    "title": "Introduction to Mediation",
    "section": "Simple Regression",
    "text": "Simple Regression\nSo far we have learnt that a simple regression is the linear relation between a predictor (or an independent variable, or an observed exogenous variable) and an outcome (or observed endogenous variable).\n\\[\ny_1 = b_0 +b_1x_1 + ε_1\n\\]\nwhere \\(b_0\\) is the intercept and \\(b_1\\) is the coefficient of \\(x_1\\) (observed predictor) and \\(ε_1\\) is the residual with \\(y_1\\) being the outcome."
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#simple-regression-cont.",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#simple-regression-cont.",
    "title": "Introduction to Mediation",
    "section": "Simple Regression cont.",
    "text": "Simple Regression cont.\nI strongly recommend reading Kline chapter 2, pages 25-30 at minimum, Regression Fundamentals.\n\\[\n\\hat{Y} = B_XX + A_X\n\\]\nThe above equation represents predicting Y from X\nAlso referred to as regressing Y on X, with \\(\\hat{Y}\\) representing predicted scores, \\(B_X\\) unstandardised regression coefficient for predictor \\(X\\), also known as slope, and \\(A_X\\) is the intercept.\nGenerally, with linear models we would use ordinary least square (OLS) so that the least squares criterion is satisfied. In practice, we are trying to minimise the sum of squared residuals, \\(\\sum(Y-\\hat{Y})\\)"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#let-us-see-an-example-with-data",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#let-us-see-an-example-with-data",
    "title": "Introduction to Mediation",
    "section": "Let us see an example with data",
    "text": "Let us see an example with data\nWe will work with the randomly generated data included in the random.csv\nThe datafile includes three variables:\nreading: reading ability as assessed in school\nincome: weekly family income in £\nbooks: number of books read in a month (on average)\nWe will create a simple regression model of reading  regressing on income using lm()"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#let-us-explore-an-example-with-data",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#let-us-explore-an-example-with-data",
    "title": "Introduction to Mediation",
    "section": "Let us explore an example with data",
    "text": "Let us explore an example with data\nFirst we will create some random data. As we mentioned last week, in this module we will be working exclusively with randomly generated data. Please note that it will also be required to generate random data for your assignments too.\n\n\nCode\nset.seed(13548) #note that we set the seed to a specific value\nX &lt;- rnorm(250) #number of observations\nM &lt;- 0.60*X + rnorm(250) # what does the 0.4 and rnorm represent here?\nY &lt;- 0.35*M +  rnorm(250) # what does the 0.6 and rnorm represent here?\n\ndf &lt;- data.frame(X=X, Y=Y, M=M)"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#we-will-now-specify-and-fit-a-path-analysis-model",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#we-will-now-specify-and-fit-a-path-analysis-model",
    "title": "Introduction to Mediation",
    "section": "We will now specify and fit a path analysis model",
    "text": "We will now specify and fit a path analysis model\n\n\nCode\nlibrary(lavaan) \nlibrary(semPlot)  \nmodel2_med &lt;- ' \n# alternative syntax of the model            \nY ~ c*X + b1*M1 + b2*M2           \nM1 ~ a1*X              \nM2 ~ a2*X     \n\n# indirect effects (a*b)              \na1b1 := a1*b1\na2b2 := a2*b2\n# total effect              \ntotal := c + (a1*b1) + (a2*b2)          \n' \nmodel2_med_fit &lt;- sem(model2_med, data = df, se = 'bootstrap', bootstrap = 1000)"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#exploring-the-model",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#exploring-the-model",
    "title": "Introduction to Mediation",
    "section": "Exploring the model",
    "text": "Exploring the model\n\n\nCode\nsummary(model_med_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE, ci = TRUE)\n\n\nlavaan 0.6.17 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         5\n\n  Number of observations                           250\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nModel Test Baseline Model:\n\n  Test statistic                               113.139\n  Degrees of freedom                                 3\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.000\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)               -712.004\n  Loglikelihood unrestricted model (H1)       -712.004\n                                                      \n  Akaike (AIC)                                1434.009\n  Bayesian (BIC)                              1451.616\n  Sample-size adjusted Bayesian (SABIC)       1435.766\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: RMSEA &lt;= 0.050                       NA\n  P-value H_0: RMSEA &gt;= 0.080                       NA\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.000\n\nParameter Estimates:\n\n  Standard errors                            Bootstrap\n  Number of requested bootstrap draws             1000\n  Number of successful bootstrap draws            1000\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n  Y ~                                                                   \n    X          (c)   -0.065    0.086   -0.762    0.446   -0.241    0.097\n  M ~                                                                   \n    X          (a)    0.611    0.066    9.219    0.000    0.475    0.737\n  Y ~                                                                   \n    M          (b)    0.381    0.068    5.604    0.000    0.248    0.517\n   Std.lv  Std.all\n                  \n   -0.065   -0.058\n                  \n    0.611    0.507\n                  \n    0.381    0.406\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n   .Y                 1.018    0.086   11.783    0.000    0.839    1.185\n   .M                 1.002    0.083   12.063    0.000    0.843    1.175\n   Std.lv  Std.all\n    1.018    0.856\n    1.002    0.743\n\nR-Square:\n                   Estimate\n    Y                 0.144\n    M                 0.257\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n    ab                0.233    0.053    4.365    0.000    0.135    0.347\n    total             0.168    0.074    2.259    0.024    0.025    0.306\n   Std.lv  Std.all\n    0.233    0.206\n    0.168    0.148"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#first-we-will-investigate-the-three-simple-regressions",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#first-we-will-investigate-the-three-simple-regressions",
    "title": "Introduction to Mediation",
    "section": "First we will investigate the three simple regressions",
    "text": "First we will investigate the three simple regressions\n\n\nCode\nmodel_XY &lt;- lm(Y~X, data = df) # seen above as path c\nsummary(model_XY)\n\n\n\nCall:\nlm(formula = Y ~ X, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.5268 -0.7677 -0.0760  0.7206  3.5104 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  0.03976    0.06851   0.580   0.5622  \nX            0.16775    0.07119   2.356   0.0192 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.083 on 248 degrees of freedom\nMultiple R-squared:  0.0219,    Adjusted R-squared:  0.01796 \nF-statistic: 5.553 on 1 and 248 DF,  p-value: 0.01923"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#first-we-will-investigate-the-three-simple-regressions-1",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#first-we-will-investigate-the-three-simple-regressions-1",
    "title": "Introduction to Mediation",
    "section": "First we will investigate the three simple regressions",
    "text": "First we will investigate the three simple regressions\n\n\nCode\nmodel_XM &lt;- lm(M~X, data = df) # seen above as path a \nsummary(model_XM)\n\n\n\nCall:\nlm(formula = M ~ X, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.1693 -0.7082  0.0269  0.6571  3.2536 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.00179    0.06358  -0.028    0.978    \nX            0.61127    0.06606   9.253   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.005 on 248 degrees of freedom\nMultiple R-squared:  0.2566,    Adjusted R-squared:  0.2536 \nF-statistic: 85.61 on 1 and 248 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#first-we-will-investigate-the-three-simple-regressions-2",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#first-we-will-investigate-the-three-simple-regressions-2",
    "title": "Introduction to Mediation",
    "section": "First we will investigate the three simple regressions",
    "text": "First we will investigate the three simple regressions\n\n\nCode\nmodel_MY &lt;- lm(Y~M, data = df) # seen above as path b  \nsummary(model_MY)\n\n\n\nCall:\nlm(formula = Y ~ M, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.31772 -0.66815 -0.00959  0.65951  2.66509 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.03999    0.06416   0.623    0.534    \nM            0.35395    0.05525   6.406 7.46e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.015 on 248 degrees of freedom\nMultiple R-squared:  0.142, Adjusted R-squared:  0.1385 \nF-statistic: 41.03 on 1 and 248 DF,  p-value: 7.458e-10"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#first-we-will-investigate-path-c-regression",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#first-we-will-investigate-path-c-regression",
    "title": "Introduction to Mediation",
    "section": "First we will investigate path c regression",
    "text": "First we will investigate path c regression\n\n\nCode\nmodel_XY &lt;- lm(Y~X, data = df) # seen above as path c\nsummary(model_XY)\n\n\n\nCall:\nlm(formula = Y ~ X, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.5268 -0.7677 -0.0760  0.7206  3.5104 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  0.03976    0.06851   0.580   0.5622  \nX            0.16775    0.07119   2.356   0.0192 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.083 on 248 degrees of freedom\nMultiple R-squared:  0.0219,    Adjusted R-squared:  0.01796 \nF-statistic: 5.553 on 1 and 248 DF,  p-value: 0.01923"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#path-a-regression",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#path-a-regression",
    "title": "Introduction to Mediation",
    "section": "Path a regression",
    "text": "Path a regression\n\n\nCode\nmodel_XM &lt;- lm(M~X, data = df) # seen above as path a \nsummary(model_XM)\n\n\n\nCall:\nlm(formula = M ~ X, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.1693 -0.7082  0.0269  0.6571  3.2536 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.00179    0.06358  -0.028    0.978    \nX            0.61127    0.06606   9.253   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.005 on 248 degrees of freedom\nMultiple R-squared:  0.2566,    Adjusted R-squared:  0.2536 \nF-statistic: 85.61 on 1 and 248 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#path-b-regression",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#path-b-regression",
    "title": "Introduction to Mediation",
    "section": "Path b regression",
    "text": "Path b regression\n\n\nCode\nmodel_MY &lt;- lm(Y~M, data = df) # seen above as path b  \nsummary(model_MY)\n\n\n\nCall:\nlm(formula = Y ~ M, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.31772 -0.66815 -0.00959  0.65951  2.66509 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.03999    0.06416   0.623    0.534    \nM            0.35395    0.05525   6.406 7.46e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.015 on 248 degrees of freedom\nMultiple R-squared:  0.142, Adjusted R-squared:  0.1385 \nF-statistic: 41.03 on 1 and 248 DF,  p-value: 7.458e-10"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#visualising-the-path-analysis-mediation-model",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#visualising-the-path-analysis-mediation-model",
    "title": "Introduction to Mediation",
    "section": "Visualising the path analysis mediation model",
    "text": "Visualising the path analysis mediation model\n\n\nCode\nsemPaths(model_med_fit,\n         whatLabels = \"est\",\n         sizeMan = 10,          \n         style = \"ram\",          \n         layout = \"tree\",\n         intercepts = FALSE,\n         rotation = 2)"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#add-significance-stars",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#add-significance-stars",
    "title": "Introduction to Mediation",
    "section": "Add significance stars",
    "text": "Add significance stars\n\n\nCode\nlibrary(semptools)\nplot_med &lt;- semPaths(model_med_fit,\n         whatLabels = \"est\",\n         sizeMan = 10,          \n         style = \"ram\",          \n         layout = \"tree\",\n         intercepts = FALSE,\n         rotation = 2, edge.label.cex = 1)"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#how-can-we-interpret-the-above",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#how-can-we-interpret-the-above",
    "title": "Introduction to Mediation",
    "section": "How can we interpret the above?",
    "text": "How can we interpret the above?\nWe see that the paths a and b are significant. The same cannot be said for the path c. We also have a significant indirect effect and a significant total effect.\nIn this case our direct path c is no longer significant, hence this is a case of full mediation\nIf both the indirect path ab and the direct path c were significant this would have been a case of partial mediation.\nIf the indirect path was non-significant this would have been a case of no mediation."
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#we-can-also-extract-confidence-intervals",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#we-can-also-extract-confidence-intervals",
    "title": "Introduction to Mediation",
    "section": "We can also extract confidence intervals",
    "text": "We can also extract confidence intervals\n\n\nCode\nlavaan::standardizedsolution(model_med_fit)\n\n\n    lhs op     rhs label est.std    se      z pvalue ci.lower ci.upper\n1     Y  ~       X     c  -0.058 0.076 -0.760  0.447   -0.207    0.091\n2     M  ~       X     a   0.507 0.043 11.678  0.000    0.422    0.592\n3     Y  ~       M     b   0.406 0.074  5.497  0.000    0.261    0.551\n4     Y ~~       Y         0.856 0.046 18.675  0.000    0.766    0.945\n5     M ~~       M         0.743 0.044 16.914  0.000    0.657    0.830\n6     X ~~       X         1.000 0.000     NA     NA    1.000    1.000\n7    ab :=     a*b    ab   0.206 0.045  4.573  0.000    0.118    0.294\n8 total := c+(a*b) total   0.148 0.064  2.315  0.021    0.023    0.273"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#how-do-we-report-it",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#how-do-we-report-it",
    "title": "Introduction to Mediation",
    "section": "How do we report it?",
    "text": "How do we report it?\nNote that the following is just a guideline and was written based on the standardized estimates.\n‘Initial analysis indicated that X significantly predicted M and Y, and M significantly predicted Y. However, when M was incorporated as a mediator in the X and Y relationship a full mediation occurred where the indirect effect was significant (b = 0.23, p &lt; .001, z = 4.57, se = 0.21, ci[0.95] = (0.12 - 0.29), the total effect was also significant (b = 0.17, p &lt; .021, z= 2.32, se = 0.06, ci(0.95) = [0.02 - 0.27]. The direct path of X predicting y was no longer significant (b = 0.06, p = .447, z= -0.76, se = 0.08, ci(0.95) = [-0.21 - 0.09]. Both paths a (X-&gt;M) and b (M-&gt;Y) were significant, (b = 0.51, p &lt; .001, z = 11.68, se = 0.04, ci[0.95] = (0.42 - 0.59) and (b = 0.41, p &lt; .001, z = 5.50, se = 0.07, ci[0.95] = (0.26 - 0.55) respectively. This was in line with our hypothesis that the relationship between X and Y can be entirely explained by changes in M.’"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#parameter-estimates",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#parameter-estimates",
    "title": "Introduction to Mediation",
    "section": "Parameter Estimates",
    "text": "Parameter Estimates\n\n\nCode\nparameterEstimates(model_med_fit, level = .95)\n\n\n    lhs op     rhs label    est    se      z pvalue ci.lower ci.upper\n1     Y  ~       X     c -0.065 0.086 -0.762  0.446   -0.241    0.097\n2     M  ~       X     a  0.611 0.066  9.219  0.000    0.475    0.737\n3     Y  ~       M     b  0.381 0.068  5.604  0.000    0.248    0.517\n4     Y ~~       Y        1.018 0.086 11.783  0.000    0.839    1.185\n5     M ~~       M        1.002 0.083 12.063  0.000    0.843    1.175\n6     X ~~       X        0.926 0.000     NA     NA    0.926    0.926\n7    ab :=     a*b    ab  0.233 0.053  4.365  0.000    0.135    0.347\n8 total := c+(a*b) total  0.168 0.074  2.259  0.024    0.025    0.306"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#standardised-parameter-estimates",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#standardised-parameter-estimates",
    "title": "Introduction to Mediation",
    "section": "Standardised Parameter Estimates",
    "text": "Standardised Parameter Estimates\n\n\nCode\nlavaan::standardizedsolution(model_med_fit, level = .95)\n\n\n    lhs op     rhs label est.std    se      z pvalue ci.lower ci.upper\n1     Y  ~       X     c  -0.058 0.076 -0.760  0.447   -0.207    0.091\n2     M  ~       X     a   0.507 0.043 11.678  0.000    0.422    0.592\n3     Y  ~       M     b   0.406 0.074  5.497  0.000    0.261    0.551\n4     Y ~~       Y         0.856 0.046 18.675  0.000    0.766    0.945\n5     M ~~       M         0.743 0.044 16.914  0.000    0.657    0.830\n6     X ~~       X         1.000 0.000     NA     NA    1.000    1.000\n7    ab :=     a*b    ab   0.206 0.045  4.573  0.000    0.118    0.294\n8 total := c+(a*b) total   0.148 0.064  2.315  0.021    0.023    0.273"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#we-will-now-apply-what-we-learnt-with-a-more-complex-example",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#we-will-now-apply-what-we-learnt-with-a-more-complex-example",
    "title": "Introduction to Mediation",
    "section": "We will now apply what we learnt with a more complex example",
    "text": "We will now apply what we learnt with a more complex example\nPlease log on to your Posit account and access the Week2 project where you can work on an example of serial mediation with 2 mediators."
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#parallel-mediation",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#parallel-mediation",
    "title": "Introduction to Mediation",
    "section": "Parallel Mediation",
    "text": "Parallel Mediation"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#create-the-data",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#create-the-data",
    "title": "Introduction to Mediation",
    "section": "Create the data",
    "text": "Create the data\n\n\nCode\nset.seed(13548) #note that we set the seed to a specific value\nX &lt;- rnorm(250) #number of observations\nM1 &lt;- 0.60*X + rnorm(250)# what does the 0.4 and rnorm represent here?\nM2 &lt;- 0.50*X + rnorm(250)\nY &lt;- 0.35*M +  rnorm(250) # what does the 0.6 and rnorm represent here?\n\ndf &lt;- data.frame(X=X, Y=Y, M1=M1, M2=M2)"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#we-will-now-specify-and-fit-a-path-analysis-model-1",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#we-will-now-specify-and-fit-a-path-analysis-model-1",
    "title": "Introduction to Mediation",
    "section": "We will now specify and fit a path analysis model",
    "text": "We will now specify and fit a path analysis model\n\n\nCode\nlibrary(lavaan) \nlibrary(semPlot)  \nmodel2_med &lt;- ' \n# alternative syntax of the model            \nY ~ c*X + b1*M1 + b2*M2           \nM1 ~ a1*X              \nM2 ~ a2*X     \n\n# indirect effects (a*b)              \na1b1 := a1*b1\na2b2 := a2*b2\n# total effect              \ntotal := c + (a1*b1) + (a2*b2)          \n' \nmodel2_med_fit &lt;- sem(model2_med, data = df, se = 'bootstrap', bootstrap = 1000)"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#exploring-the-model-1",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#exploring-the-model-1",
    "title": "Introduction to Mediation",
    "section": "Exploring the model",
    "text": "Exploring the model\n\n\nCode\nsummary(model2_med_fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE, ci = TRUE)\n\n\nlavaan 0.6.17 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         8\n\n  Number of observations                           250\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.243\n  Degrees of freedom                                 1\n  P-value (Chi-square)                           0.622\n\nModel Test Baseline Model:\n\n  Test statistic                               140.934\n  Degrees of freedom                                 6\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.034\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -1076.137\n  Loglikelihood unrestricted model (H1)      -1076.016\n                                                      \n  Akaike (AIC)                                2168.275\n  Bayesian (BIC)                              2196.446\n  Sample-size adjusted Bayesian (SABIC)       2171.086\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.132\n  P-value H_0: RMSEA &lt;= 0.050                    0.717\n  P-value H_0: RMSEA &gt;= 0.080                    0.181\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.008\n\nParameter Estimates:\n\n  Standard errors                            Bootstrap\n  Number of requested bootstrap draws             1000\n  Number of successful bootstrap draws            1000\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n  Y ~                                                                   \n    X          (c)    0.003    0.090    0.035    0.972   -0.174    0.172\n    M1        (b1)    0.279    0.069    4.019    0.000    0.143    0.414\n    M2        (b2)    0.008    0.062    0.121    0.904   -0.113    0.132\n  M1 ~                                                                  \n    X         (a1)    0.611    0.063    9.718    0.000    0.477    0.731\n  M2 ~                                                                  \n    X         (a2)    0.454    0.067    6.736    0.000    0.320    0.593\n   Std.lv  Std.all\n                  \n    0.003    0.003\n    0.279    0.297\n    0.008    0.008\n                  \n    0.611    0.507\n                  \n    0.454    0.397\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n   .Y                 1.077    0.093   11.551    0.000    0.885    1.248\n   .M1                1.002    0.083   12.104    0.000    0.831    1.156\n   .M2                1.019    0.086   11.887    0.000    0.852    1.184\n   Std.lv  Std.all\n    1.077    0.910\n    1.002    0.743\n    1.019    0.842\n\nR-Square:\n                   Estimate\n    Y                 0.090\n    M1                0.257\n    M2                0.158\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|) ci.lower ci.upper\n    a1b1              0.170    0.045    3.815    0.000    0.086    0.259\n    a2b2              0.003    0.029    0.119    0.905   -0.052    0.062\n    total             0.177    0.079    2.227    0.026    0.011    0.336\n   Std.lv  Std.all\n    0.170    0.151\n    0.003    0.003\n    0.177    0.156"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#parameter-estimates-1",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#parameter-estimates-1",
    "title": "Introduction to Mediation",
    "section": "Parameter Estimates",
    "text": "Parameter Estimates\n\n\nCode\nparameterEstimates(model_med_fit, level = .95)\n\n\n    lhs op     rhs label    est    se      z pvalue ci.lower ci.upper\n1     Y  ~       X     c -0.065 0.086 -0.762  0.446   -0.241    0.097\n2     M  ~       X     a  0.611 0.066  9.219  0.000    0.475    0.737\n3     Y  ~       M     b  0.381 0.068  5.604  0.000    0.248    0.517\n4     Y ~~       Y        1.018 0.086 11.783  0.000    0.839    1.185\n5     M ~~       M        1.002 0.083 12.063  0.000    0.843    1.175\n6     X ~~       X        0.926 0.000     NA     NA    0.926    0.926\n7    ab :=     a*b    ab  0.233 0.053  4.365  0.000    0.135    0.347\n8 total := c+(a*b) total  0.168 0.074  2.259  0.024    0.025    0.306"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#visualisation-of-the-model",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#visualisation-of-the-model",
    "title": "Introduction to Mediation",
    "section": "Visualisation of the model",
    "text": "Visualisation of the model\n\n\nCode\nplot_med2 &lt;- semPaths(model2_med_fit,\n         whatLabels = \"est\",\n         sizeMan = 10,          \n         style = \"ram\",          \n         layout = \"tree\",\n         intercepts = FALSE,\n         rotation = 2, edge.label.cex = 1)"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#add-significance-stars-1",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#add-significance-stars-1",
    "title": "Introduction to Mediation",
    "section": "Add significance stars",
    "text": "Add significance stars\n\n\nCode\nplot_med2 &lt;- mark_sig(plot_med2,model2_med_fit)\nplot(plot_med2)"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#what-would-your-interpretation-be",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#what-would-your-interpretation-be",
    "title": "Introduction to Mediation",
    "section": "What would your interpretation be?",
    "text": "What would your interpretation be?"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#follow-up-topics",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#follow-up-topics",
    "title": "Introduction to Mediation",
    "section": "Follow-up topics",
    "text": "Follow-up topics\nThis Friday we are having our first Zoom session at 12 noon.\nThere we will be chatting about questions you might have but I also want you to explore 2 questions.\n\nHow can I add a variable in order to investigate for moderation effects between X and M?\nHow can I control for a specific variable in the simple mediation model? For example how can I control for the impact of age?"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html",
    "href": "posts/Introduction to Advanced Statistics/index.html",
    "title": "Introduction to Advanced Statistics",
    "section": "",
    "text": "This is me (used to be)\nI love my two boys\nI also love computer games, motorbikes, music, food\nFunnily enough I love maths and statistics, and even more teaching these\nI am also dyslexic so please do tell me when you spot typing mistakes, usually whole words missing."
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#first-we-will-investigate-the-c-path-in-regression",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#first-we-will-investigate-the-c-path-in-regression",
    "title": "Introduction to Mediation",
    "section": "First we will investigate the c path in regression",
    "text": "First we will investigate the c path in regression\n\n\nCode\nmodel_XY &lt;- lm(Y~X, data = df) # seen above as path c\nsummary(model_XY)\n\n\n\nCall:\nlm(formula = Y ~ X, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.5268 -0.7677 -0.0760  0.7206  3.5104 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  0.03976    0.06851   0.580   0.5622  \nX            0.16775    0.07119   2.356   0.0192 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.083 on 248 degrees of freedom\nMultiple R-squared:  0.0219,    Adjusted R-squared:  0.01796 \nF-statistic: 5.553 on 1 and 248 DF,  p-value: 0.01923"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#path-a-in-regression",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#path-a-in-regression",
    "title": "Introduction to Mediation",
    "section": "Path a in regression",
    "text": "Path a in regression\n\n\nCode\nmodel_XM &lt;- lm(M~X, data = df) # seen above as path a \nsummary(model_XM)\n\n\n\nCall:\nlm(formula = M ~ X, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.1693 -0.7082  0.0269  0.6571  3.2536 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.00179    0.06358  -0.028    0.978    \nX            0.61127    0.06606   9.253   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.005 on 248 degrees of freedom\nMultiple R-squared:  0.2566,    Adjusted R-squared:  0.2536 \nF-statistic: 85.61 on 1 and 248 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#path-b-in-regression",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#path-b-in-regression",
    "title": "Introduction to Mediation",
    "section": "Path b in regression",
    "text": "Path b in regression\n\n\nCode\nmodel_MY &lt;- lm(Y~M, data = df) # seen above as path b  \nsummary(model_MY)\n\n\n\nCall:\nlm(formula = Y ~ M, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.31772 -0.66815 -0.00959  0.65951  2.66509 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.03999    0.06416   0.623    0.534    \nM            0.35395    0.05525   6.406 7.46e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.015 on 248 degrees of freedom\nMultiple R-squared:  0.142, Adjusted R-squared:  0.1385 \nF-statistic: 41.03 on 1 and 248 DF,  p-value: 7.458e-10"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#the-simple-mediation-model-visually-1",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#the-simple-mediation-model-visually-1",
    "title": "Introduction to Mediation",
    "section": "The Simple Mediation Model visually",
    "text": "The Simple Mediation Model visually"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#we-will-now-specify-and-fit-a-path-analysis-mediaton-model",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#we-will-now-specify-and-fit-a-path-analysis-mediaton-model",
    "title": "Introduction to Mediation",
    "section": "We will now specify and fit a path analysis mediaton model",
    "text": "We will now specify and fit a path analysis mediaton model\n\n\nCode\nlibrary(lavaan)\nlibrary(semPlot)\n\nmodel_med &lt;- ' # direct effect\n             Y ~ c*X\n           # mediator\n             M ~ a*X\n             Y ~ b*M\n           # indirect effect (a*b)\n             ab := a*b\n           # total effect\n             total := c + (a*b)\n         '\nmodel_med_fit &lt;- sem(model_med, data = df, se = 'bootstrap', bootstrap = 1000, )"
  },
  {
    "objectID": "posts/Introduction to Mediation (with lavaan)/index.html#guidelines-prior-to-carry-out-mediation-analysis-1",
    "href": "posts/Introduction to Mediation (with lavaan)/index.html#guidelines-prior-to-carry-out-mediation-analysis-1",
    "title": "Introduction to Mediation",
    "section": "Guidelines prior to carry out mediation analysis",
    "text": "Guidelines prior to carry out mediation analysis\n\n\n\n\n\nmodel_med &lt;- ' # direct effect\n             Y ~ c*X\n           # mediator\n             M ~ a*X\n             Y ~ b*M\n           # indirect effect (a*b)\n             ab := a*b\n           # total effect\n             total := c + (a*b)\n         '"
  },
  {
    "objectID": "posts/Assignment 1/index.html",
    "href": "posts/Assignment 1/index.html",
    "title": "Assignment 1",
    "section": "",
    "text": "The following is a template of a study design where we randomly generated data to test our code. Once you run the code it will generate 5 variables, X, M1, M2, M3, and Y. Where X is our independent variable, M1, M2, M3 the three mediators, and Y our outcome. Feel free to rename the variables in this assignment but do not change the code that generates the data in terms of numbers (You will see that further down in this text).\nFor the purposes of this assignment we will treat the randomly generated data as if they were the real data. We hypothesise that our predictor X is fully mediated by a combination of serial and parallel mediations, where M1 and M2 mediate in sequence and M3 in parallel with M1 and M2. You can see this in the diagram below."
  },
  {
    "objectID": "posts/Assignment 1/index.html#study-design",
    "href": "posts/Assignment 1/index.html#study-design",
    "title": "Assignment 1",
    "section": "",
    "text": "The following is a template of a study design where we randomly generated data to test our code. Once you run the code it will generate 5 variables, X, M1, M2, M3, and Y. Where X is our independent variable, M1, M2, M3 the three mediators, and Y our outcome. Feel free to rename the variables in this assignment but do not change the code that generates the data in terms of numbers (You will see that further down in this text).\nFor the purposes of this assignment we will treat the randomly generated data as if they were the real data. We hypothesise that our predictor X is fully mediated by a combination of serial and parallel mediations, where M1 and M2 mediate in sequence and M3 in parallel with M1 and M2. You can see this in the diagram below."
  },
  {
    "objectID": "posts/Assignment 1/index.html#assignment",
    "href": "posts/Assignment 1/index.html#assignment",
    "title": "Assignment 1",
    "section": "Assignment",
    "text": "Assignment\nWrite the appropriate code that will model the above hypothesis in lavaan and run all required analyses. As we discussed in our workshop there are specific steps that need to be followed before we even run a mediation model. Make sure that you have followed all these steps prior to running a mediation analysis. Following your analysis evaluate whether our hypothesis can be supported, if not suggest and test a more appropriate model. Make sure to offer a justification on the course you chose to follow. This is the point where you can demonstrate a deeper understanding of mediation analysis and critical thinking will allow you to receive higher grades. You should complete your assignment in a quarto file, render it and save as a pdf. You should then upload that pdf file in Canvas. The deadline for this report is Tuesday 13th of February at 4pm."
  },
  {
    "objectID": "posts/Assignment 1/index.html#marking-criteria",
    "href": "posts/Assignment 1/index.html#marking-criteria",
    "title": "Assignment 1",
    "section": "Marking criteria",
    "text": "Marking criteria\nYour work will be evaluated based on the following criteria\n\nCorrect statistical analyses including descriptive statistics and linear models\nReporting all statistical figures according to the APA 7 guidelines\nYou should include at minimum one table and one graph, however more may be required based on the need for a follow up analysis\nYour writing should be clear and concise, try to avoid repetitive statements.\nAs an exception and purely for assessment purposes we expect you to report the mediation analyses both in text and in tables."
  },
  {
    "objectID": "posts/Assignment 1/index.html#generating-the-data",
    "href": "posts/Assignment 1/index.html#generating-the-data",
    "title": "Assignment 1",
    "section": "Generating the data",
    "text": "Generating the data\nMake sure to run the following code snipet as is in terms of numbers, any alterations may lead to errors and you will be marked down. You can however rename the variables (for example instead of X you may choose to declare Anxiety etc)\n\nset.seed(22335) #\nX &lt;- rnorm(250) \nM1 &lt;- 0.60*X + rnorm(250) \nM2 &lt;- 0.35*X + rnorm(250)\nM3 &lt;- 0.55*X + rnorm(250)\nY &lt;- 0.35*M1 + 0.1*M2 + 0.33*M3 + rnorm(250) \n\ndf &lt;- data.frame(X=X, M1=M1, M2=M2, M3=M3, Y=Y)"
  },
  {
    "objectID": "posts/Introduction to S.E.M/index.html#todays-aims",
    "href": "posts/Introduction to S.E.M/index.html#todays-aims",
    "title": "Introduction to S.E.M",
    "section": "Today’s Aims",
    "text": "Today’s Aims\nToday we will use Specification of observed variable models (Path) as means to introduce S.E.M.\nOverall, today’s session will be theoretical but we will also use coding to apply what will be discussed."
  },
  {
    "objectID": "posts/Introduction to S.E.M/index.html#steps-of-sem",
    "href": "posts/Introduction to S.E.M/index.html#steps-of-sem",
    "title": "Introduction to S.E.M.",
    "section": "Steps of SEM",
    "text": "Steps of SEM"
  },
  {
    "objectID": "posts/Introduction to S.E.M/index.html#basic-steps",
    "href": "posts/Introduction to S.E.M/index.html#basic-steps",
    "title": "Introduction to S.E.M",
    "section": "Basic Steps",
    "text": "Basic Steps\n\nBasic steps to SEM as proposed by Kline, page 118"
  },
  {
    "objectID": "posts/Introduction to S.E.M/index.html#specification",
    "href": "posts/Introduction to S.E.M/index.html#specification",
    "title": "Introduction to S.E.M",
    "section": "Specification",
    "text": "Specification\nSpecifying the model is probably the most important step in the process as it is assumed that the hypothesis, hence the model, is valid and correct.\nKline also suggests that alternative possible models could also be noted as potential models should they be justified by theory or data findings."
  },
  {
    "objectID": "posts/Introduction to S.E.M/index.html#identification",
    "href": "posts/Introduction to S.E.M/index.html#identification",
    "title": "Introduction to S.E.M",
    "section": "Identification",
    "text": "Identification\nSpecification of a model is usually a conceptual starting point and regardless of how well a model is grounded in theory, there will always be the need to be supported by statistical findings.\nWe therefore, build statistical models comprised of equations that define model parameters. These statistical parameters set and test relationships between our variables.\nA statistical model must be identified, meaning it is theoretically possible to algorithmically aarrive to a unique solution for each of the model parameters.\nImportant: Identification occurs regardless of our data and is a property of the model."
  },
  {
    "objectID": "posts/Introduction to S.E.M/index.html#an-oversimplified-example",
    "href": "posts/Introduction to S.E.M/index.html#an-oversimplified-example",
    "title": "Introduction to S.E.M",
    "section": "An oversimplified example",
    "text": "An oversimplified example\nIn school you might have come across the following simple example\n\\[\n3x + 4y =10\n\\]\n\\[\nx-y=1\n\\]\nThis is an example where we have two sources of information, in this case two equations, and we need to infer two unknown parameters, \\(x\\) and \\(y\\)."
  },
  {
    "objectID": "posts/Introduction to S.E.M/index.html#model-identification",
    "href": "posts/Introduction to S.E.M/index.html#model-identification",
    "title": "Introduction to S.E.M",
    "section": "Model Identification",
    "text": "Model Identification\n\nWhen the number of parameters specified in our model is equal to the number of unique sources of information then we have df = 0 and a just-identified model.\nWhen the number of parameters specified in our model is less than the number of unique sources of information then we have df &gt; 0 and an over-identified model.\nWhen the number of parameters specified in our model is greater then the number of unique sources of information then we have df &lt; 0 and an under-identified model.\n\ndf = number of unique sources of information - number of parameters specified"
  },
  {
    "objectID": "posts/Introduction to S.E.M/index.html#a-visual-example",
    "href": "posts/Introduction to S.E.M/index.html#a-visual-example",
    "title": "Introduction to S.E.M",
    "section": "A visual example",
    "text": "A visual example"
  },
  {
    "objectID": "posts/Introduction to S.E.M/index.html#model-identification-of-our-example",
    "href": "posts/Introduction to S.E.M/index.html#model-identification-of-our-example",
    "title": "Introduction to S.E.M",
    "section": "Model identification of our example",
    "text": "Model identification of our example\nWe have 5 variables so this is equivalent to 15 unique sources of information. You can visualise this if you imagine a triangular correlation matrix.\n\nTo make your life easier you can use this formula,\n\\(p*(p+1)/2\\)\nWhere \\(p\\) is the number of variables in your model."
  },
  {
    "objectID": "posts/Introduction to S.E.M/index.html#select-measures-and-collect-data",
    "href": "posts/Introduction to S.E.M/index.html#select-measures-and-collect-data",
    "title": "Introduction to S.E.M",
    "section": "Select measures and collect data",
    "text": "Select measures and collect data\nYour confidence in your statistical findings of your model will only be as good as your measurements and the quality of your data collection.\nIt is therefore vital to select good measures and follow solid methodological guidelines in terms of data collection and data analysis."
  },
  {
    "objectID": "posts/Introduction to S.E.M/index.html#estimation",
    "href": "posts/Introduction to S.E.M/index.html#estimation",
    "title": "Introduction to S.E.M",
    "section": "Estimation",
    "text": "Estimation\nUse of statistical software to carry out the SEM analysis and go through a cycle of the following steps:\n\nEvaluate model fit (only when df &gt; 0)\n\nChi-square Test \\((x^2)\\)\nComparative Fit Index (CFI)\nTucker-Lewis Index (TLI)\nRoot Mean Square Error of Approximation (RMSEA)\nStandardized Root Mean Square Residual (SRMR)"
  },
  {
    "objectID": "posts/Introduction to S.E.M/index.html#parameters-to-estimate",
    "href": "posts/Introduction to S.E.M/index.html#parameters-to-estimate",
    "title": "Introduction to S.E.M",
    "section": "Parameters to Estimate",
    "text": "Parameters to Estimate\n\nBeware of software default settings. You may not necessarily request an estimate but the software might calculate it by default. For example, the residual errors."
  },
  {
    "objectID": "posts/Introduction to S.E.M/index.html#model-identification-1",
    "href": "posts/Introduction to S.E.M/index.html#model-identification-1",
    "title": "Introduction to S.E.M",
    "section": "Model Identification",
    "text": "Model Identification\n15 unique sources of information - 12 parameters to estimate = 3 (df = 3)\nTherefore, this is an over-identified model!"
  },
  {
    "objectID": "posts/Introduction to S.E.M/index.html#chi-square-test-x2",
    "href": "posts/Introduction to S.E.M/index.html#chi-square-test-x2",
    "title": "Introduction to S.E.M",
    "section": "Chi-square Test \\((x^2)\\)",
    "text": "Chi-square Test \\((x^2)\\)\nThe chi-square test assesses whether our model fits the data with \\(p &lt; .05\\) indicating that the model does not fit the data well. Be aware of this as many students are fixated to significant p-values!\nAlso, as you might have heard multiple times before, this test is sensitive to sample size and does not behave well with non-normal distributions!"
  },
  {
    "objectID": "posts/Introduction to S.E.M/index.html#comparative-fit-index-cfi",
    "href": "posts/Introduction to S.E.M/index.html#comparative-fit-index-cfi",
    "title": "Introduction to S.E.M",
    "section": "Comparative Fit Index, CFI",
    "text": "Comparative Fit Index, CFI\nThis fit index is, as the naming strongly implies, a comparative fit index. This means it compares our current model to a baseline model, typically a null or empty model.\nWe tend to regard CFI values above .90 as good fit index for our model.\nCFI behaves better than the chi-square test with regards to sample size."
  },
  {
    "objectID": "posts/Introduction to S.E.M/index.html#tucker-lewis-index-tli",
    "href": "posts/Introduction to S.E.M/index.html#tucker-lewis-index-tli",
    "title": "Introduction to S.E.M",
    "section": "Tucker-Lewis Index (TLI)",
    "text": "Tucker-Lewis Index (TLI)\nAgain a comparative fit index where we compare our model to a null or empty model.\nIt is also, a better fit index compared to CFI when we have small sample sizes.\nWe tend to regard TLI values equal or greater than .95 as good fit index for our model, with .90 being considered as the absolute lowest acceptable value."
  },
  {
    "objectID": "posts/Introduction to S.E.M/index.html#root-mean-square-error-of-approximation-rmsea",
    "href": "posts/Introduction to S.E.M/index.html#root-mean-square-error-of-approximation-rmsea",
    "title": "Introduction to S.E.M",
    "section": "Root Mean Square Error of Approximation (RMSEA)",
    "text": "Root Mean Square Error of Approximation (RMSEA)\nContrary to the previous two, RMSEA is an absolute fit index.\nIt strongly favours more parsimonious models and heavily penalizes more complex models (keep that in mind).\nWe tend to regard RMSEA values equal or less than .08 as good fit index for our model, with .10 being considered as the absolute highest acceptable value.\nHeadache question: How would you expect the RMSEA value to behave in relation to df?"
  },
  {
    "objectID": "posts/Introduction to S.E.M/index.html#standardized-root-mean-square-residual-srmr",
    "href": "posts/Introduction to S.E.M/index.html#standardized-root-mean-square-residual-srmr",
    "title": "Introduction to S.E.M",
    "section": "Standardized Root Mean Square Residual (SRMR)",
    "text": "Standardized Root Mean Square Residual (SRMR)\nAlso an absolute fit index, with values less or equal to .08 indicating good fit."
  },
  {
    "objectID": "posts/Introduction to S.E.M/index.html#some-good-news-and-some-bad-news",
    "href": "posts/Introduction to S.E.M/index.html#some-good-news-and-some-bad-news",
    "title": "Introduction to S.E.M",
    "section": "Some good news and some bad news",
    "text": "Some good news and some bad news\nVery often you will have fit indices that contradict each other, for example CFI and TLI may be pointing towards good fit whereas RMSEA and SRMR may be pointing towards the opposite direction. What should you do?\nHowever, if all of the fit indices are saying that you do not have a good model fit then you should not proceed to interpret model parameters!\nInstead you should …"
  },
  {
    "objectID": "posts/Introduction to S.E.M/index.html#instead-you-should",
    "href": "posts/Introduction to S.E.M/index.html#instead-you-should",
    "title": "Introduction to S.E.M",
    "section": "Instead you should …",
    "text": "Instead you should …"
  },
  {
    "objectID": "posts/Introduction to S.E.M/index.html#respecify-or-report-results",
    "href": "posts/Introduction to S.E.M/index.html#respecify-or-report-results",
    "title": "Introduction to S.E.M",
    "section": "Respecify or report results?",
    "text": "Respecify or report results?\nKind of both. Report your initial results and if you have a solid justification of why you could respecify then go ahead and respecify. If you do not have that solid reasoning then mention that and finish with reporting the inadequate model fit!"
  },
  {
    "objectID": "posts/Introduction to S.E.M/index.html#my-model-fit-is-good",
    "href": "posts/Introduction to S.E.M/index.html#my-model-fit-is-good",
    "title": "Introduction to S.E.M",
    "section": "My model fit is good",
    "text": "My model fit is good\nMy model fit indices indicated a good fit, should I just report my findings?\nKline suggests that researchers should also consider equivalent or near-equivalent models, demonstrating that their initial model is a better model than the near equivalent ones.\nIt will very often be the case that these models may have better fit than the initial model so it is the researcher’s duty to argue why these need to be rejected at the expense of the proposed model!"
  },
  {
    "objectID": "posts/Introduction to S.E.M/index.html#reporting-sem-analysis",
    "href": "posts/Introduction to S.E.M/index.html#reporting-sem-analysis",
    "title": "Introduction to S.E.M",
    "section": "Reporting SEM analysis",
    "text": "Reporting SEM analysis\nIt is vital that all analyses steps are fully disclosed and all statistical figures are accurately reported. It should also be explicitly stated how all the above steps were taken and how the proposed initial/final model is a better model compared to equivalent ones!"
  },
  {
    "objectID": "posts/Introduction to S.E.M/index.html#follow-up-reading",
    "href": "posts/Introduction to S.E.M/index.html#follow-up-reading",
    "title": "Introduction to S.E.M",
    "section": "Follow-up reading",
    "text": "Follow-up reading\nI strongly advise you to finish reading chapter 6, pages 117 to 142, Principles and Practice of Structural Equation Modeling, Fourth Edition."
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#todays-aims",
    "href": "posts/Exploratory Factor Analysis/index.html#todays-aims",
    "title": "Exploratory Factor Analysis",
    "section": "Today’s Aims",
    "text": "Today’s Aims\nToday we will define what exploratory factor analysis (EFA) is, we focus on the steps required to carry out EFA using R."
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#what-is-factor-analysis",
    "href": "posts/Exploratory Factor Analysis/index.html#what-is-factor-analysis",
    "title": "Exploratory Factor Analysis",
    "section": "What is Factor Analysis",
    "text": "What is Factor Analysis\nFactor Analysis in the statistical technique that seeks to identify underlying relationships between observed variables.\nSpecifically, grouping these variables into groups where in-group variables correlate highly.\nIdeally, we want variables to correlate highly only with their in-group variables, and correlate weakly or not at all with variables belonging to other groups.\nWe will be referring to the term group as factor. There factors are unobserved variables…"
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#what-is-exploratory-factor-analysis",
    "href": "posts/Exploratory Factor Analysis/index.html#what-is-exploratory-factor-analysis",
    "title": "Exploratory Factor Analysis",
    "section": "What is Exploratory Factor Analysis?",
    "text": "What is Exploratory Factor Analysis?\nIt is a statistical exploratory process that seeks to identify underlying relationships between observed variables.\nFurthermore, with EFA we aim to identify latent variables that might are responsible for the shared variances between the observed variables. As mentioned in our introduction lectures latent variables are variables that are not directly measured. Instead, they are inferred by the existing relationships between our observed variables."
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#specification-of-models-in-efa",
    "href": "posts/Exploratory Factor Analysis/index.html#specification-of-models-in-efa",
    "title": "Exploratory Factor Analysis",
    "section": "Specification of models in EFA",
    "text": "Specification of models in EFA\nAccording to Kline (pages 190-191):\n\nEFA does not require an a priori specification, with number of possible factors varying from one up to as many as the indicators. (Highly not advised, but theoretically possible)\nIn EFA we have unrestricted measurement models where indicators are allowed to depend on all factors\nMultiple factors models in EFA are not actually identified. Headache question: Why would that be the case?\nIn EFA we assume that specific variance of each indicator is not shared what that of any other indicator\n\nNote: Next week we will contrast all the above points with Confirmatory Factor Analysis (CFA)"
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#visualisation",
    "href": "posts/Exploratory Factor Analysis/index.html#visualisation",
    "title": "Exploratory Factor Analysis",
    "section": "Visualisation",
    "text": "Visualisation"
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#do-we-analyze-correlated-factors",
    "href": "posts/Exploratory Factor Analysis/index.html#do-we-analyze-correlated-factors",
    "title": "Exploratory Factor Analysis",
    "section": "Do we analyze correlated Factors?",
    "text": "Do we analyze correlated Factors?\nTypically this is not required. However, we can specify a rotation that will allow us to analyze correlated factors!\nRotation?\nRotation allows us to simplify our model further, thus enhance its interpretation. This is an option that is applied after our initial solution and its aim is to achieve a solution where an indicator has high loading to one factor (or as few as possible) and low loading on all other factors."
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#rotations",
    "href": "posts/Exploratory Factor Analysis/index.html#rotations",
    "title": "Exploratory Factor Analysis",
    "section": "Rotations",
    "text": "Rotations\n\nOrthogonal rotation, usually the default setting for most EFA functions, treats all factors are non-correlated. The most commonly used is Varimax, however there are others. Be cautious when using orthogonal rotations, refer back to your theoretical background in order to make sure that your possible factors are indeed expected to be uncorrelated.\nOblique rotation, allows for correlated factors. The most commonly used is Promax, however there are others.\n\nSo which one should we use? Outside of the correlated or uncorrelated allowance it is difficult to decide. Many different rotation methods may give similarly valid results. I advise you to look into the factor score indeterminacy for more details."
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#software-considerations",
    "href": "posts/Exploratory Factor Analysis/index.html#software-considerations",
    "title": "Exploratory Factor Analysis",
    "section": "Software Considerations",
    "text": "Software Considerations\nCan we use lavaan to carry our EFA?\nYes, however …… psych package might make your life easier.\nEFAtools, will definitely make your life easier.\nWe will use all three in combination in the coming examples and you can decide on your own."
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#can-we-always-carry-out-efa",
    "href": "posts/Exploratory Factor Analysis/index.html#can-we-always-carry-out-efa",
    "title": "Exploratory Factor Analysis",
    "section": "Can we always carry out EFA?",
    "text": "Can we always carry out EFA?\nSome researchers argue that you could. You can definitely try, in terms of coding and running the relevant software.\nHowever, you shouldn’t if you do not meet the following two criteria (at minimum).\nKMO: Kaiser-Meyer-Olkin measure of sampling adequacy: Evaluates whether our sample is suitable for factor analysis.It does so by evaluating the proportion of variance among variable that could be attributed to underlying factors. Ranges from 0 - 1, and values closer to 1 indicate higher suitability.\nBartlett’s test of Sphericity: Assesses whether our variables/indicators have significant correlations. If our correlations are non-significant then we should not proceed. Here we are looking for evidence (p &lt; .05) in order to reject the null hypothesis that our inter-variable correlations are zero."
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#other-useful-terminology",
    "href": "posts/Exploratory Factor Analysis/index.html#other-useful-terminology",
    "title": "Exploratory Factor Analysis",
    "section": "Other Useful Terminology",
    "text": "Other Useful Terminology\n\nCommunality: The proportion of variance explained by the common factor. This will be used as a decision criterion to include or exclude indicators to a factor.\nPercentage of Variance: The percentage of variance that is due to one factor in relation to the total variance in all factors.\nEigenvalue: The total variance explained by each factor, we are ideally looking for eigenvalues above 1."
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#let-us-work-through-an-example-together",
    "href": "posts/Exploratory Factor Analysis/index.html#let-us-work-through-an-example-together",
    "title": "Exploratory Factor Analysis",
    "section": "Let us work through an example together",
    "text": "Let us work through an example together\nFirst, let’s create the random data.\n\n\nCode\nset.seed(1212)\n### normally distributed factors\n### these are just to help me set the indicators\n### the f1 and f2 will not be included in the data.frame\nf1 &lt;- rnorm(250)\nf2 &lt;- rnorm(250)\n\n### f1 indicators x1 to x3\nx1 &lt;- f1 + rnorm(250, sd=0.15)\nx2 &lt;- f1 + rnorm(250, sd=0.15)\nx3 &lt;- f1 + rnorm(250, sd=0.15)\n\n### f2 indicators x4 to x6\nx4 &lt;- f2 + rnorm(250, sd=0.15)\nx5 &lt;- f2 + rnorm(250, sd=0.15)\nx6 &lt;- f2 + rnorm(250, sd=0.15)\n\n### creating the dataframe\ndf &lt;- data.frame(x1=x1, x2=x2, x3=x3, x4=x4, x5=x5, x6=x6)"
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#assess-kmo-and-bartletts-test-of-sphericity",
    "href": "posts/Exploratory Factor Analysis/index.html#assess-kmo-and-bartletts-test-of-sphericity",
    "title": "Exploratory Factor Analysis",
    "section": "Assess KMO and Bartlett’s test of Sphericity",
    "text": "Assess KMO and Bartlett’s test of Sphericity\nEFAtools\n\n\nCode\nlibrary(psych)\nlibrary(EFAtools)\n\nKMO(df) ### THIS IS NOW MASKED BY EFAtools\n\n\n\n── Kaiser-Meyer-Olkin criterion (KMO) ──────────────────────────────────────────\n\n✔ The overall KMO value for your data is middling.\n  These data are probably suitable for factor analysis.\n\n  Overall: 0.795\n\n  For each variable:\n   x1    x2    x3    x4    x5    x6 \n0.790 0.775 0.819 0.789 0.793 0.807 \n\n\nCode\nBARTLETT(df) ### THIS IS NOW MASKED BY EFAtools\n\n\n\n✔ The Bartlett's test of sphericity was significant at an alpha level of .05.\n  These data are probably suitable for factor analysis.\n\n  𝜒²(15) = 3272.23, p &lt; .001"
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#assess-kmo-and-bartletts-test-of-sphericity-1",
    "href": "posts/Exploratory Factor Analysis/index.html#assess-kmo-and-bartletts-test-of-sphericity-1",
    "title": "Exploratory Factor Analysis",
    "section": "Assess KMO and Bartlett’s test of Sphericity",
    "text": "Assess KMO and Bartlett’s test of Sphericity\npsych\n\n\nCode\npsych::KMO(df)\n\n\nKaiser-Meyer-Olkin factor adequacy\nCall: psych::KMO(r = df)\nOverall MSA =  0.8\nMSA for each item = \n  x1   x2   x3   x4   x5   x6 \n0.79 0.77 0.82 0.79 0.79 0.81 \n\n\nCode\nr &lt;- cor(df)\npsych::cortest.bartlett(r)\n\n\n$chisq\n[1] 1278.319\n\n$p.value\n[1] 2.428264e-263\n\n$df\n[1] 15"
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#determining-number-of-factors",
    "href": "posts/Exploratory Factor Analysis/index.html#determining-number-of-factors",
    "title": "Exploratory Factor Analysis",
    "section": "Determining number of factors",
    "text": "Determining number of factors\nEFAtools\n\n\nCode\nPARALLEL(df, eigen_type = \"PCA\")\n\n\nParallel Analysis performed using 1000 simulated random data sets\nEigenvalues were found using PCA\n\nDecision rule used: means\n\n── Number of factors to retain according to ────────────────────────────────────\n\n◌ PCA-determined eigenvalues:  2"
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#determining-number-of-factors-1",
    "href": "posts/Exploratory Factor Analysis/index.html#determining-number-of-factors-1",
    "title": "Exploratory Factor Analysis",
    "section": "Determining number of factors",
    "text": "Determining number of factors\npsych\n\n\nCode\nfa.parallel(df, fa=\"pc\")\n\n\n\nParallel analysis suggests that the number of factors =  NA  and the number of components =  2"
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#in-efatools-you-can-also-run-multiple-retention-methods",
    "href": "posts/Exploratory Factor Analysis/index.html#in-efatools-you-can-also-run-multiple-retention-methods",
    "title": "Exploratory Factor Analysis",
    "section": "In EFAtools you can also run multiple retention methods",
    "text": "In EFAtools you can also run multiple retention methods\n\n\nCode\nN_FACTORS(df, criteria = c(\"PARALLEL\", \"EKC\", \"SMT\"),\n          eigen_type_other = c(\"SMC\", \"PCA\"))\n\n\n\n                                                                                                                                                                \n  🏃 ◯ ◯ Running EKC\n                                                                                                                                                                \n ◉ 🏃 ◯ Running PARALLEL\n                                                                                                                                                                \n ◉ ◉ 🏃  Running SMT\n                                                                                                                                                                \n ◉ ◉ ◉ Done!\n\n\n\n── Tests for the suitability of the data for factor analysis ───────────────────\n\nBartlett's test of sphericity\n\n✔ The Bartlett's test of sphericity was significant at an alpha level of .05.\n  These data are probably suitable for factor analysis.\n\n  𝜒²(15) = 3272.23, p &lt; .001\n\nKaiser-Meyer-Olkin criterion (KMO)\n\n✔ The overall KMO value for your data is middling with 0.795.\n  These data are probably suitable for factor analysis.\n\n── Number of factors suggested by the different factor retention criteria ──────\n\n◌ Empirical Kaiser criterion: 2\n◌ Parallel analysis with PCA: 2\n◌ Parallel analysis with SMC: 2\n◌ Sequential 𝜒² model tests: 2\n◌ Lower bound of RMSEA 90% confidence interval: 2\n◌ Akaike Information Criterion: 2"
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#multiple-scree-plots",
    "href": "posts/Exploratory Factor Analysis/index.html#multiple-scree-plots",
    "title": "Exploratory Factor Analysis",
    "section": "Multiple Scree-plots",
    "text": "Multiple Scree-plots\nTry the following code at home by removing the #\n\n\nCode\n# N_FACTORS(df, method = \"ULS\")"
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#factor-extraction",
    "href": "posts/Exploratory Factor Analysis/index.html#factor-extraction",
    "title": "Exploratory Factor Analysis",
    "section": "Factor Extraction",
    "text": "Factor Extraction\nEFAtools\n\n\nCode\nEFA(df, n_factors = 2, method = \"ML\")\n\n\n\nEFA performed with type = 'EFAtools', method = 'ML', and rotation = 'none'.\n\n── Unrotated Loadings ──────────────────────────────────────────────────────────\n\n     F1      F2  \nx1  -.561    .815\nx2  -.562    .816\nx3  -.552    .818\nx4   .846    .515\nx5   .855    .500\nx6   .844    .515\n\n── Variances Accounted for ─────────────────────────────────────────────────────\n\n                     F1      F2  \nSS loadings          3.093   2.781\nProp Tot Var         0.515   0.463\nCum Prop Tot Var     0.515   0.979\nProp Comm Var        0.527   0.473\nCum Prop Comm Var    0.527   1.000\n\n── Model Fit ───────────────────────────────────────────────────────────────────\n\n𝜒²(4) =  1.47, p = .832\nCFI = 1.00\nRMSEA [90% CI] = .00 [.00; .06]\nAIC = -6.53\nBIC = -20.61\nCAF = .50"
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#rotating-solution",
    "href": "posts/Exploratory Factor Analysis/index.html#rotating-solution",
    "title": "Exploratory Factor Analysis",
    "section": "Rotating solution",
    "text": "Rotating solution\nEFAtools\n\n\nCode\nEFA(df, n_factors = 2, rotation = \"promax\")\n\n\n\nEFA performed with type = 'EFAtools', method = 'PAF', and rotation = 'promax'.\n\n── Rotated Loadings ────────────────────────────────────────────────────────────\n\n     F1      F2  \nx1  -.004    .989\nx2  -.003    .991\nx3   .006    .987\nx4   .991    .005\nx5   .989   -.012\nx6   .989    .007\n\n── Factor Intercorrelations ────────────────────────────────────────────────────\n\n     F1      F2  \nF1   1.000  -0.058\nF2  -0.058   1.000\n\n── Variances Accounted for ─────────────────────────────────────────────────────\n\n                     F1      F2  \nSS loadings          3.107   2.766\nProp Tot Var         0.518   0.461\nCum Prop Tot Var     0.518   0.979\nProp Comm Var        0.529   0.471\nCum Prop Comm Var    0.529   1.000\n\n── Model Fit ───────────────────────────────────────────────────────────────────\n\nCAF: .50\ndf:   4"
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#factor-extraction-1",
    "href": "posts/Exploratory Factor Analysis/index.html#factor-extraction-1",
    "title": "Exploratory Factor Analysis",
    "section": "Factor Extraction",
    "text": "Factor Extraction\npsych\n\n\nCode\nFA_df&lt;- fa(df, nfactors=2, fm=\"ml\")\nsummary.psych(FA_df)\n\n\n\nFactor analysis with Call: fa(r = df, nfactors = 2, fm = \"ml\")\n\nTest of the hypothesis that 2 factors are sufficient.\nThe degrees of freedom for the model is 4  and the objective function was  0.01 \nThe number of observations was  250  with Chi Square =  1.45  with prob &lt;  0.84 \n\nThe root mean square of the residuals (RMSA) is  0 \nThe df corrected root mean square of the residuals is  0 \n\nTucker Lewis Index of factoring reliability =  1.003\nRMSEA index =  0  and the 10 % confidence intervals are  0 0.055\nBIC =  -20.64\n With factor correlations of \n      ML1   ML2\nML1  1.00 -0.06\nML2 -0.06  1.00"
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#factor-extraction-psych-continued",
    "href": "posts/Exploratory Factor Analysis/index.html#factor-extraction-psych-continued",
    "title": "Exploratory Factor Analysis",
    "section": "Factor Extraction, psych continued",
    "text": "Factor Extraction, psych continued\nExamine the residuals\n\n\nCode\nresiduals.psych(FA_df)\n\n\n   x1   x2   x3   x4   x5   x6  \nx1 0.02                         \nx2 0.00 0.02                    \nx3 0.00 0.00 0.03               \nx4 0.00 0.00 0.00 0.02          \nx5 0.00 0.00 0.00 0.00 0.02     \nx6 0.00 0.00 0.00 0.00 0.00 0.02"
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#interpretation-psych",
    "href": "posts/Exploratory Factor Analysis/index.html#interpretation-psych",
    "title": "Exploratory Factor Analysis",
    "section": "Interpretation, psych",
    "text": "Interpretation, psych\n\n\nCode\nFA_df$loadings\n\n\n\nLoadings:\n   ML1    ML2   \nx1         0.989\nx2         0.991\nx3         0.987\nx4  0.991       \nx5  0.989       \nx6  0.989       \n\n                 ML1   ML2\nSS loadings    2.939 2.935\nProportion Var 0.490 0.489\nCumulative Var 0.490 0.979"
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#rotating-solution-2-oblimin",
    "href": "posts/Exploratory Factor Analysis/index.html#rotating-solution-2-oblimin",
    "title": "Exploratory Factor Analysis",
    "section": "Rotating solution 2, oblimin",
    "text": "Rotating solution 2, oblimin\nEFAtools\n\n\nCode\nEFA(df, n_factors = 2, rotation = \"oblimin\", method = \"ULS\")\n\n\n\nEFA performed with type = 'EFAtools', method = 'ULS', and rotation = 'oblimin'.\n\n── Rotated Loadings ────────────────────────────────────────────────────────────\n\n     F1      F2  \nx1  -.004    .989\nx2  -.003    .991\nx3   .006    .987\nx4   .991    .005\nx5   .989   -.012\nx6   .989    .007\n\n── Factor Intercorrelations ────────────────────────────────────────────────────\n\n     F1      F2  \nF1   1.000  -0.058\nF2  -0.058   1.000\n\n── Variances Accounted for ─────────────────────────────────────────────────────\n\n                     F1      F2  \nSS loadings          3.107   2.766\nProp Tot Var         0.518   0.461\nCum Prop Tot Var     0.518   0.979\nProp Comm Var        0.529   0.471\nCum Prop Comm Var    0.529   1.000\n\n── Model Fit ───────────────────────────────────────────────────────────────────\n\n𝜒²(4) =  0.00, p =1.000\nCFI = 1.00\nRMSEA [90% CI] = .00 [.00; .00]\nAIC = -8.00\nBIC = -22.09\nCAF = .50"
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#mental-break-down",
    "href": "posts/Exploratory Factor Analysis/index.html#mental-break-down",
    "title": "Exploratory Factor Analysis",
    "section": "Mental Break (down)",
    "text": "Mental Break (down)\nProbably a lot to process in one go. We will take a mental break here and then we will work together through the next example."
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#but-but-what-about-lavaan",
    "href": "posts/Exploratory Factor Analysis/index.html#but-but-what-about-lavaan",
    "title": "Exploratory Factor Analysis",
    "section": "But, but, what about lavaan???",
    "text": "But, but, what about lavaan???\nYes we have left lavaan out so far. Now is the time to have a look of how we could go through the above process using lavaan (partially).\n\n\nCode\nlibrary(lavaan)\n\nefa.model.fit &lt;- lavaan::efa(data = df, nfactors = 2, rotation = \"promax\")\nsummary(efa.model.fit)\n\n\nThis is lavaan 0.6.17 -- running exploratory factor analysis\n\n  Estimator                                         ML\n  Rotation method                       PROMAX OBLIQUE\n  Promax kappa                                       4\n  Rotation algorithm (rstarts)              PROMAX (0)\n  Standardized metric                             TRUE\n  Row weights                                   Kaiser\n\n  Number of observations                           250\n\nFit measures:\n                   aic      bic    sabic chisq df pvalue cfi rmsea\n  nfactors = 2 1025.02 1084.885 1030.994 1.478  4  0.831   1     0\n\nEigenvalues correlation matrix:\n\n     ev1      ev2      ev3      ev4      ev5      ev6 \n  3.1285   2.7871   0.0247   0.0214   0.0193   0.0189 \n\nStandardized loadings:\n\n       f1     f2      unique.var   communalities\nx1  0.989                  0.021           0.979\nx2  0.991                  0.018           0.982\nx3  0.987                  0.026           0.974\nx4         0.991           0.019           0.981\nx5         0.989           0.020           0.980\nx6         0.989           0.022           0.978\n\n                              f2    f1 total\nSum of sq (obliq) loadings 2.939 2.935 5.873\nProportion of total        0.500 0.500 1.000\nProportion var             0.490 0.489 0.979\nCumulative var             0.490 0.979 0.979\n\nFactor correlations:\n\n       f1     f2\nf1  1.000       \nf2 -0.058  1.000"
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#ok-time-for-our-collaborative-example",
    "href": "posts/Exploratory Factor Analysis/index.html#ok-time-for-our-collaborative-example",
    "title": "Exploratory Factor Analysis",
    "section": "OK, time for our collaborative example",
    "text": "OK, time for our collaborative example\nWe will use a lavaan built-in dataset called HolzingerSwineford1939\nThe data consists of mental ability test scores of seventh- and eighth-grade children from two different schools (Pasteur and Grant-White). In our version of the dataset, only 9 out of the original 26 tests are included. A CFA model that is often proposed for these 9 variables consists of three latent variables (or factors), each with three indicators:\n\na visual factor measured by 3 variables: x1, x2 and x3\na textual factor measured by 3 variables: x4, x5 and x6\na speed factor measured by 3 variables: x7, x8 and x9"
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#visual-model",
    "href": "posts/Exploratory Factor Analysis/index.html#visual-model",
    "title": "Exploratory Factor Analysis",
    "section": "Visual Model",
    "text": "Visual Model"
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#let-us-assign-the-data-to-a-dataframe",
    "href": "posts/Exploratory Factor Analysis/index.html#let-us-assign-the-data-to-a-dataframe",
    "title": "Exploratory Factor Analysis",
    "section": "Let us assign the data to a dataframe",
    "text": "Let us assign the data to a dataframe\n\n\nCode\ndata(HolzingerSwineford1939)\ndf2 &lt;- HolzingerSwineford1939\nhead(df2)\n\n\n  id sex ageyr agemo  school grade       x1   x2    x3       x4   x5        x6\n1  1   1    13     1 Pasteur     7 3.333333 7.75 0.375 2.333333 5.75 1.2857143\n2  2   2    13     7 Pasteur     7 5.333333 5.25 2.125 1.666667 3.00 1.2857143\n3  3   2    13     1 Pasteur     7 4.500000 5.25 1.875 1.000000 1.75 0.4285714\n4  4   1    13     2 Pasteur     7 5.333333 7.75 3.000 2.666667 4.50 2.4285714\n5  5   2    12     2 Pasteur     7 4.833333 4.75 0.875 2.666667 4.00 2.5714286\n6  6   2    14     1 Pasteur     7 5.333333 5.00 2.250 1.000000 3.00 0.8571429\n        x7   x8       x9\n1 3.391304 5.75 6.361111\n2 3.782609 6.25 7.916667\n3 3.260870 3.90 4.416667\n4 3.000000 5.30 4.861111\n5 3.695652 6.30 5.916667\n6 4.347826 6.65 7.500000"
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#keep-only-the-indicators-columns",
    "href": "posts/Exploratory Factor Analysis/index.html#keep-only-the-indicators-columns",
    "title": "Exploratory Factor Analysis",
    "section": "Keep only the indicators columns",
    "text": "Keep only the indicators columns\n\n\nCode\nlibrary(tidyverse)\ndf2 &lt;- df2 |&gt; \n  dplyr::select(7:15)"
  },
  {
    "objectID": "posts/Exploratory Factor Analysis/index.html#what-should-be-our-first-step",
    "href": "posts/Exploratory Factor Analysis/index.html#what-should-be-our-first-step",
    "title": "Exploratory Factor Analysis",
    "section": "What should be our first step?",
    "text": "What should be our first step?"
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#todays-aims",
    "href": "posts/Confirmatory Factor Analysis/index.html#todays-aims",
    "title": "Confirmatory Factor Analysis",
    "section": "Today’s Aims",
    "text": "Today’s Aims\nToday we will go through confirmatory factor analysis using lavaan. Our main focus will be lavaan syntax and the interpretation of output for different models and not going through the detailed mathematics behind the CFA processes. We will get a chance to talk about the mathematics and more during the next weeks of more intermediate and advanced topics."
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#todays-examples",
    "href": "posts/Confirmatory Factor Analysis/index.html#todays-examples",
    "title": "Confirmatory Factor Analysis",
    "section": "Today’s examples",
    "text": "Today’s examples\nWe will be working on the same variables that we generated last week during the EFA. We will go through the process of one factor CFA and two factor CFA.\nWe specifically explored a model with two factors and overall 6 items, today we will first attempt to confirm a model where three items load to latent variable A. This will be our one-factor CFA.\nIn the second part of our workshop we will attempt to confirm a model with two latent variables A and B."
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#terminology",
    "href": "posts/Confirmatory Factor Analysis/index.html#terminology",
    "title": "Confirmatory Factor Analysis",
    "section": "Terminology",
    "text": "Terminology\nToday we will also be referring back to many of the terms that we have defined in the past.\n\nObserved variables\nLatent variables\nDirectional/regression paths\nNon-directional paths/covariance/variance\nModel parameters\nExogenous, endogenous variables\nMeasurement and structural model"
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#we-will-expand-on-terminology-today",
    "href": "posts/Confirmatory Factor Analysis/index.html#we-will-expand-on-terminology-today",
    "title": "Confirmatory Factor Analysis",
    "section": "We will expand on terminology today",
    "text": "We will expand on terminology today\n\nScale: latent variables do not have a measurement scale, instead we have to define one for them. To do that we need to set an origin and a unit\n\nOrigin: we can set the mean to 0\nUnit\n\nEither set the variance to 1\nOr, use the same unit as that of one of the measured variables ( only 1 item)"
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#notations-in-lavaan-refresher",
    "href": "posts/Confirmatory Factor Analysis/index.html#notations-in-lavaan-refresher",
    "title": "Confirmatory Factor Analysis",
    "section": "Notations in lavaan (refresher)",
    "text": "Notations in lavaan (refresher)\n\n~ predict, used for regression of observed outcome to observed predictors\n=~ indicator, used for latent variable to observed indicators\n~~ covariance\n1* fixes parameter or loading to 1\nNA* frees parameter or loading\n~1 intercept or mean (e.g., x1 ~ 1 estimates the mean of variable x1)\na* defines the parameter ‘a’,"
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#number-of-parameters-refresher",
    "href": "posts/Confirmatory Factor Analysis/index.html#number-of-parameters-refresher",
    "title": "Confirmatory Factor Analysis",
    "section": "Number of parameters (refresher)",
    "text": "Number of parameters (refresher)\nAs mentioned before every path or (co)variance that has not been fixed to a specific value will have to be estimated\n\nFactor loadings\nFactor covariances\nFactor variances\nError variances"
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#last-weeks-example",
    "href": "posts/Confirmatory Factor Analysis/index.html#last-weeks-example",
    "title": "Confirmatory Factor Analysis",
    "section": "Last week’s example",
    "text": "Last week’s example\nLast week we generated random data using the 1212 seed. Today we will carry out CFA on the same model but using 3131 as a seed.\n\n\nCode\nset.seed(3131)\n### normally distributed factors\n### these are just to help me set the indicators\n### the f1 and f2 will not be included in the data.frame\nf1 &lt;- rnorm(250)\nf2 &lt;- rnorm(250)\n\n### f1 indicators x1 to x3\nx1 &lt;- f1 + rnorm(250, sd=0.15)\nx2 &lt;- f1 + rnorm(250, sd=0.15)\nx3 &lt;- f1 + rnorm(250, sd=0.15)\n\n### f2 indicators x4 to x6\nx4 &lt;- f2 + rnorm(250, sd=0.15)\nx5 &lt;- f2 + rnorm(250, sd=0.15)\nx6 &lt;- f2 + rnorm(250, sd=0.15)\n\n### creating the dataframe\ndf &lt;- data.frame(x1=x1, x2=x2, x3=x3, x4=x4, x5=x5, x6=x6)"
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#one-factor-cfa",
    "href": "posts/Confirmatory Factor Analysis/index.html#one-factor-cfa",
    "title": "Confirmatory Factor Analysis",
    "section": "One factor CFA",
    "text": "One factor CFA"
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#one-factor-cfa-expanded",
    "href": "posts/Confirmatory Factor Analysis/index.html#one-factor-cfa-expanded",
    "title": "Confirmatory Factor Analysis",
    "section": "One factor CFA, expanded",
    "text": "One factor CFA, expanded"
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#once-factor-cfa-expanded-again",
    "href": "posts/Confirmatory Factor Analysis/index.html#once-factor-cfa-expanded-again",
    "title": "Confirmatory Factor Analysis",
    "section": "Once factor CFA, expanded (again)",
    "text": "Once factor CFA, expanded (again)"
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#one-factor-cfa-and-degrees-of-freedom",
    "href": "posts/Confirmatory Factor Analysis/index.html#one-factor-cfa-and-degrees-of-freedom",
    "title": "Confirmatory Factor Analysis",
    "section": "One Factor CFA and degrees of freedom",
    "text": "One Factor CFA and degrees of freedom\n\ndf &lt; 0, the model is under-identified\ndf = 0, the model is just-identified (also known as saturated), no model fit\ndf &gt; 0, over-identified, we can assess model fit\n\nReminder: df = number of known values - number of parameters to estimate"
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#one-factor-cfa-and-degrees-of-freedom-1",
    "href": "posts/Confirmatory Factor Analysis/index.html#one-factor-cfa-and-degrees-of-freedom-1",
    "title": "Confirmatory Factor Analysis",
    "section": "One Factor CFA and degrees of freedom",
    "text": "One Factor CFA and degrees of freedom\n\nTotal number of parameters (alson knows as “known values”) as previously discussed\n\\[\np(p+1)/2\n\\] \\[\n3(4)/2=6\n\\]\nNumber of parameters to estimate???? (Let us revisit the slide with the model visualisation)"
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#identification-methods",
    "href": "posts/Confirmatory Factor Analysis/index.html#identification-methods",
    "title": "Confirmatory Factor Analysis",
    "section": "Identification Methods",
    "text": "Identification Methods\n\nmarker method: we fix the first loading of each factor to 1 (what does this mean?)\nvariance standardization method: we fix the variance of each factor to 1 and we freely estimate all other loadings (what does this mean?)\nstandardization all method, standardizes the variance of each factor to 1 but also standardizes the items\n\nNote: default lavaan method is the marker method"
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#let-us-see-this-example-in-lavaan",
    "href": "posts/Confirmatory Factor Analysis/index.html#let-us-see-this-example-in-lavaan",
    "title": "Confirmatory Factor Analysis",
    "section": "Let us see this example in lavaan",
    "text": "Let us see this example in lavaan\n\n\nCode\nlibrary(lavaan)\n\nmodel1 &lt;- '\nf1 =~ x1 + x2 + x3\n'\n\nmodel1.fit &lt;- cfa(model1, data = df)\n\nsummary(model1.fit)\n\n\nlavaan 0.6.17 ended normally after 51 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         6\n\n  Number of observations                           250\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  f1 =~                                               \n    x1                1.000                           \n    x2                0.992    0.014   72.880    0.000\n    x3                0.991    0.014   72.206    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                0.019    0.003    6.471    0.000\n   .x2                0.023    0.003    7.277    0.000\n   .x3                0.023    0.003    7.424    0.000\n    f1                0.912    0.083   10.951    0.000"
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#how-can-we-interpret-this-output",
    "href": "posts/Confirmatory Factor Analysis/index.html#how-can-we-interpret-this-output",
    "title": "Confirmatory Factor Analysis",
    "section": "How can we interpret this output?",
    "text": "How can we interpret this output?\n\nx1 estimate is 1.000 and has no std error, z-value, nor p-value. This is because lavaan uses the marker method by default. x1 has been fixed to 1 and is now the scale of our factor 1.\nx2 estimate is 0.992. For an increase of 1 unit in f1, x2 increases by 0.992. The 1 unit in f1 is the unit of x1 as this was set to be the scale\n.x1 refers to residual variances, hence the . in front of x1\nf estimate of 0.912 is the variance of our latent variable (factor)\np-values are just telling us if our estimates are significantly greater than zero\n\nBut what about intercept???"
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#adding-intercept-in-lavaan",
    "href": "posts/Confirmatory Factor Analysis/index.html#adding-intercept-in-lavaan",
    "title": "Confirmatory Factor Analysis",
    "section": "Adding intercept in lavaan",
    "text": "Adding intercept in lavaan\n\n\nCode\nlibrary(lavaan)\n\nmodel1.inter &lt;- '\nf1 =~ x1 + x2 + x3\nf1 ~ 1 \n'\n\nmodel1.inter.fit &lt;- cfa(model1.inter, data = df)\n\nsummary(model1.inter.fit)\n\n\nlavaan 0.6.17 ended normally after 51 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n  Number of observations                           250\n\nModel Test User Model:\n                                                      \n  Test statistic                                    NA\n  Degrees of freedom                                -1\n  P-value (Unknown)                                 NA\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  f1 =~                                               \n    x1                1.000                           \n    x2                0.992       NA                  \n    x3                0.991       NA                  \n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    f1                0.000       NA                  \n   .x1                0.009       NA                  \n   .x2                0.036       NA                  \n   .x3                0.014       NA                  \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                0.019       NA                  \n   .x2                0.023       NA                  \n   .x3                0.023       NA                  \n    f1                0.912       NA"
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#what-about-variance-standardization-method",
    "href": "posts/Confirmatory Factor Analysis/index.html#what-about-variance-standardization-method",
    "title": "Confirmatory Factor Analysis",
    "section": "What about variance standardization method",
    "text": "What about variance standardization method\n\n\nCode\nlibrary(lavaan)\n\nmodel1.var &lt;- '\nf1 =~ NA*x1 + x2 + x3\nf1 ~~ 1*f1 \n'\n\nmodel1.var.fit &lt;- cfa(model1.var, data = df)\n\nsummary(model1.var.fit)\n\n\nlavaan 0.6.17 ended normally after 31 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         6\n\n  Number of observations                           250\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  f1 =~                                               \n    x1                0.955    0.044   21.901    0.000\n    x2                0.948    0.043   21.811    0.000\n    x3                0.946    0.043   21.792    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    f1                1.000                           \n   .x1                0.019    0.003    6.471    0.000\n   .x2                0.023    0.003    7.277    0.000\n   .x3                0.023    0.003    7.424    0.000"
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#thanks-for-nothing-laz",
    "href": "posts/Confirmatory Factor Analysis/index.html#thanks-for-nothing-laz",
    "title": "Confirmatory Factor Analysis",
    "section": "Thanks for nothing Laz!",
    "text": "Thanks for nothing Laz!\n\n\nCode\nmodel1 &lt;- '\nf1 =~ x1 + x2 + x3\n'\n\nmodel1.fit &lt;- cfa(model1, std.lv=TRUE, data = df)\n\nsummary(model1.fit)\n\n\nlavaan 0.6.17 ended normally after 31 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         6\n\n  Number of observations                           250\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  f1 =~                                               \n    x1                0.955    0.044   21.901    0.000\n    x2                0.948    0.043   21.811    0.000\n    x3                0.946    0.043   21.792    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                0.019    0.003    6.471    0.000\n   .x2                0.023    0.003    7.277    0.000\n   .x3                0.023    0.003    7.424    0.000\n    f1                1.000"
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#interpretation",
    "href": "posts/Confirmatory Factor Analysis/index.html#interpretation",
    "title": "Confirmatory Factor Analysis",
    "section": "Interpretation",
    "text": "Interpretation\nRemember this method standardizes our factor, so we will need to speak in terms of standard deviations\nx1 estimate of 0.955: For an increase of 1 standard deviation in f1, x1 increases by 0.955"
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#and-the-magnificent-standardization-all-walks-in",
    "href": "posts/Confirmatory Factor Analysis/index.html#and-the-magnificent-standardization-all-walks-in",
    "title": "Confirmatory Factor Analysis",
    "section": "And the magnificent standardization all walks in",
    "text": "And the magnificent standardization all walks in\n\n\nCode\nmodel1 &lt;- '\nf1 =~ x1 + x2 + x3\n'\n\nmodel1.fit &lt;- cfa(model1, data = df)\n\nsummary(model1.fit, standardized=TRUE)\n\n\nlavaan 0.6.17 ended normally after 51 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         6\n\n  Number of observations                           250\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  f1 =~                                                                 \n    x1                1.000                               0.955    0.990\n    x2                0.992    0.014   72.880    0.000    0.948    0.988\n    x3                0.991    0.014   72.206    0.000    0.946    0.987\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .x1                0.019    0.003    6.471    0.000    0.019    0.020\n   .x2                0.023    0.003    7.277    0.000    0.023    0.024\n   .x3                0.023    0.003    7.424    0.000    0.023    0.025\n    f1                0.912    0.083   10.951    0.000    1.000    1.000"
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#moving-on-to-model-fit-statistics",
    "href": "posts/Confirmatory Factor Analysis/index.html#moving-on-to-model-fit-statistics",
    "title": "Confirmatory Factor Analysis",
    "section": "Moving on to Model fit statistics",
    "text": "Moving on to Model fit statistics\nAs things are now we cannot obtain model fit statistics as df =0\nSo our model is just-identified (saturated)\n\n\nCode\nmodel2 &lt;- '\nf1 =~ x1 + x2 + x3 + x4\nf2 =~ x5 + x6\n'\n\nmodel2.fit &lt;- cfa(model2, data = df)\nsummary(model2.fit, standardized=TRUE, fit.measures=TRUE)\n\n\nlavaan 0.6.17 ended normally after 48 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        13\n\n  Number of observations                           250\n\nModel Test User Model:\n                                                      \n  Test statistic                               812.060\n  Degrees of freedom                                 8\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              3202.055\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.748\n  Tucker-Lewis Index (TLI)                       0.527\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)               -889.645\n  Loglikelihood unrestricted model (H1)       -483.616\n                                                      \n  Akaike (AIC)                                1805.291\n  Bayesian (BIC)                              1851.070\n  Sample-size adjusted Bayesian (SABIC)       1809.859\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.634\n  90 Percent confidence interval - lower         0.598\n  90 Percent confidence interval - upper         0.671\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.299\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  f1 =~                                                                 \n    x1                1.000                               0.955    0.990\n    x2                0.992    0.014   72.869    0.000    0.948    0.988\n    x3                0.991    0.014   72.277    0.000    0.946    0.987\n    x4               -0.090    0.065   -1.397    0.162   -0.086   -0.088\n  f2 =~                                                                 \n    x5                1.000                               0.960    0.961\n    x6                1.028    0.185    5.548    0.000    0.986    1.017\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  f1 ~~                                                                 \n    f2               -0.072    0.061   -1.180    0.238   -0.078   -0.078\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .x1                0.019    0.003    6.451    0.000    0.019    0.020\n   .x2                0.023    0.003    7.296    0.000    0.023    0.025\n   .x3                0.023    0.003    7.424    0.000    0.023    0.025\n   .x4                0.947    0.085   11.180    0.000    0.947    0.992\n   .x5                0.076    0.166    0.456    0.648    0.076    0.076\n   .x6               -0.032    0.175   -0.183    0.855   -0.032   -0.034\n    f1                0.912    0.083   10.952    0.000    1.000    1.000\n    f2                0.921    0.188    4.903    0.000    1.000    1.000"
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#what-if-we-make-sure-there-is-no-covariance-between-factors",
    "href": "posts/Confirmatory Factor Analysis/index.html#what-if-we-make-sure-there-is-no-covariance-between-factors",
    "title": "Confirmatory Factor Analysis",
    "section": "What if we make sure there is no covariance between factors?",
    "text": "What if we make sure there is no covariance between factors?\n\n\nCode\nmodel2.nocov &lt;- '\nf1 =~ x1 + x2 + x3 + x4\nf2 =~ x5 + x6\nf1~~0*f2\n'\n\nmodel2.nocov.fit &lt;- cfa(model2.nocov, data = df)\nsummary(model2.nocov.fit, standardized=TRUE, fit.measures=TRUE)\n\n\nlavaan 0.6.17 ended normally after 52 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        12\n\n  Number of observations                           250\n\nModel Test User Model:\n                                                      \n  Test statistic                               813.666\n  Degrees of freedom                                 9\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              3202.055\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.748\n  Tucker-Lewis Index (TLI)                       0.579\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)               -890.449\n  Loglikelihood unrestricted model (H1)       -483.616\n                                                      \n  Akaike (AIC)                                1804.898\n  Bayesian (BIC)                              1847.155\n  Sample-size adjusted Bayesian (SABIC)       1809.114\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.598\n  90 Percent confidence interval - lower         0.564\n  90 Percent confidence interval - upper         0.633\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.303\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  f1 =~                                                                 \n    x1                1.000                               0.955    0.990\n    x2                0.992       NA                      0.948    0.988\n    x3                0.991       NA                      0.946    0.987\n    x4               -0.090       NA                     -0.086   -0.088\n  f2 =~                                                                 \n    x5                1.000                               0.921    0.922\n    x6                1.116       NA                      1.028    1.060\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  f1 ~~                                                                 \n    f2                0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .x1                0.019       NA                      0.019    0.020\n   .x2                0.023       NA                      0.023    0.025\n   .x3                0.023       NA                      0.023    0.025\n   .x4                0.948       NA                      0.948    0.992\n   .x5                0.149       NA                      0.149    0.149\n   .x6               -0.116       NA                     -0.116   -0.123\n    f1                0.912       NA                      1.000    1.000\n    f2                0.848       NA                      1.000    1.000"
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#can-we-improve-our-model",
    "href": "posts/Confirmatory Factor Analysis/index.html#can-we-improve-our-model",
    "title": "Confirmatory Factor Analysis",
    "section": "Can we improve our model?",
    "text": "Can we improve our model?\nOne way to do that is to look into our model residuals. Model residuals are an absolute fit index where we compare our model with th observed data. Generally, you regard absolute goodness of fit as the “discrepancy” between our model and the observed data. Higher residuals indicate greater discrepancy.\nSo how high is bad? We can request either correlations or standardized residuals."
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#correlations",
    "href": "posts/Confirmatory Factor Analysis/index.html#correlations",
    "title": "Confirmatory Factor Analysis",
    "section": "Correlations",
    "text": "Correlations\nHere both observed and estimated covariances are converted into correlations and then we calculate the differences. Greater differences indicate problematic items.\n\n\nCode\nresiduals(model2.fit, type=\"cor\")\n\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n       x1     x2     x3     x4     x5     x6\nx1  0.000                                   \nx2  0.000  0.000                            \nx3  0.000  0.000  0.000                     \nx4 -0.010  0.013  0.001  0.000              \nx5 -0.010  0.015  0.000  0.967  0.000       \nx6 -0.010  0.015 -0.002  0.968  0.000  0.000"
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#standardized-residuals",
    "href": "posts/Confirmatory Factor Analysis/index.html#standardized-residuals",
    "title": "Confirmatory Factor Analysis",
    "section": "Standardized residuals",
    "text": "Standardized residuals\nHere we standardize the covariance and in practice treat it as a z-score, values greater than 1.96 indicate problematic cases.\n\n\nCode\nresiduals(model2.fit, type=\"standardized\")\n\n\n$type\n[1] \"standardized\"\n\n$cov\n       x1     x2     x3     x4     x5     x6\nx1  0.000                                   \nx2  0.000  0.000                            \nx3 -1.636  1.394  0.000                     \nx4 -1.342  1.533  0.108  0.000              \nx5 -1.462  1.783  0.010 11.027  0.000       \nx6 -1.347  1.868 -0.199 11.039  0.000  0.000"
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#modification-indices",
    "href": "posts/Confirmatory Factor Analysis/index.html#modification-indices",
    "title": "Confirmatory Factor Analysis",
    "section": "Modification Indices",
    "text": "Modification Indices\nWe should look at modification indices that give as an estimate change of our chi-square value if we make changes to our model.\n\n\nCode\nmodificationindices(model2.fit)\n\n\n   lhs op rhs      mi    epc sepc.lv sepc.all sepc.nox\n18  f2 =~  x1   1.522 -0.014  -0.013   -0.014   -0.014\n19  f2 =~  x2   3.561  0.022   0.021    0.022    0.022\n20  f2 =~  x3   0.117 -0.004  -0.004   -0.004   -0.004\n21  f2 =~  x4 225.561  0.939   0.901    0.923    0.923\n22  x1 ~~  x2   0.105  0.030   0.030    1.446    1.446\n23  x1 ~~  x3   5.001 -0.204  -0.204   -9.753   -9.753\n24  x1 ~~  x4   1.821 -0.015  -0.015   -0.109   -0.109\n25  x1 ~~  x5   0.481 -0.002  -0.002   -0.043   -0.043\n26  x1 ~~  x6   0.153  0.001   0.001    0.036    0.036\n27  x2 ~~  x3   3.971  0.173   0.173    7.516    7.516\n28  x2 ~~  x4   2.366  0.017   0.017    0.117    0.117\n29  x2 ~~  x5   0.042  0.000   0.000   -0.012   -0.012\n30  x2 ~~  x6   0.357  0.001   0.001    0.052    0.052\n31  x3 ~~  x4   0.012  0.001   0.001    0.008    0.008\n32  x3 ~~  x5   0.931  0.002   0.002    0.056    0.056\n33  x3 ~~  x6   0.970 -0.002  -0.002   -0.085   -0.085\n34  x4 ~~  x5   2.227  0.019   0.019    0.072    0.072\n35  x4 ~~  x6   3.243  0.023   0.023    0.130    0.130"
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#modifying-the-model",
    "href": "posts/Confirmatory Factor Analysis/index.html#modifying-the-model",
    "title": "Confirmatory Factor Analysis",
    "section": "Modifying the model",
    "text": "Modifying the model\n\n\nCode\nmodel2.1 &lt;- '\nf1 =~ x1 + x2 + x3 \nf2 =~ x4 + x5 + x6\n'\n\nmodel2.1.fit &lt;- cfa(model2.1, data = df)\nsummary(model2.1.fit, standardized=TRUE, fit.measures=TRUE)\n\n\nlavaan 0.6.17 ended normally after 58 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        13\n\n  Number of observations                           250\n\nModel Test User Model:\n                                                      \n  Test statistic                                 7.569\n  Degrees of freedom                                 8\n  P-value (Chi-square)                           0.477\n\nModel Test Baseline Model:\n\n  Test statistic                              3202.055\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.000\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)               -487.400\n  Loglikelihood unrestricted model (H1)       -483.616\n                                                      \n  Akaike (AIC)                                1000.800\n  Bayesian (BIC)                              1046.579\n  Sample-size adjusted Bayesian (SABIC)       1005.368\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.072\n  P-value H_0: RMSEA &lt;= 0.050                    0.817\n  P-value H_0: RMSEA &gt;= 0.080                    0.025\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.007\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  f1 =~                                                                 \n    x1                1.000                               0.955    0.990\n    x2                0.992    0.014   72.877    0.000    0.948    0.988\n    x3                0.991    0.014   72.242    0.000    0.946    0.987\n  f2 =~                                                                 \n    x4                1.000                               0.963    0.986\n    x5                1.024    0.015   66.812    0.000    0.986    0.988\n    x6                0.997    0.014   69.570    0.000    0.960    0.990\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  f1 ~~                                                                 \n    f2               -0.074    0.059   -1.263    0.206   -0.081   -0.081\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .x1                0.019    0.003    6.461    0.000    0.019    0.020\n   .x2                0.023    0.003    7.286    0.000    0.023    0.025\n   .x3                0.023    0.003    7.424    0.000    0.023    0.025\n   .x4                0.027    0.004    7.763    0.000    0.027    0.029\n   .x5                0.025    0.003    7.099    0.000    0.025    0.025\n   .x6                0.019    0.003    6.260    0.000    0.019    0.020\n    f1                0.912    0.083   10.951    0.000    1.000    1.000\n    f2                0.927    0.085   10.858    0.000    1.000    1.000"
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#let-us-see-if-the-residuals-tell-the-same-story",
    "href": "posts/Confirmatory Factor Analysis/index.html#let-us-see-if-the-residuals-tell-the-same-story",
    "title": "Confirmatory Factor Analysis",
    "section": "Let us see if the residuals tell the same story",
    "text": "Let us see if the residuals tell the same story\n\n\nCode\nresiduals(model2.1.fit, type=\"standardized\")\n\n\n$type\n[1] \"standardized\"\n\n$cov\n       x1     x2     x3     x4     x5     x6\nx1  0.000                                   \nx2  0.000  0.000                            \nx3 -1.669  1.497  0.000                     \nx4 -1.576  0.327 -0.624  0.000              \nx5 -0.533  1.678  0.406  0.000  0.000       \nx6 -0.907  1.477 -0.104 -0.684  0.861  0.000"
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#what-about-modification-indices",
    "href": "posts/Confirmatory Factor Analysis/index.html#what-about-modification-indices",
    "title": "Confirmatory Factor Analysis",
    "section": "What about modification indices",
    "text": "What about modification indices\n\n\nCode\nmodificationindices(model2.1.fit)\n\n\n   lhs op rhs    mi    epc sepc.lv sepc.all sepc.nox\n16  f1 =~  x4 0.787 -0.012  -0.011   -0.011   -0.011\n17  f1 =~  x5 0.439  0.008   0.008    0.008    0.008\n18  f1 =~  x6 0.031  0.002   0.002    0.002    0.002\n19  f2 =~  x1 2.236 -0.017  -0.017   -0.017   -0.017\n20  f2 =~  x2 2.856  0.020   0.020    0.020    0.020\n21  f2 =~  x3 0.016 -0.002  -0.001   -0.002   -0.002\n22  x1 ~~  x2 0.016  0.017   0.017    0.839    0.839\n23  x1 ~~  x3 2.856 -0.229  -0.229  -10.923  -10.923\n24  x1 ~~  x4 0.031  0.000   0.000    0.016    0.016\n25  x1 ~~  x5 0.476 -0.001  -0.001   -0.067   -0.067\n26  x1 ~~  x6 0.051  0.000   0.000    0.023    0.023\n27  x2 ~~  x3 2.236  0.192   0.192    8.386    8.386\n28  x2 ~~  x4 1.923 -0.003  -0.003   -0.124   -0.124\n29  x2 ~~  x5 0.174  0.001   0.001    0.039    0.039\n30  x2 ~~  x6 1.393  0.002   0.002    0.116    0.116\n31  x3 ~~  x4 1.090  0.002   0.002    0.093    0.093\n32  x3 ~~  x5 0.189  0.001   0.001    0.040    0.040\n33  x3 ~~  x6 1.946 -0.003  -0.003   -0.136   -0.136\n34  x4 ~~  x5 0.031 -0.024  -0.024   -0.941   -0.941\n35  x4 ~~  x6 0.439 -0.093  -0.093   -4.050   -4.050\n36  x5 ~~  x6 0.787  0.133   0.133    6.150    6.150"
  },
  {
    "objectID": "posts/Confirmatory Factor Analysis/index.html#exercise",
    "href": "posts/Confirmatory Factor Analysis/index.html#exercise",
    "title": "Confirmatory Factor Analysis",
    "section": "Exercise",
    "text": "Exercise\nLet us try a different model now. Go onto your posit cloud account and open the Week 5 project, follow the instructions and carry out the required CFA."
  },
  {
    "objectID": "posts/Latent Growth Models/index.html#todays-aims",
    "href": "posts/Latent Growth Models/index.html#todays-aims",
    "title": "Latent Growth Models",
    "section": "Today’s Aims",
    "text": "Today’s Aims\nToday we will discuss Latent Growth Modeling (LGM) also known as Latent Growth Curve Analysis (LGCA), how it help us measure change over time (repeated measurements) and what advantages they have in comparison to other repeated measures analysis such as repeated measures ANOVA or hierarchical linear models.\nSpecifically we will cover:\n\nAssumptions for LGM\nSpecifying LGM using lavaan\nInterpreting output\nImproving a model\nAdding a covariate (predictor)"
  },
  {
    "objectID": "posts/Latent Growth Models/index.html#notations-in-lavaan-refresher",
    "href": "posts/Latent Growth Models/index.html#notations-in-lavaan-refresher",
    "title": "Latent Growth Models",
    "section": "Notations in lavaan (refresher)",
    "text": "Notations in lavaan (refresher)\n\n~ predict, used for regression of observed outcome to observed predictors\n=~ indicator, used for latent variable to observed indicators\n~~ covariance\n1* fixes parameter or loading to 1\nNA* frees parameter or loading\n~1 intercept or mean (e.g., x1 ~ 1 estimates the mean of variable x1)\na* defines the parameter ‘a’,"
  },
  {
    "objectID": "posts/Latent Growth Models/index.html#lavaan-package",
    "href": "posts/Latent Growth Models/index.html#lavaan-package",
    "title": "Latent Growth Models",
    "section": "lavaan package",
    "text": "lavaan package\nToday we will also expand our lavaan usage beyond the analysis. We will also use the function simulateData to create random data for our LGM. If you like challenges try to randomly create data that will be suitable for LGM."
  },
  {
    "objectID": "posts/Latent Growth Models/index.html#what-is-lgm",
    "href": "posts/Latent Growth Models/index.html#what-is-lgm",
    "title": "Latent Growth Models",
    "section": "What is LGM?",
    "text": "What is LGM?\nGenerally speaking, LGM is a special case of CFA where we incorporate a longitudinal element. This implies that we have a set of repeated measurements, at least three, and we want investigate change over these repeated time measurements.\n\nLGM allows us to estimate means and covariances\nLGM allows us to estimate observed and latent values\nWe will only focus on continuous measurements\nAll of our participants needs to be measured with the same time measurement information\nWe will also primarily focus on linear relationship (not exclusively though)"
  },
  {
    "objectID": "posts/Latent Growth Models/index.html#let-us-describe-a-longitudinalrepeated-measurements-paradigm",
    "href": "posts/Latent Growth Models/index.html#let-us-describe-a-longitudinalrepeated-measurements-paradigm",
    "title": "Latent Growth Models",
    "section": "Let us describe a longitudinal/repeated measurements paradigm",
    "text": "Let us describe a longitudinal/repeated measurements paradigm\nWe measure a psychological construct over 4 time points T1, T2, T3, T4. We are interested in investigating whether the measurements will “grow” (technically not grow) across these time points.\nWe will specify two latent variables, the intercept and the slope. Each latent variable will pass through each of the measurement points.\nWe will set the intercept factor loadings to 1 as we do not want to estimate them.\nWe will set the slope factor loadings to increasing integer values, usually starting from 0."
  },
  {
    "objectID": "posts/Latent Growth Models/index.html#conceptualstatistical-model",
    "href": "posts/Latent Growth Models/index.html#conceptualstatistical-model",
    "title": "Latent Growth Models",
    "section": "Conceptual/Statistical Model",
    "text": "Conceptual/Statistical Model"
  },
  {
    "objectID": "posts/Latent Growth Models/index.html#conceptualstatistical-model-continued",
    "href": "posts/Latent Growth Models/index.html#conceptualstatistical-model-continued",
    "title": "Latent Growth Models",
    "section": "Conceptual/Statistical Model continued",
    "text": "Conceptual/Statistical Model continued"
  },
  {
    "objectID": "posts/Latent Growth Models/index.html#let-us-generate-some-simulated-data",
    "href": "posts/Latent Growth Models/index.html#let-us-generate-some-simulated-data",
    "title": "Latent Growth Models",
    "section": "Let us generate some simulated data",
    "text": "Let us generate some simulated data\nHere we just demonstrate the code, we will return to this code once we have explained our growth model in greater detail.\n\n\nCode\nlibrary(lavaan)\nlibrary(tidyverse)\nset.seed(25256)\n\n# Define the latent growth model\nlgm_model1 &lt;- '\ni =~ 1*T0 + 1*T1 +1*T2 +1*T3\ns =~ 0*T0 + 1*T1 +2*T2 +3*T3\ngender|0*t1\n'\n# Generate random data\nsimulated_data1 &lt;- simulateData(lgm_model1, model.type = \"growth\")\nsimulated_data1 &lt;- simulated_data1 |&gt; \n  mutate(id = row_number())\n# Check the generated data\nhead(simulated_data1)\n\n\n            T0          T1         T2         T3 gender id\n1  0.979298116  0.01583834 -0.8306769 -0.6729512      1  1\n2  1.567274129  0.34300527 -0.4162988  0.6335290      1  2\n3 -0.937327749 -0.83246257 -1.3628852 -0.2684692      2  3\n4  3.293394471  4.92009237  4.0263852  5.3132807      1  4\n5 -0.007578629  1.09421820  1.7625738  0.2041869      2  5\n6 -0.550506563  3.25869067  4.6268938  6.4532276      1  6"
  },
  {
    "objectID": "posts/Latent Growth Models/index.html#analysis",
    "href": "posts/Latent Growth Models/index.html#analysis",
    "title": "Latent Growth Models",
    "section": "Analysis",
    "text": "Analysis\n\n\nCode\n# Define the latent growth model\nlgm_model1 &lt;- '\ni =~ 1*T0 + 1*T1 +1*T2 +1*T3\ns =~ 0*T0 + 1*T1 +2*T2 +3*T3\n'\nfit1 &lt;- growth(lgm_model1, data = simulated_data1)\nsummary(fit1)\n\n\nlavaan 0.6.17 ended normally after 26 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                                 2.409\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.790\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  i =~                                                \n    T0                1.000                           \n    T1                1.000                           \n    T2                1.000                           \n    T3                1.000                           \n  s =~                                                \n    T0                0.000                           \n    T1                1.000                           \n    T2                2.000                           \n    T3                3.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  i ~~                                                \n    s                -0.099    0.073   -1.362    0.173\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    i                -0.071    0.059   -1.195    0.232\n    s                 0.073    0.049    1.490    0.136\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .T0                0.696    0.109    6.385    0.000\n   .T1                1.014    0.081   12.469    0.000\n   .T2                1.138    0.115    9.907    0.000\n   .T3                1.092    0.215    5.083    0.000\n    i                 1.206    0.128    9.406    0.000\n    s                 1.025    0.079   13.010    0.000"
  },
  {
    "objectID": "posts/Latent Growth Models/index.html#reading-through-the-output",
    "href": "posts/Latent Growth Models/index.html#reading-through-the-output",
    "title": "Latent Growth Models",
    "section": "Reading through the output",
    "text": "Reading through the output"
  },
  {
    "objectID": "posts/Latent Growth Models/index.html#reading-through-the-output-1",
    "href": "posts/Latent Growth Models/index.html#reading-through-the-output-1",
    "title": "Latent Growth Models",
    "section": "Reading through the output",
    "text": "Reading through the output\n\ni represents the mean of intercepts for all our participants at T0.\ns represents the mean of the slope, as we move from each measurement time point to the next we should observe an increase of 0.051\nFor example, we would expect the slope at the last time point to be:\ns = -0.038 + 3x0.051"
  },
  {
    "objectID": "posts/Latent Growth Models/index.html#reading-through-the-output-2",
    "href": "posts/Latent Growth Models/index.html#reading-through-the-output-2",
    "title": "Latent Growth Models",
    "section": "Reading through the output",
    "text": "Reading through the output\n\nRemember dots denote residuals. i and s do not have a dot in from of them so these values denote variances."
  },
  {
    "objectID": "posts/Latent Growth Models/index.html#technically-this-is-the-full-model",
    "href": "posts/Latent Growth Models/index.html#technically-this-is-the-full-model",
    "title": "Latent Growth Models",
    "section": "Technically, this is the full model",
    "text": "Technically, this is the full model"
  },
  {
    "objectID": "posts/Latent Growth Models/index.html#getting-an-even-better-look",
    "href": "posts/Latent Growth Models/index.html#getting-an-even-better-look",
    "title": "Latent Growth Models",
    "section": "Getting an even better look",
    "text": "Getting an even better look\n\n\nCode\nsummary(fit1, standardized=TRUE, fit.measures=TRUE)\n\n\nlavaan 0.6.17 ended normally after 26 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                                 2.409\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.790\n\nModel Test Baseline Model:\n\n  Test statistic                              1115.899\n  Degrees of freedom                                 6\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.003\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3767.597\n  Loglikelihood unrestricted model (H1)      -3766.393\n                                                      \n  Akaike (AIC)                                7553.195\n  Bayesian (BIC)                              7591.126\n  Sample-size adjusted Bayesian (SABIC)       7562.560\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.040\n  P-value H_0: RMSEA &lt;= 0.050                    0.977\n  P-value H_0: RMSEA &gt;= 0.080                    0.001\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.008\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i =~                                                                  \n    T0                1.000                               1.098    0.796\n    T1                1.000                               1.098    0.629\n    T2                1.000                               1.098    0.447\n    T3                1.000                               1.098    0.332\n  s =~                                                                  \n    T0                0.000                               0.000    0.000\n    T1                1.000                               1.012    0.580\n    T2                2.000                               2.025    0.823\n    T3                3.000                               3.037    0.919\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i ~~                                                                  \n    s                -0.099    0.073   -1.362    0.173   -0.089   -0.089\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    i                -0.071    0.059   -1.195    0.232   -0.064   -0.064\n    s                 0.073    0.049    1.490    0.136    0.072    0.072\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .T0                0.696    0.109    6.385    0.000    0.696    0.366\n   .T1                1.014    0.081   12.469    0.000    1.014    0.333\n   .T2                1.138    0.115    9.907    0.000    1.138    0.188\n   .T3                1.092    0.215    5.083    0.000    1.092    0.100\n    i                 1.206    0.128    9.406    0.000    1.000    1.000\n    s                 1.025    0.079   13.010    0.000    1.000    1.000"
  },
  {
    "objectID": "posts/Latent Growth Models/index.html#looking-at-residuals",
    "href": "posts/Latent Growth Models/index.html#looking-at-residuals",
    "title": "Latent Growth Models",
    "section": "Looking at residuals",
    "text": "Looking at residuals\nWe can look at residuals to acquire more information about specific predictors.\n\n\nCode\nresiduals(fit1, type=\"cor\")\n\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n       T0     T1     T2     T3\nT0  0.000                     \nT1  0.005  0.000              \nT2 -0.005 -0.003  0.000       \nT3  0.004 -0.001  0.002  0.000\n\n$mean\n    T0     T1     T2     T3 \n 0.009 -0.001 -0.024  0.011"
  },
  {
    "objectID": "posts/Latent Growth Models/index.html#looking-at-modification-indices",
    "href": "posts/Latent Growth Models/index.html#looking-at-modification-indices",
    "title": "Latent Growth Models",
    "section": "Looking at modification indices",
    "text": "Looking at modification indices\nEven though you can acquire modification indices be very mindful on what these might mean about your model. In LGM our predictors are more than just items. Our main goal is to investigate how our measurements change across time points.\n\n\nCode\nmodificationindices(fit1)\n\n\n   lhs op rhs    mi    epc sepc.lv sepc.all sepc.nox\n1    i =~  T0 0.000  0.001   0.001    0.001    0.001\n2    i =~  T1 0.057  0.024   0.026    0.015    0.015\n3    i =~  T2 0.094 -0.019  -0.021   -0.009   -0.009\n4    i =~  T3 0.074  0.030   0.033    0.010    0.010\n5    s =~  T0 0.043  0.020   0.021    0.015    0.015\n6    s =~  T1 0.025 -0.009  -0.009   -0.005   -0.005\n7    s =~  T2 0.005 -0.005  -0.006   -0.002   -0.002\n8    s =~  T3 0.040  0.027   0.027    0.008    0.008\n16  T0 ~1     0.487  0.055   0.055    0.040    0.040\n17  T1 ~1     0.001 -0.002  -0.002   -0.001   -0.001\n18  T2 ~1     2.044 -0.081  -0.081   -0.033   -0.033\n19  T3 ~1     2.053  0.120   0.120    0.036    0.036\n22  T0 ~~  T1 0.196  0.070   0.070    0.083    0.083\n23  T0 ~~  T2 0.134 -0.026  -0.026   -0.029   -0.029\n24  T0 ~~  T3 0.155  0.052   0.052    0.060    0.060\n25  T1 ~~  T2 0.005 -0.006  -0.006   -0.006   -0.006\n26  T1 ~~  T3 0.001 -0.004  -0.004   -0.004   -0.004\n27  T2 ~~  T3 0.020  0.048   0.048    0.043    0.043"
  },
  {
    "objectID": "posts/Latent Growth Models/index.html#constraining-our-time-point-measurements",
    "href": "posts/Latent Growth Models/index.html#constraining-our-time-point-measurements",
    "title": "Latent Growth Models",
    "section": "Constraining our Time Point Measurements",
    "text": "Constraining our Time Point Measurements\nWe can leave our observed residual variances free to be estimated but constrain all of them to be the same.\nDoes this ring a bell in terms of hierarchical linear models?\n\n\nCode\nlgm_model2 &lt;- '\ni =~ 1*T0 + 1*T1 +1*T2 +1*T3\ns =~ 0*T0 + 1*T1 +2*T2 +3*T3\nT0 ~~ a*T0\nT1 ~~ a*T1\nT2 ~~ a*T2\nT3 ~~ a*T3\n'\nfit2 &lt;- growth(lgm_model2, data = simulated_data1)\nsummary(fit2, standardized=TRUE, fit.measures=TRUE)\n\n\nlavaan 0.6.17 ended normally after 21 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n  Number of equality constraints                     3\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                                12.248\n  Degrees of freedom                                 8\n  P-value (Chi-square)                           0.140\n\nModel Test Baseline Model:\n\n  Test statistic                              1115.899\n  Degrees of freedom                                 6\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.996\n  Tucker-Lewis Index (TLI)                       0.997\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3772.517\n  Loglikelihood unrestricted model (H1)      -3766.393\n                                                      \n  Akaike (AIC)                                7557.034\n  Bayesian (BIC)                              7582.321\n  Sample-size adjusted Bayesian (SABIC)       7563.277\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.033\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.067\n  P-value H_0: RMSEA &lt;= 0.050                    0.766\n  P-value H_0: RMSEA &gt;= 0.080                    0.008\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.028\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i =~                                                                  \n    T0                1.000                               1.024    0.712\n    T1                1.000                               1.024    0.590\n    T2                1.000                               1.024    0.420\n    T3                1.000                               1.024    0.310\n  s =~                                                                  \n    T0                0.000                               0.000    0.000\n    T1                1.000                               1.002    0.578\n    T2                2.000                               2.004    0.822\n    T3                3.000                               3.006    0.910\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i ~~                                                                  \n    s                -0.034    0.068   -0.499    0.618   -0.033   -0.033\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    i                -0.076    0.059   -1.275    0.202   -0.074   -0.074\n    s                 0.075    0.049    1.525    0.127    0.075    0.075\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .T0         (a)    1.022    0.046   22.361    0.000    1.022    0.494\n   .T1         (a)    1.022    0.046   22.361    0.000    1.022    0.340\n   .T2         (a)    1.022    0.046   22.361    0.000    1.022    0.172\n   .T3         (a)    1.022    0.046   22.361    0.000    1.022    0.094\n    i                 1.048    0.116    9.033    0.000    1.000    1.000\n    s                 1.004    0.077   13.045    0.000    1.000    1.000"
  },
  {
    "objectID": "posts/Latent Growth Models/index.html#is-this-a-better-model-or-the-same",
    "href": "posts/Latent Growth Models/index.html#is-this-a-better-model-or-the-same",
    "title": "Latent Growth Models",
    "section": "Is this a better model? Or the same?",
    "text": "Is this a better model? Or the same?\nBased on the output above is model2 better than model1?"
  },
  {
    "objectID": "posts/Latent Growth Models/index.html#visualization",
    "href": "posts/Latent Growth Models/index.html#visualization",
    "title": "Latent Growth Models",
    "section": "Visualization",
    "text": "Visualization\nTypically you could acquire this at the start so you can get a feel of your data.\n\n\nCode\nlibrary(lcsm)\n\nplot_trajectories(data = simulated_data1,\n                  id_var = \"id\", \n                  var_list = c(\"T0\", \"T1\", \"T2\", \"T3\"),\n                  xlab = \"Time points T0-T3\",\n                  ylab = \"Measurement\",\n                  line_colour = \"black\")"
  },
  {
    "objectID": "posts/Latent Growth Models/index.html#can-we-also-add-a-predictor-to-your-model",
    "href": "posts/Latent Growth Models/index.html#can-we-also-add-a-predictor-to-your-model",
    "title": "Latent Growth Models",
    "section": "Can we also add a predictor to your model?",
    "text": "Can we also add a predictor to your model?\nLet us consider the research question that gender can predict differences in the slope and intercept latent variables. In other words, I want to explore whether there are gender differences in our longitudinal measurement.\nBefore we move on I want you to think now in terms of endogenous and exogenous variables."
  },
  {
    "objectID": "posts/Latent Growth Models/index.html#coding-a-predictor-in-our-model",
    "href": "posts/Latent Growth Models/index.html#coding-a-predictor-in-our-model",
    "title": "Latent Growth Models",
    "section": "Coding a predictor in our model",
    "text": "Coding a predictor in our model\n\n\nCode\nlgm_model3 &lt;- '\ni =~ 1*T0 + 1*T1 +1*T2 +1*T3\ns =~ 0*T0 + 1*T1 +2*T2 +3*T3\ni ~ gender\ns ~ gender\nT0 ~~ a*T0\nT1 ~~ a*T1\nT2 ~~ a*T2\nT3 ~~ a*T3\n'\nfit3 &lt;- growth(lgm_model3, data = simulated_data1)\nsummary(fit3, standardized=TRUE, fit.measures=TRUE)\n\n\nlavaan 0.6.17 ended normally after 33 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        11\n  Number of equality constraints                     3\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                                14.742\n  Degrees of freedom                                10\n  P-value (Chi-square)                           0.142\n\nModel Test Baseline Model:\n\n  Test statistic                              1122.538\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.996\n  Tucker-Lewis Index (TLI)                       0.996\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3770.444\n  Loglikelihood unrestricted model (H1)      -3763.073\n                                                      \n  Akaike (AIC)                                7556.888\n  Bayesian (BIC)                              7590.605\n  Sample-size adjusted Bayesian (SABIC)       7565.213\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.031\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.062\n  P-value H_0: RMSEA &lt;= 0.050                    0.823\n  P-value H_0: RMSEA &gt;= 0.080                    0.003\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.024\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i =~                                                                  \n    T0                1.000                               1.024    0.712\n    T1                1.000                               1.024    0.590\n    T2                1.000                               1.024    0.420\n    T3                1.000                               1.024    0.310\n  s =~                                                                  \n    T0                0.000                               0.000    0.000\n    T1                1.000                               1.002    0.578\n    T2                2.000                               2.004    0.822\n    T3                3.000                               3.006    0.910\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i ~                                                                   \n    gender           -0.061    0.119   -0.512    0.609   -0.059   -0.030\n  s ~                                                                   \n    gender           -0.176    0.098   -1.798    0.072   -0.176   -0.088\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .i ~~                                                                  \n   .s                -0.037    0.068   -0.540    0.589   -0.036   -0.036\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .i                 0.017    0.191    0.090    0.928    0.017    0.017\n   .s                 0.344    0.158    2.185    0.029    0.344    0.344\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .T0         (a)    1.022    0.046   22.361    0.000    1.022    0.494\n   .T1         (a)    1.022    0.046   22.361    0.000    1.022    0.340\n   .T2         (a)    1.022    0.046   22.361    0.000    1.022    0.172\n   .T3         (a)    1.022    0.046   22.361    0.000    1.022    0.094\n   .i                 1.047    0.116    9.029    0.000    0.999    0.999\n   .s                 0.996    0.076   13.026    0.000    0.992    0.992"
  },
  {
    "objectID": "posts/Latent Growth Models/index.html#this-weeks-exerciseschallenges",
    "href": "posts/Latent Growth Models/index.html#this-weeks-exerciseschallenges",
    "title": "Latent Growth Models",
    "section": "This week’s exercises/challenges",
    "text": "This week’s exercises/challenges\nExercise 1:\nI want you to specify the mode3 in lavaan but write all the necessary components in order to run it using the cfa() instead of growth().\nIn other words think in terms of specifying, fixing, and freeing parameters as we would have done in a CFAt in order to acquire the exact same values in your output. You can find the relevant project in our Posit Cloud under the name Week 6 exercise.\nExercise 2:\nI want you to modify model1 in order to test for a quadratic projection instead of a linear one. Think in terms of how a quadratic relation works and amend the slope accordingly."
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#todays-aims",
    "href": "posts/Meta Analysis I/index.html#todays-aims",
    "title": "Meta-Analysis I",
    "section": "Today’s Aims",
    "text": "Today’s Aims\nToday we will discuss meta-analysis as a statistical technique to synthesize results from different studies.\nThe main aim of a meta-analysis is to assess the evidence as presented across a number of similar studies. This assessment can help us establish whether there is a true effect, especially in the case of conflicting findings."
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#what-can-meta-analysis-tell-us",
    "href": "posts/Meta Analysis I/index.html#what-can-meta-analysis-tell-us",
    "title": "Meta-Analysis I",
    "section": "What can meta-analysis tell us?",
    "text": "What can meta-analysis tell us?\nAs mentioned in, Field, A. P., & Gillett, R. (2010). How to do a meta‐analysis. British Journal of Mathematical and Statistical Psychology, 63(3), 665-694.\n\nMean and variance of underlying population effects\nVariability in effects across studies\nModerator variables"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#strengths-of-meta-analysis",
    "href": "posts/Meta Analysis I/index.html#strengths-of-meta-analysis",
    "title": "Meta-Analysis I",
    "section": "Strengths of meta-analysis",
    "text": "Strengths of meta-analysis\n\nMeta-analysis can go beyond the answers provided in individual studies\nIt can also improve precision of findings as it includes more information\nStatistically, we have increased power in a meta-analysis compared to individual studies\nAs mentioned above, we can shed more light in the case of conflicting results\nMakes is possible to compare subgroups\n\nActually plenty of other strenghts …"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#weaknesses-of-meta-analysis",
    "href": "posts/Meta Analysis I/index.html#weaknesses-of-meta-analysis",
    "title": "Meta-Analysis I",
    "section": "Weaknesses of meta-analysis",
    "text": "Weaknesses of meta-analysis\n\nPublication bias, meta-analysis is primarily relying on published results\nSmall studies vs large studies\nHeterogeneity (we will discuss this in greater detail)\nFixed vs Random Effect (we will discuss this in greater detail)\n\nActually plenty of other limitations …"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#carrying-out-meta-analysis-is-simple-right",
    "href": "posts/Meta Analysis I/index.html#carrying-out-meta-analysis-is-simple-right",
    "title": "Meta-Analysis I",
    "section": "Carrying out meta-analysis is simple, right?",
    "text": "Carrying out meta-analysis is simple, right?"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#well-let-us-see-a-published-example",
    "href": "posts/Meta Analysis I/index.html#well-let-us-see-a-published-example",
    "title": "Meta-Analysis I",
    "section": "Well let us see a published example",
    "text": "Well let us see a published example"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#well-let-us-see-a-published-example-1",
    "href": "posts/Meta Analysis I/index.html#well-let-us-see-a-published-example-1",
    "title": "Meta-Analysis I",
    "section": "Well let us see a published example",
    "text": "Well let us see a published example"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#well-let-us-see-a-published-example-2",
    "href": "posts/Meta Analysis I/index.html#well-let-us-see-a-published-example-2",
    "title": "Meta-Analysis I",
    "section": "Well let us see a published example",
    "text": "Well let us see a published example"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#technically-it-is-simple-in-practice-it-is-not",
    "href": "posts/Meta Analysis I/index.html#technically-it-is-simple-in-practice-it-is-not",
    "title": "Meta-Analysis I",
    "section": "Technically it is simple, in practice it is not",
    "text": "Technically it is simple, in practice it is not\n\nWe should carry out a literature review\nWe should have a research question/hypothesis (hmm not before the literature review?)\nWe should have a list of inclusion/exclusion criteria\nWe should then carry out a literature review (again?)\nCollate statistical findings, calculate effect sizes\n\nCarry out the statistical analysis (wait, is the calculation of effect sizes not part of this?). Also referred to as basic meta-analysis\nYou might also want to carry out more advanced statistical analysis e.g., moderator analysis\n\nReport your findings"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#in-practice-it-is-more-complex",
    "href": "posts/Meta Analysis I/index.html#in-practice-it-is-more-complex",
    "title": "Meta-Analysis I",
    "section": "In practice it is more complex",
    "text": "In practice it is more complex\nIn practice, all of the above steps require extensive reading and clear decision making based on sound understanding of your research field and the statistics behind effect sizes and meta-analysis."
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#first-thing-first-well-not-really",
    "href": "posts/Meta Analysis I/index.html#first-thing-first-well-not-really",
    "title": "Meta-Analysis I",
    "section": "First thing first, well not really!",
    "text": "First thing first, well not really!\nToday we will start from step 5. Collate statistical findings, calculate effect sizes.\nEffect Size\nQuite often, the effect size is reported and can be directly extracted, not always though. In order to perform a meta-analysis we need effect sizes and we have to choose wisely as they can affect our results.\n\nComputable\nComparable\nReliable\nInterpretable\n\nFor more details you can see Higgins, Julian, James Thomas, Jacqueline Chandler, Miranda Cumpston, Tianjing Li, Matthew J Page, and Vivian A Welch. 2019. Cochrane Handbook for Systematic Reviews of Interventions. John Wiley & Sons."
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#so-what-is-effect-size",
    "href": "posts/Meta Analysis I/index.html#so-what-is-effect-size",
    "title": "Meta-Analysis I",
    "section": "So what is effect size?",
    "text": "So what is effect size?\nYou most probably have come across the term effect size in the past, in general terms it is a metric of the magnitude of the difference or relationship between variables. A larger effect size denotes a more meaningful difference/relationship between variables. If we consider two arithmetic means \\(\\bar{x}_1\\) and \\(\\bar{x}_2\\) then we can use Cohen’s d effect size.\nCohen’s d\n\\(d = \\frac{{\\bar{x}_1 - \\bar{x}_2}}{{s_p}}\\)\nwhere\n\\(\\bar{x}_1\\) and \\(\\bar{x}_2\\) are the mean of the two groups, and \\(s_p\\) is the pooled standard deviation defined as:\n\\(s_p = \\sqrt{\\frac{{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}}{{n_1 + n_2 - 2}}}\\)"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#a-cautionary-tale",
    "href": "posts/Meta Analysis I/index.html#a-cautionary-tale",
    "title": "Meta-Analysis I",
    "section": "A cautionary tale",
    "text": "A cautionary tale\nDo not be misguided by the word effect. There is no suggestion of causality hence you should not interpret it as such. Instead, you should treat effect size with caution.\nAlso, keep in mind that we will be working with measures of central tendency, so we need to be cautious and not automatically label everything as effect size. Furthermore, our observed effect size \\(\\hat{\\theta}_1\\) from a study 1 is only an estimate of the true effect size \\(\\theta\\). Therefore, it is subdue to sampling error:\n\\(\\hat{\\theta}_i= {\\theta}_i + {\\epsilon}_i\\)"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#we-can-see-a-very-basic-example-here",
    "href": "posts/Meta Analysis I/index.html#we-can-see-a-very-basic-example-here",
    "title": "Meta-Analysis I",
    "section": "We can see a very basic example here",
    "text": "We can see a very basic example here\n\n\nCode\nset.seed(23234)\nsample1 &lt;- rnorm(n=250, mean = 100, sd = 15)\n\nmean(sample1)\n\n\n[1] 100.4061"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#we-can-see-a-very-basic-example-here-1",
    "href": "posts/Meta Analysis I/index.html#we-can-see-a-very-basic-example-here-1",
    "title": "Meta-Analysis I",
    "section": "We can see a very basic example here",
    "text": "We can see a very basic example here\nWe can now repeat this process 1,000 times and calculate the standard deviation of this sampling distribution. This is our standard error of the mean.\n\n\nCode\nsamplemean &lt;- data.frame(m=1:1000,s=1:1000)\nfor (i in 1:1000) {\n  sample1 &lt;- rnorm(n=250, mean = 100, sd = 15)\n  samplemean$m[i] &lt;- mean(sample1)\n  samplemean$s[i] &lt;- sd(sample1)\n}\nhist(samplemean$m, breaks = 20)"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#standard-error-of-the-mean",
    "href": "posts/Meta Analysis I/index.html#standard-error-of-the-mean",
    "title": "Meta-Analysis I",
    "section": "Standard error of the mean",
    "text": "Standard error of the mean\n\\(SE=\\frac{s}{\\sqrt{n}}\\)\n\n\nCode\nsd(samplemean$m)\n\n\n[1] 0.9708343\n\n\nCode\nsd(sample1)/sqrt(250)\n\n\n[1] 1.041905\n\n\nWe can observe that the two values are similar but not quite the same."
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#standard-error-of-the-mean-1",
    "href": "posts/Meta Analysis I/index.html#standard-error-of-the-mean-1",
    "title": "Meta-Analysis I",
    "section": "Standard error of the mean",
    "text": "Standard error of the mean\n\\(SE=\\frac{s}{\\sqrt{n}}\\) We can also see that our value will depend on the sample size \\(n\\). The larger our sample size, the larger our denominator will be, hence the smaller our standard error.\n\n\nCode\nsamplemean2 &lt;- data.frame(n=1:999, m=1:999,se=1:999)\nfor (i in 2:1000) {\n  sample1 &lt;- rnorm(n=i, mean = 100, sd = 15)\n  samplemean2$n[i-1] &lt;- i\n  samplemean2$m[i-1] &lt;- mean(sample1)\n  samplemean2$se[i-1] &lt;- sd(sample1)/sqrt(i)\n}\nlibrary(ggplot2)\nggplot(samplemean2, aes(x=n, y=m))+\n  geom_line()+\n  labs(x = \"Sample Size\")+\n  labs(y = \"Mean\")"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#standard-error-of-the-mean-2",
    "href": "posts/Meta Analysis I/index.html#standard-error-of-the-mean-2",
    "title": "Meta-Analysis I",
    "section": "Standard error of the mean",
    "text": "Standard error of the mean\n\n\nCode\nggplot(samplemean2, aes(x=n, y=se))+\n  geom_line()+\n  labs(x = \"Sample Size\")+\n  labs(y = \"Standard Error\")\n\n\n\nWhat are the implications of these two plots on the Cohen’s d?\n\\(d = \\frac{{\\bar{x}_1 - \\bar{x}_2}}{{s_p}}\\)\n\\(s_p = \\sqrt{\\frac{{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}}{{n_1 + n_2 - 2}}}\\)\nRequired figures: sample size, mean, and standard deviation of each study"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#proportions-and-effect-size",
    "href": "posts/Meta Analysis I/index.html#proportions-and-effect-size",
    "title": "Meta-Analysis I",
    "section": "Proportions and effect size",
    "text": "Proportions and effect size\nBesides arithmetic means as measures of central tendency we may also work with paradigms using proportions. In order to calculate a proportion we need to divide the number of observation \\(j\\) that have a characteristic by the total sample size \\(n\\)\n\\(p=\\frac{j}{n}\\)\n\\(SE_p=\\sqrt{\\frac{p(1-p)}{n}}\\)"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#proportions-and-effect-size-1",
    "href": "posts/Meta Analysis I/index.html#proportions-and-effect-size-1",
    "title": "Meta-Analysis I",
    "section": "Proportions and effect size",
    "text": "Proportions and effect size\nLet us see a basic example. We have sample of 250 observations with 100 of them exhibiting a specific behaviour.\n\n\nCode\nn &lt;- 250\nj &lt;- 100\np &lt;- j/n\nse &lt;- sqrt((p*(1-p))/n)\np\n\n\n[1] 0.4\n\n\nCode\nse\n\n\n[1] 0.03098387\n\n\nProportions can be problematic in terms of standard error as they can only vary between 0 and 1. Can you think of the reason why?"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#transforming-proportions",
    "href": "posts/Meta Analysis I/index.html#transforming-proportions",
    "title": "Meta-Analysis I",
    "section": "Transforming proportions",
    "text": "Transforming proportions\nIn order to avoid this limitation we logit-transform proportions after we calculate the odds. The formulae for this process are:\n\\(p_{logit}=log_e(\\frac{p}{1-p})\\)\n\\(SE_{plogit}=\\sqrt{{\\frac{1}{np}}+\\frac{1}{n(1-p)}}\\)\nRequired figures: Sample size and the number of observations with a specific characteristic"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#correlations",
    "href": "posts/Meta Analysis I/index.html#correlations",
    "title": "Meta-Analysis I",
    "section": "Correlations",
    "text": "Correlations\nThe correlation expresses the amount of covariance between two variables, the most commonly used is the Pearson correlation (for continuous variables).\n\\(r_{xy}=\\frac{\\sigma^2_{xy}}{\\sigma_x\\sigma_y}\\)\n\\(SE_{r_{xy}}=\\frac{1-r^2_{xy}}{\\sqrt{n-2}}\\)\nReminder 1: this is a standardized score which means we can immediately compare correlations between variables that are not necessarily on the same scale.\nReminder 2: correlations are scores that naturally vary between -1 and 1. Does this bring in mind any potential problems?"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#fishers-z-transformation-of-correlations",
    "href": "posts/Meta Analysis I/index.html#fishers-z-transformation-of-correlations",
    "title": "Meta-Analysis I",
    "section": "Fisher’s \\(z\\) transformation of correlations",
    "text": "Fisher’s \\(z\\) transformation of correlations\n\\(z=0.5log_e(\\frac{1+r}{1-r})\\)\n\\(SE_z=\\frac{1}{\\sqrt{n-3}}\\)\nRequired figures: correlation coefficient and sample size"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#more-experimental-paradigms-and-effect-size-corrections",
    "href": "posts/Meta Analysis I/index.html#more-experimental-paradigms-and-effect-size-corrections",
    "title": "Meta-Analysis I",
    "section": "More experimental paradigms and Effect size corrections",
    "text": "More experimental paradigms and Effect size corrections\nNext week we will present more cases experimental designs and how we can calculate SE and effect sizes. Furthermore, we will discuss how we can correct our effect sizes to account for systematic biases such as small sizes."
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#fixed-effect-vs-random-effects-model",
    "href": "posts/Meta Analysis I/index.html#fixed-effect-vs-random-effects-model",
    "title": "Meta-Analysis I",
    "section": "Fixed-Effect vs Random-Effects Model",
    "text": "Fixed-Effect vs Random-Effects Model\nTypically in statistical models we work with either fixed-effect or random-effect models.\nFixed-effect models assume that all the effect sizes in our studies are derived from a homogeneous population with a fixed effect size. This means that our sample effect sizes are also homogeneous.\nRandom-effect models assume our population effect size varies randomly between our studies, therefore we should not assume that our sample sizes are homogeneous.\nIn practice, fixed-effect models have one error term and random-effect models have two error terms."
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#fixed-effect-vs-random-effects-model-1",
    "href": "posts/Meta Analysis I/index.html#fixed-effect-vs-random-effects-model-1",
    "title": "Meta-Analysis I",
    "section": "Fixed-Effect vs Random-Effects Model",
    "text": "Fixed-Effect vs Random-Effects Model\nIn our fixed-effect model our observed effect size in all studies deviates from the true effect size due to sampling error.\n\\(\\hat{\\theta}_i= \\theta + {\\epsilon}_i\\)\nIn our random-effect model our study’s observed effect size deviates from our study’s true effect size due to sampling error.\n\\(\\hat{\\theta}_i= \\theta_i + {\\epsilon}_i\\)\nThere is however a second error, that of our study’s true effect size deviating from the overarching distribution of effect sizes with a mean score of \\(\\mu\\)\n\\(\\theta_i= \\mu + {\\zeta}_i\\)\nTherefore, our study’s effect size deviates from the overall \\(\\mu\\) by two error terms.\n\\(\\hat{\\theta}_i= \\mu + {\\zeta}_i+ {\\epsilon}_i\\)"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#exchangeability-assumption-of-random-effects-model",
    "href": "posts/Meta Analysis I/index.html#exchangeability-assumption-of-random-effects-model",
    "title": "Meta-Analysis I",
    "section": "Exchangeability assumption of random-effects model",
    "text": "Exchangeability assumption of random-effects model\nThe exchangeability assumption in random-effects model dictates that our \\({\\zeta}_i\\) error from the superpopulation mean \\(\\mu\\) should be independent of \\(i\\). In other words, we cannot know in advance if \\({\\zeta}_i\\) in one study is going to be larger or smaller that the \\({\\zeta}_j\\) in another study. What does this mean for the true effect sizes for each of the study?"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#thanks-for-the-headache-but-which-model-should-i-use",
    "href": "posts/Meta Analysis I/index.html#thanks-for-the-headache-but-which-model-should-i-use",
    "title": "Meta-Analysis I",
    "section": "Thanks for the headache but which model should I use?",
    "text": "Thanks for the headache but which model should I use?\nWell as usual I have the perfect answer for you, can you guess it?\nYou can get more information on the debate on which method is more suitable for different cases here:\nhttps://journals.sagepub.com/doi/10.1177/25152459221120427\nOther authors also suggest that random-effects should be the go-to approach:\nhttps://jamiefield.github.io/files/Field%20and%20Gillett%20(2010).%20How%20to%20do%20a%20meta-analysis..pdf"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#between-study-heterogeneity",
    "href": "posts/Meta Analysis I/index.html#between-study-heterogeneity",
    "title": "Meta-Analysis I",
    "section": "Between-Study Heterogeneity",
    "text": "Between-Study Heterogeneity\nAssuming we decided to go for the random-effects model approach it instantly becomes apparent that we are facing the challenge of accounting for the \\({\\zeta}_i\\) error.\nWe will address this by estimating the variance of the distribution of true effect sizes, also known as \\(\\tau^2\\) (tau-squared). This also a measure of heterogeneity among the true effect sizes (between studies variance).\nThere are multiple estimators that you can use, DerSimonian-Laird (\"DL\"), Restricted Maximum Likelihood (\"REML\"), Maximum Likelihood (\"ML\"), Empirical Bayes (\"EB\"), The Sidik-Jonkman (\"SJ\")  are only some examples.\nFor more information you can read:\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC4950030/"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#which-estimator-should-we-use",
    "href": "posts/Meta Analysis I/index.html#which-estimator-should-we-use",
    "title": "Meta-Analysis I",
    "section": "Which estimator should we use?",
    "text": "Which estimator should we use?\nAs a rule of thumb, if you have continuous outcomes you can use maximum likelihood.\nIf you are concerned that you have very high heterogeneity you can use Sidik-Jonkman\nIt has also been suggested that DerSimonian-Laird allows for more reproducible findings. This is because this estimator uses a formula contrary to ML that uses an iterative algorithm."
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#which-package-should-i-use",
    "href": "posts/Meta Analysis I/index.html#which-package-should-i-use",
    "title": "Meta-Analysis I",
    "section": "Which package should I use?",
    "text": "Which package should I use?\nWe will be mainly using metafor although you can also use meta if you want.\nWe will also use metadat package that has datasets for education purposes specifically for meta-analysis.\nThere are also other packages which I have not used so I will be quite limited in supporting you with other packages."
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#lets-work-through-an-example",
    "href": "posts/Meta Analysis I/index.html#lets-work-through-an-example",
    "title": "Meta-Analysis I",
    "section": "Lets work through an example",
    "text": "Lets work through an example\n\n\nCode\nlibrary(tidyverse)\nlibrary(metafor)\nlibrary(metadat)\n\ndf &lt;- get(data(dat.molloy2014))\n\nhead(df)\n\n\n              authors year  ni    ri controls          design   a_measure\n1     Axelsson et al. 2009 109 0.187     none cross-sectional self-report\n2     Axelsson et al. 2011 749 0.162     none cross-sectional self-report\n3        Bruce et al. 2010  55 0.340     none     prospective       other\n4  Christensen et al. 1999 107 0.320     none cross-sectional self-report\n5 Christensen & Smith 1995  72 0.270     none     prospective       other\n6        Cohen et al. 2004  65 0.000     none     prospective       other\n  c_measure meanage quality\n1     other   22.00       1\n2       NEO   53.59       1\n3       NEO   43.36       2\n4     other   41.70       1\n5       NEO   46.39       2\n6       NEO   41.20       2"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#keep-only-the-relevant-variables-and-add-id",
    "href": "posts/Meta Analysis I/index.html#keep-only-the-relevant-variables-and-add-id",
    "title": "Meta-Analysis I",
    "section": "Keep only the relevant variables and add id",
    "text": "Keep only the relevant variables and add id\n\n\nCode\ndf &lt;- df |&gt; \n  select(1:6)\ndf &lt;- df |&gt; \n  mutate(id=row_number())\nhead(df)\n\n\n              authors year  ni    ri controls          design id\n1     Axelsson et al. 2009 109 0.187     none cross-sectional  1\n2     Axelsson et al. 2011 749 0.162     none cross-sectional  2\n3        Bruce et al. 2010  55 0.340     none     prospective  3\n4  Christensen et al. 1999 107 0.320     none cross-sectional  4\n5 Christensen & Smith 1995  72 0.270     none     prospective  5\n6        Cohen et al. 2004  65 0.000     none     prospective  6"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#remember-pearsons-correlation-coefficient-limitation",
    "href": "posts/Meta Analysis I/index.html#remember-pearsons-correlation-coefficient-limitation",
    "title": "Meta-Analysis I",
    "section": "Remember Pearson’s correlation coefficient limitation?",
    "text": "Remember Pearson’s correlation coefficient limitation?\nWe will transform to FIscher’s \\(z\\)\n\n\nCode\ndf &lt;- escalc(measure = \"ZCOR\", ri=ri, ni=ni, data=df, slab = paste(authors, year, sep=\",\"))\n\nhead(df)\n\n\n\n              authors year  ni    ri controls          design id     yi     vi \n1     Axelsson et al. 2009 109 0.187     none cross-sectional  1 0.1892 0.0094 \n2     Axelsson et al. 2011 749 0.162     none cross-sectional  2 0.1634 0.0013 \n3        Bruce et al. 2010  55 0.340     none     prospective  3 0.3541 0.0192 \n4  Christensen et al. 1999 107 0.320     none cross-sectional  4 0.3316 0.0096 \n5 Christensen & Smith 1995  72 0.270     none     prospective  5 0.2769 0.0145 \n6        Cohen et al. 2004  65 0.000     none     prospective  6 0.0000 0.0161"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#random-effects-size",
    "href": "posts/Meta Analysis I/index.html#random-effects-size",
    "title": "Meta-Analysis I",
    "section": "Random-Effects Size",
    "text": "Random-Effects Size\n\n\nCode\nma.res &lt;- rma(yi, vi, data=df)\nma.res\n\n\n\nRandom-Effects Model (k = 16; tau^2 estimator: REML)\n\ntau^2 (estimated amount of total heterogeneity): 0.0081 (SE = 0.0055)\ntau (square root of estimated tau^2 value):      0.0901\nI^2 (total heterogeneity / total variability):   61.73%\nH^2 (total variability / sampling variability):  2.61\n\nTest for Heterogeneity:\nQ(df = 15) = 38.1595, p-val = 0.0009\n\nModel Results:\n\nestimate      se    zval    pval   ci.lb   ci.ub      \n  0.1499  0.0316  4.7501  &lt;.0001  0.0881  0.2118  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#prediction-and-confidence-intervals",
    "href": "posts/Meta Analysis I/index.html#prediction-and-confidence-intervals",
    "title": "Meta-Analysis I",
    "section": "Prediction and confidence intervals",
    "text": "Prediction and confidence intervals\n\n\nCode\npredict(ma.res)\n\n\n\n   pred     se  ci.lb  ci.ub   pi.lb  pi.ub \n 0.1499 0.0316 0.0881 0.2118 -0.0371 0.3370 \n\n\nCode\nconfint(ma.res)\n\n\n\n       estimate   ci.lb   ci.ub \ntau^2    0.0081  0.0017  0.0378 \ntau      0.0901  0.0412  0.1944 \nI^2(%)  61.7324 25.2799 88.2545 \nH^2      2.6132  1.3383  8.5139"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#exploring-for-influencial-studies",
    "href": "posts/Meta Analysis I/index.html#exploring-for-influencial-studies",
    "title": "Meta-Analysis I",
    "section": "Exploring for influencial studies",
    "text": "Exploring for influencial studies\n\n\nCode\nma.inf &lt;- influence(ma.res)\nprint(ma.inf)\n\n\n\n                         rstudent  dffits cook.d  cov.r tau2.del  QE.del    hat \nAxelsson et al.,2009       0.2918  0.0485 0.0025 1.1331   0.0091 37.7109 0.0568 \nAxelsson et al.,2011       0.1196 -0.0031 0.0000 1.2595   0.0100 36.7672 0.1054 \nBruce et al.,2010          1.2740  0.2595 0.0660 0.9942   0.0075 35.3930 0.0364 \nChristensen et al.,1999    1.4711  0.3946 0.1439 0.9544   0.0068 33.5886 0.0562 \nChristensen & Smith,1995   0.8622  0.1838 0.0339 1.0505   0.0082 36.5396 0.0441 \nCohen et al.,2004         -0.9795 -0.2121 0.0455 1.0639   0.0084 37.1703 0.0411 \nDobbels et al.,2005        0.2177  0.0296 0.0010 1.1740   0.0094 37.6797 0.0714 \nEdiger et al.,2007        -0.9774 -0.3120 0.1001 1.1215   0.0084 36.1484 0.0889 \nInsel et al.,2006          0.7264  0.1392 0.0195 1.0561   0.0083 37.0495 0.0379 \nJerant et al.,2011        -1.8667 -0.5861 0.2198 0.8502   0.0047 25.0661 0.1058 \nMoran et al.,1997         -1.4985 -0.2771 0.0756 1.0073   0.0077 35.6617 0.0369 \nO'Cleirigh et al.,2007     1.8776  0.4918 0.2148 0.8819   0.0059 31.9021 0.0511 \nPenedo et al.,2003        -1.1892 -0.2939 0.0859 1.0550   0.0080 36.3291 0.0587 \nQuine et al.,2012         -0.0020 -0.0423 0.0021 1.2524   0.0100 37.7339 0.0998 \nStilley et al.,2004        0.8066  0.2126 0.0459 1.0907   0.0083 35.8385 0.0684 \nWiebe & Christensen,1997  -0.7160 -0.1656 0.0280 1.0853   0.0087 37.7017 0.0411 \n                          weight    dfbs inf \nAxelsson et al.,2009      5.6776  0.0481     \nAxelsson et al.,2011     10.5396 -0.0032     \nBruce et al.,2010         3.6432  0.2623     \nChristensen et al.,1999   5.6195  0.3994     \nChristensen & Smith,1995  4.4069  0.1837     \nCohen et al.,2004         4.1094 -0.2112     \nDobbels et al.,2005       7.1362  0.0296     \nEdiger et al.,2007        8.8886 -0.3128     \nInsel et al.,2006         3.7886  0.1387     \nJerant et al.,2011       10.5826 -0.5430     \nMoran et al.,1997         3.6922 -0.2791     \nO'Cleirigh et al.,2007    5.1150  0.5059     \nPenedo et al.,2003        5.8732 -0.2941     \nQuine et al.,2012         9.9778 -0.0434     \nStilley et al.,2004       6.8403  0.2125     \nWiebe & Christensen,1997  4.1094 -0.1642"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#heterogeneity-variance-tau2",
    "href": "posts/Meta Analysis I/index.html#heterogeneity-variance-tau2",
    "title": "Meta-Analysis I",
    "section": "Heterogeneity Variance \\(\\tau^2\\)",
    "text": "Heterogeneity Variance \\(\\tau^2\\)\nThis gives us an estimate of the variance of the true effect sizes. Similarly to the variance when we square root it we get the standard deviation. Therefore, \\(\\tau\\) is the standard deviation of the true effect sizes.\nCan you think of an advantage of the \\(\\tau\\) that \\(\\tau^2\\) does not offer?"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#q-test",
    "href": "posts/Meta Analysis I/index.html#q-test",
    "title": "Meta-Analysis I",
    "section": "Q-test",
    "text": "Q-test\nQ approximates the chi-squared distribution with k-1 degrees of freedom. Significant Q-test denotes significant between-study heterogeneity.\nHowever, be very careful and do not rely exclusively on the Q-test. Its values increases as the number of studies increases as well as when the sample size of study increases.\nDo not decide between fixed and random effects model based on this result."
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#the-i2-higgins-and-thompson-statistic",
    "href": "posts/Meta Analysis I/index.html#the-i2-higgins-and-thompson-statistic",
    "title": "Meta-Analysis I",
    "section": "The \\(I^2\\) Higgins and Thompson Statistic",
    "text": "The \\(I^2\\) Higgins and Thompson Statistic\n\\(I^2=\\frac{Q-(k-1)}{Q}\\)\nWe can use the following rule of thumb:\n\n~25%, low heterogeneity\n~50% moderate heterogeneity\n~75% substantial heterogeneity\nWhat about negative values?"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#the-h2-higgins-and-thompson-statistic",
    "href": "posts/Meta Analysis I/index.html#the-h2-higgins-and-thompson-statistic",
    "title": "Meta-Analysis I",
    "section": "The \\(H^2\\) Higgins and Thompson Statistic",
    "text": "The \\(H^2\\) Higgins and Thompson Statistic\n\\(H^2=\\frac{Q}{K-1}\\)\nValues greater than 1 indicate the presence of between-study heterogeneity"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#so-which-measure-is-more-important",
    "href": "posts/Meta Analysis I/index.html#so-which-measure-is-more-important",
    "title": "Meta-Analysis I",
    "section": "So which measure is more important?",
    "text": "So which measure is more important?\nQ’s significance depends on the size of your pool. \\(I^2\\) does not suffer from this limitation but it is still affected by the precision of the included studies, larger sample sizes will result in sampling error that will be approaching zero. \\(H^2\\) shares similarities with \\(I^2\\). Finally, \\(\\tau^2\\) does not have any of these limitations, but it is hard to interpret it as there are no rules of thumb. The best approach would be to report at least \\(\\tau^2\\), \\(I^2\\), and \\(Q\\) and examine them in conjunction. You should also report confidence and prediction intervals."
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#plot",
    "href": "posts/Meta Analysis I/index.html#plot",
    "title": "Meta-Analysis I",
    "section": "Plot",
    "text": "Plot\n\n\nCode\nplot(ma.inf)"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#forest-plot",
    "href": "posts/Meta Analysis I/index.html#forest-plot",
    "title": "Meta-Analysis I",
    "section": "Forest Plot",
    "text": "Forest Plot\n\n\nCode\nforest(ma.res)"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#funnel-plot",
    "href": "posts/Meta Analysis I/index.html#funnel-plot",
    "title": "Meta-Analysis I",
    "section": "Funnel Plot",
    "text": "Funnel Plot\nPlease be aware that these are not publication bias tests!\n\n\nCode\nfunnel(ma.res)"
  },
  {
    "objectID": "posts/Meta Analysis I/index.html#small-study-bias",
    "href": "posts/Meta Analysis I/index.html#small-study-bias",
    "title": "Meta-Analysis I",
    "section": "Small Study Bias",
    "text": "Small Study Bias\n\n\nCode\nregtest(ma.res)\n\n\n\nRegression Test for Funnel Plot Asymmetry\n\nModel:     mixed-effects meta-regression model\nPredictor: standard error\n\nTest for Funnel Plot Asymmetry: z = 1.0216, p = 0.3070\nLimit Estimate (as sei -&gt; 0):   b = 0.0790 (CI: -0.0686, 0.2266)"
  },
  {
    "objectID": "posts/Meta Analysis II/index.html#metafor-diagram",
    "href": "posts/Meta Analysis II/index.html#metafor-diagram",
    "title": "Meta-Analysis II: Subgroup Analysis",
    "section": "metafor diagram",
    "text": "metafor diagram"
  },
  {
    "objectID": "posts/Meta Analysis II/index.html#lets-work-through-an-example",
    "href": "posts/Meta Analysis II/index.html#lets-work-through-an-example",
    "title": "Meta-Analysis II: Subgroup Analysis",
    "section": "Lets work through an example",
    "text": "Lets work through an example\n\n\nCode\nlibrary(tidyverse)\nlibrary(metafor)\nlibrary(metadat)\n\ndf &lt;- get(data(dat.molloy2014))\n\nhead(df)\n\n\n              authors year  ni    ri controls          design   a_measure\n1     Axelsson et al. 2009 109 0.187     none cross-sectional self-report\n2     Axelsson et al. 2011 749 0.162     none cross-sectional self-report\n3        Bruce et al. 2010  55 0.340     none     prospective       other\n4  Christensen et al. 1999 107 0.320     none cross-sectional self-report\n5 Christensen & Smith 1995  72 0.270     none     prospective       other\n6        Cohen et al. 2004  65 0.000     none     prospective       other\n  c_measure meanage quality\n1     other   22.00       1\n2       NEO   53.59       1\n3       NEO   43.36       2\n4     other   41.70       1\n5       NEO   46.39       2\n6       NEO   41.20       2\n\n\n\n\nCode\ndf &lt;- escalc(measure = \"ZCOR\", ri=ri, ni=ni, data=df, slab = paste(authors, year, sep=\",\"))\nma.res &lt;- rma(yi, vi, data=df)\nma.res\n\n\n\nRandom-Effects Model (k = 16; tau^2 estimator: REML)\n\ntau^2 (estimated amount of total heterogeneity): 0.0081 (SE = 0.0055)\ntau (square root of estimated tau^2 value):      0.0901\nI^2 (total heterogeneity / total variability):   61.73%\nH^2 (total variability / sampling variability):  2.61\n\nTest for Heterogeneity:\nQ(df = 15) = 38.1595, p-val = 0.0009\n\nModel Results:\n\nestimate      se    zval    pval   ci.lb   ci.ub      \n  0.1499  0.0316  4.7501  &lt;.0001  0.0881  0.2118  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\npredict(ma.res)\n\n\n\n   pred     se  ci.lb  ci.ub   pi.lb  pi.ub \n 0.1499 0.0316 0.0881 0.2118 -0.0371 0.3370 \n\n\nCode\nconfint(ma.res)\n\n\n\n       estimate   ci.lb   ci.ub \ntau^2    0.0081  0.0017  0.0378 \ntau      0.0901  0.0412  0.1944 \nI^2(%)  61.7324 25.2799 88.2545 \nH^2      2.6132  1.3383  8.5139"
  },
  {
    "objectID": "posts/Meta Analysis II/index.html#exploring-for-influential-studies",
    "href": "posts/Meta Analysis II/index.html#exploring-for-influential-studies",
    "title": "Meta-Analysis II: Subgroup Analysis",
    "section": "Exploring for influential studies",
    "text": "Exploring for influential studies\n\n\nCode\nma.inf &lt;- influence(ma.res)\nprint(ma.inf)\n\n\n\n                         rstudent  dffits cook.d  cov.r tau2.del  QE.del    hat \nAxelsson et al.,2009       0.2918  0.0485 0.0025 1.1331   0.0091 37.7109 0.0568 \nAxelsson et al.,2011       0.1196 -0.0031 0.0000 1.2595   0.0100 36.7672 0.1054 \nBruce et al.,2010          1.2740  0.2595 0.0660 0.9942   0.0075 35.3930 0.0364 \nChristensen et al.,1999    1.4711  0.3946 0.1439 0.9544   0.0068 33.5886 0.0562 \nChristensen & Smith,1995   0.8622  0.1838 0.0339 1.0505   0.0082 36.5396 0.0441 \nCohen et al.,2004         -0.9795 -0.2121 0.0455 1.0639   0.0084 37.1703 0.0411 \nDobbels et al.,2005        0.2177  0.0296 0.0010 1.1740   0.0094 37.6797 0.0714 \nEdiger et al.,2007        -0.9774 -0.3120 0.1001 1.1215   0.0084 36.1484 0.0889 \nInsel et al.,2006          0.7264  0.1392 0.0195 1.0561   0.0083 37.0495 0.0379 \nJerant et al.,2011        -1.8667 -0.5861 0.2198 0.8502   0.0047 25.0661 0.1058 \nMoran et al.,1997         -1.4985 -0.2771 0.0756 1.0073   0.0077 35.6617 0.0369 \nO'Cleirigh et al.,2007     1.8776  0.4918 0.2148 0.8819   0.0059 31.9021 0.0511 \nPenedo et al.,2003        -1.1892 -0.2939 0.0859 1.0550   0.0080 36.3291 0.0587 \nQuine et al.,2012         -0.0020 -0.0423 0.0021 1.2524   0.0100 37.7339 0.0998 \nStilley et al.,2004        0.8066  0.2126 0.0459 1.0907   0.0083 35.8385 0.0684 \nWiebe & Christensen,1997  -0.7160 -0.1656 0.0280 1.0853   0.0087 37.7017 0.0411 \n                          weight    dfbs inf \nAxelsson et al.,2009      5.6776  0.0481     \nAxelsson et al.,2011     10.5396 -0.0032     \nBruce et al.,2010         3.6432  0.2623     \nChristensen et al.,1999   5.6195  0.3994     \nChristensen & Smith,1995  4.4069  0.1837     \nCohen et al.,2004         4.1094 -0.2112     \nDobbels et al.,2005       7.1362  0.0296     \nEdiger et al.,2007        8.8886 -0.3128     \nInsel et al.,2006         3.7886  0.1387     \nJerant et al.,2011       10.5826 -0.5430     \nMoran et al.,1997         3.6922 -0.2791     \nO'Cleirigh et al.,2007    5.1150  0.5059     \nPenedo et al.,2003        5.8732 -0.2941     \nQuine et al.,2012         9.9778 -0.0434     \nStilley et al.,2004       6.8403  0.2125     \nWiebe & Christensen,1997  4.1094 -0.1642"
  },
  {
    "objectID": "posts/Meta Analysis II/index.html#plot",
    "href": "posts/Meta Analysis II/index.html#plot",
    "title": "Meta-Analysis II: Subgroup Analysis",
    "section": "Plot",
    "text": "Plot\n\n\nCode\nplot(ma.inf)"
  },
  {
    "objectID": "posts/Meta Analysis II/index.html#forest-plot",
    "href": "posts/Meta Analysis II/index.html#forest-plot",
    "title": "Meta-Analysis II: Subgroup Analysis",
    "section": "Forest Plot",
    "text": "Forest Plot\n\n\nCode\nforest(ma.res)\n\n\n\n\n\n\n\nCode\nforest(ma.res, addpred = TRUE, header = TRUE)\n\n\n\n\n\n\n\nCode\nforest(ma.res, header = TRUE)"
  },
  {
    "objectID": "posts/Meta Analysis II/index.html#funnel-plot",
    "href": "posts/Meta Analysis II/index.html#funnel-plot",
    "title": "Meta-Analysis II: Subgroup Analysis",
    "section": "Funnel Plot",
    "text": "Funnel Plot\nPlease be aware that these are not publication bias tests!\n\n\nCode\nfunnel(ma.res, las=3, digits = list(2L,2))"
  },
  {
    "objectID": "posts/Meta Analysis II/index.html#small-study-bias",
    "href": "posts/Meta Analysis II/index.html#small-study-bias",
    "title": "Meta-Analysis II: Subgroup Analysis",
    "section": "Small Study Bias",
    "text": "Small Study Bias\n\n\nCode\nregtest(ma.res)\n\n\n\nRegression Test for Funnel Plot Asymmetry\n\nModel:     mixed-effects meta-regression model\nPredictor: standard error\n\nTest for Funnel Plot Asymmetry: z = 1.0216, p = 0.3070\nLimit Estimate (as sei -&gt; 0):   b = 0.0790 (CI: -0.0686, 0.2266)"
  },
  {
    "objectID": "posts/Meta Analysis II/index.html#adding-a-moderator",
    "href": "posts/Meta Analysis II/index.html#adding-a-moderator",
    "title": "Meta-Analysis II: Subgroup Analysis",
    "section": "Adding a moderator",
    "text": "Adding a moderator\n\n\nCode\nma.res1 &lt;- rma(yi, vi, data=df, mods = ~factor(design))\nma.res1\n\n\n\nMixed-Effects Model (k = 16; tau^2 estimator: REML)\n\ntau^2 (estimated amount of residual heterogeneity):     0.0090 (SE = 0.0062)\ntau (square root of estimated tau^2 value):             0.0946\nI^2 (residual heterogeneity / unaccounted variability): 61.47%\nH^2 (unaccounted variability / sampling variability):   2.60\nR^2 (amount of heterogeneity accounted for):            0.00%\n\nTest for Residual Heterogeneity:\nQE(df = 14) = 34.8274, p-val = 0.0016\n\nTest of Moderators (coefficient 2):\nQM(df = 1) = 0.1911, p-val = 0.6620\n\nModel Results:\n\n                           estimate      se     zval    pval    ci.lb   ci.ub \nintrcpt                      0.1701  0.0551   3.0858  0.0020   0.0621  0.2781 \nfactor(design)prospective   -0.0298  0.0682  -0.4372  0.6620  -0.1636  0.1039 \n                              \nintrcpt                    ** \nfactor(design)prospective     \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\nforest(ma.res1)"
  },
  {
    "objectID": "posts/Meta Analysis II/index.html#ordered-by-year-of-publication",
    "href": "posts/Meta Analysis II/index.html#ordered-by-year-of-publication",
    "title": "Meta-Analysis II: Subgroup Analysis",
    "section": "Ordered by year of publication",
    "text": "Ordered by year of publication\n\n\nCode\ndf3 &lt;- escalc(measure = \"ZCOR\", ri=ri, ni=ni, data=df, slab = paste(authors, year, sep=\",\"))\n\ndf3 &lt;- df3[order(df3$year),] \n\nma.res3 &lt;- rma(yi, vi, data=df3)\nma.res3\n\n\n\nRandom-Effects Model (k = 16; tau^2 estimator: REML)\n\ntau^2 (estimated amount of total heterogeneity): 0.0081 (SE = 0.0055)\ntau (square root of estimated tau^2 value):      0.0901\nI^2 (total heterogeneity / total variability):   61.73%\nH^2 (total variability / sampling variability):  2.61\n\nTest for Heterogeneity:\nQ(df = 15) = 38.1595, p-val = 0.0009\n\nModel Results:\n\nestimate      se    zval    pval   ci.lb   ci.ub      \n  0.1499  0.0316  4.7501  &lt;.0001  0.0881  0.2118  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nforest(ma.res3, addpred = TRUE, header = TRUE)"
  },
  {
    "objectID": "posts/Meta Analysis II/index.html#cumulative-forest-plot",
    "href": "posts/Meta Analysis II/index.html#cumulative-forest-plot",
    "title": "Meta-Analysis II: Subgroup Analysis",
    "section": "Cumulative Forest Plot",
    "text": "Cumulative Forest Plot\n\n\nCode\nforest.cumul &lt;- cumul(ma.res3, order = order(df3$year))\nforest(forest.cumul)"
  },
  {
    "objectID": "posts/Meta Analysis II/index.html#adding-a-second-moderator",
    "href": "posts/Meta Analysis II/index.html#adding-a-second-moderator",
    "title": "Meta-Analysis II: Subgroup Analysis",
    "section": "Adding a second moderator",
    "text": "Adding a second moderator\n\n\nCode\ndf2 &lt;- escalc(measure = \"ZCOR\", ri=ri, ni=ni, data=df, slab = paste(authors, year, sep=\",\"))\n\ndf2 &lt;- df[order(df$yi,df$a_measure),] \n\nma.res2 &lt;- rma(yi, vi, data=df2, mods = ~factor(a_measure)+meanage)\nma.res2\n\n\n\nMixed-Effects Model (k = 16; tau^2 estimator: REML)\n\ntau^2 (estimated amount of residual heterogeneity):     0.0085 (SE = 0.0063)\ntau (square root of estimated tau^2 value):             0.0924\nI^2 (residual heterogeneity / unaccounted variability): 58.84%\nH^2 (unaccounted variability / sampling variability):   2.43\nR^2 (amount of heterogeneity accounted for):            0.00%\n\nTest for Residual Heterogeneity:\nQE(df = 13) = 29.7360, p-val = 0.0051\n\nTest of Moderators (coefficients 2:3):\nQM(df = 2) = 1.3179, p-val = 0.5174\n\nModel Results:\n\n                              estimate      se     zval    pval    ci.lb \nintrcpt                         0.2511  0.1428   1.7576  0.0788  -0.0289 \nfactor(a_measure)self-report    0.0183  0.0701   0.2609  0.7942  -0.1191 \nmeanage                        -0.0022  0.0023  -0.9405  0.3470  -0.0066 \n                               ci.ub    \nintrcpt                       0.5310  . \nfactor(a_measure)self-report  0.1557    \nmeanage                       0.0023    \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nforest(ma.res2)"
  },
  {
    "objectID": "posts/Cross-Lagged Panel Models/index.html#todays-aims",
    "href": "posts/Cross-Lagged Panel Models/index.html#todays-aims",
    "title": "Cross-Lagged Panel Models",
    "section": "Today’s aims",
    "text": "Today’s aims\n\nDefine cross-lagged models\n\nObserved variables\nLatent variables\n\nMultiple Indicators\nCovariates"
  },
  {
    "objectID": "posts/Cross-Lagged Panel Models/index.html#what-is-a-cross-lagged-model",
    "href": "posts/Cross-Lagged Panel Models/index.html#what-is-a-cross-lagged-model",
    "title": "Cross-Lagged Panel Models",
    "section": "What is a cross-lagged model?",
    "text": "What is a cross-lagged model?\n\n\nIt is often the case that our hypothesis will include a causal relationship between two variables, in this case Reading and Writing, where our variables are measured repeatedly, in this case twice. By designing a cross-lagged model we can investigate these relationships across measurement times and for both causal directions."
  },
  {
    "objectID": "posts/Cross-Lagged Panel Models/index.html#cross-lagged-model",
    "href": "posts/Cross-Lagged Panel Models/index.html#cross-lagged-model",
    "title": "Cross-Lagged Panel Models",
    "section": "Cross-lagged model",
    "text": "Cross-lagged model\n\nReading2 is the repeated measurement of the observed variable Reading1\nWriting2 is the repeated measurement of the observed variable Writing1\n\\(\\beta_{R_{2,1}}\\) and \\(\\beta_{W_{2,1}}\\) are autoregressive paths. These paths will indicate the effects of the past on the future after controlling for the persistence element (think of time as a confound?)\n\\(\\beta_{R_{2}W_{1}}\\) and \\(\\beta_{W_{2}R_{1}}\\) are cross-lagged paths. Here we have temporal precendence as well as no assumptions in terms of of directions of effects (Granger Causality)."
  },
  {
    "objectID": "posts/Cross-Lagged Panel Models/index.html#cross-lagged-model-improved",
    "href": "posts/Cross-Lagged Panel Models/index.html#cross-lagged-model-improved",
    "title": "Cross-Lagged Panel Models",
    "section": "Cross-lagged model improved",
    "text": "Cross-lagged model improved\n\nIn terms of structural equations: (Can you spot the mistake?)\n\\(R_2 =\\beta_{R_{2,1}}R_1 + \\beta_{R_{2}W_{1}}W_1+\\zeta_{R_{2}}\\)\n\\(W_2 =\\beta_{R_{2,1}}W_1 + \\beta_{R_{2}W_{1}}R_1+\\zeta_{W_{2}}\\)\nHow many degrees of freedom does this model have?\nWhich variables are endogenous and which ones are exogenous?"
  },
  {
    "objectID": "posts/Cross-Lagged Panel Models/index.html#cross-lagged-model-hypothesis-es",
    "href": "posts/Cross-Lagged Panel Models/index.html#cross-lagged-model-hypothesis-es",
    "title": "Cross-Lagged Panel Models",
    "section": "Cross-lagged model hypothesis (es)",
    "text": "Cross-lagged model hypothesis (es)\n\nOur model will allow us to investigate whether Reading ability can cause Writing ability or whether Writing ability can cause Reading ability. Or ever whether both paths\nIn that case we may be interested to investigate which of the two causal paths is stronger."
  },
  {
    "objectID": "posts/Cross-Lagged Panel Models/index.html#food-for-thought",
    "href": "posts/Cross-Lagged Panel Models/index.html#food-for-thought",
    "title": "Cross-Lagged Panel Models",
    "section": "Food for thought",
    "text": "Food for thought\nSince this is a cross-lagged model with observed variables what could the impact of measurement error be?\nIs there really any benefit from choosing this approach over separate regressions?\nYou can read more about this at chapters 4 and 5 of Longitudinal Structural Equation Modeling: A Comprehensive Introduction (second edition) by Jason T. Newsom\nLongitudinal Structural Equation Modeling | A Comprehensive Introducti (oclc.org)"
  },
  {
    "objectID": "posts/Cross-Lagged Panel Models/index.html#before-moving-on",
    "href": "posts/Cross-Lagged Panel Models/index.html#before-moving-on",
    "title": "Cross-Lagged Panel Models",
    "section": "Before Moving On…",
    "text": "Before Moving On…\nBefore moving on to Cross-Lagged Models with Latent Variables let us attempt to code the above model using lavaan and R\nPlease go to your posit cloud and the project for week 10 code the following model:"
  },
  {
    "objectID": "posts/Cross-Lagged Panel Models/index.html#success",
    "href": "posts/Cross-Lagged Panel Models/index.html#success",
    "title": "Cross-Lagged Panel Models",
    "section": "Success!!",
    "text": "Success!!\nAssuming you have completed the analysis should you report standardized or unstandardized solutions? (Hint: Think in terms in terms of setting equality constraints on the cross-lagged effects)\nAssuming we have succeeded in the above, what changes can we make in our model that would allow us to test whether the cross-lagged effects are significantly different?"
  },
  {
    "objectID": "posts/Cross-Lagged Panel Models/index.html#cross-lagged-models-with-latent-variables",
    "href": "posts/Cross-Lagged Panel Models/index.html#cross-lagged-models-with-latent-variables",
    "title": "Cross-Lagged Panel Models",
    "section": "Cross-Lagged Models with Latent Variables",
    "text": "Cross-Lagged Models with Latent Variables\nOne of the approaches that reduces the risk of measurement error is the use of latent variables with multiple predictors (it is however possible to have single predictor latent variables in our models as an improvement over observed variables). See conceptual model below as presented in Longitudinal Structural Equation Modeling: A Comprehensive Introduction (second edition) by Jason T. Newsom (Figure 5.2, page 142)."
  },
  {
    "objectID": "posts/Cross-Lagged Panel Models/index.html#autoregressive-model",
    "href": "posts/Cross-Lagged Panel Models/index.html#autoregressive-model",
    "title": "Cross-Lagged Panel Models",
    "section": "Autoregressive Model",
    "text": "Autoregressive Model\n\nThis autoregressive model was initially developed by Guttman in 1954 as a general concept of simplex model for longitudinal data. This model provides us with information of growth between consecutive time points. Adjacent time points are lag 1 paths and as the number of steps increases so does the lag. For example Reading1 and Reading3 are lag 2. The simplex model assumes that all lag 1 correlations are of the same magnitude. Furthermore, lag 2 correlation is the product of the two coefficients that link the two measurements. So, if our lag 1 correlation is 0.8 then the correlation between Reading1 and Reading3 should be 0.8*0.8=0.64.\nIf you picture a correlation matrix for the above model how do you expect the correlation values to behave?\nWhat can you infer from the model above?\nDoes it contain means structure?\nCan you spot a weakness of the above mode? (The Quasi-Simplex Model)"
  },
  {
    "objectID": "posts/Cross-Lagged Panel Models/index.html#the-quasi-simplex-model",
    "href": "posts/Cross-Lagged Panel Models/index.html#the-quasi-simplex-model",
    "title": "Cross-Lagged Panel Models",
    "section": "The Quasi-Simplex Model",
    "text": "The Quasi-Simplex Model\n\nHow many observed variables do we have per latent variable?\nCan this model be identified?\nFor the Quasi-Simplex Model first factor variance is constraint to be 1\nFor the Quasi-Simplex Model the residual variance for the first and last occurrence are constraint to be 0.\nIf you are using similar models to the Simplex Model you can use constraints that fit your measurements. For example, we can assume that the variances of all error terms are equal. Another approach would be to fix the factor loadings from and to each observed reading score to 1."
  },
  {
    "objectID": "posts/Cross-Lagged Panel Models/index.html#continued",
    "href": "posts/Cross-Lagged Panel Models/index.html#continued",
    "title": "Cross-Lagged Panel Models",
    "section": "Continued …",
    "text": "Continued …"
  },
  {
    "objectID": "posts/Cross-Lagged Panel Models/index.html#solutions-to-observed-variables-measurement-error",
    "href": "posts/Cross-Lagged Panel Models/index.html#solutions-to-observed-variables-measurement-error",
    "title": "Cross-Lagged Panel Models",
    "section": "Solutions to Observed Variables Measurement Error",
    "text": "Solutions to Observed Variables Measurement Error\n\nThe above is sometimes referred to as co-movement. Can you identify a potential weakness of the above model?\nWe will return to this later on as improving this will be one of the main points of discussion next week."
  },
  {
    "objectID": "posts/Cross-Lagged Panel Models/index.html#cross-lagged-models-with-latent-variables-1",
    "href": "posts/Cross-Lagged Panel Models/index.html#cross-lagged-models-with-latent-variables-1",
    "title": "Cross-Lagged Panel Models",
    "section": "Cross-Lagged Models with Latent Variables",
    "text": "Cross-Lagged Models with Latent Variables\n\nDo you observe any constraints in the model?\nIs this model identified?\n\\(df= \\frac{J(J+1)}{2}-(2J+8)\\)"
  },
  {
    "objectID": "posts/Cross-Lagged Panel Models/index.html#a-note-on-measurement-invariance",
    "href": "posts/Cross-Lagged Panel Models/index.html#a-note-on-measurement-invariance",
    "title": "Cross-Lagged Panel Models",
    "section": "A Note on Measurement Invariance",
    "text": "A Note on Measurement Invariance\nPrior to constructing a longitudinal structural model we assume measurement invariance, meaning that the measurement properties of all latent variables are stable over time. This is why we strive to maintain the same methodology and measurements across all time points in a longitudinal study.\nFor more information read Chapter 2 in Longitudinal Structural Equation Modeling: A Comprehensive Introduction (second edition) by Jason T. Newsom"
  },
  {
    "objectID": "posts/Cross-Lagged Panel Models/index.html#meier-and-spector-reciprocal-effects-of-work-stressors-and-counterproductive-work-behavior-a-five-wave-longitudinal-study-2013",
    "href": "posts/Cross-Lagged Panel Models/index.html#meier-and-spector-reciprocal-effects-of-work-stressors-and-counterproductive-work-behavior-a-five-wave-longitudinal-study-2013",
    "title": "Cross-Lagged Panel Models",
    "section": "Meier and Spector, Reciprocal effects of work stressors and counterproductive work behavior: A five-wave longitudinal study (2013)",
    "text": "Meier and Spector, Reciprocal effects of work stressors and counterproductive work behavior: A five-wave longitudinal study (2013)"
  },
  {
    "objectID": "posts/Cross-Lagged Panel Models/index.html#meier-and-spector",
    "href": "posts/Cross-Lagged Panel Models/index.html#meier-and-spector",
    "title": "Cross-Lagged Panel Models",
    "section": "Meier and Spector",
    "text": "Meier and Spector"
  },
  {
    "objectID": "posts/Cross-Lagged Panel Models/index.html#meier-and-spector-1",
    "href": "posts/Cross-Lagged Panel Models/index.html#meier-and-spector-1",
    "title": "Cross-Lagged Panel Models",
    "section": "Meier and Spector",
    "text": "Meier and Spector\nFocusing on the above one could wonder what are we actually estimating?\nWithin effects?\nBetween effects?\nSample specific or population average effects?\n(Contextual effects?)"
  },
  {
    "objectID": "posts/Cross-Lagged Panel Models/index.html#critique",
    "href": "posts/Cross-Lagged Panel Models/index.html#critique",
    "title": "Cross-Lagged Panel Models",
    "section": "Critique",
    "text": "Critique"
  },
  {
    "objectID": "posts/Cross-Lagged Panel Models/index.html#criticisms",
    "href": "posts/Cross-Lagged Panel Models/index.html#criticisms",
    "title": "Cross-Lagged Panel Models",
    "section": "Criticisms",
    "text": "Criticisms"
  },
  {
    "objectID": "posts/Cross-Lagged Panel Models/index.html#proposed-solutions",
    "href": "posts/Cross-Lagged Panel Models/index.html#proposed-solutions",
    "title": "Cross-Lagged Panel Models",
    "section": "Proposed Solutions",
    "text": "Proposed Solutions\n\nhttps://pure.uva.nl/ws/files/2688454/168970_Hamaker_Kuiper_Grasman_2015_A_Critique_of_Cross_Lagged_Panel_Model.pdf\nA Critique of the Cross-Lagged Panel Model, Hamaker et al., 2015"
  },
  {
    "objectID": "posts/Cross-Lagged Panel Models/index.html#proposed-solutions-1",
    "href": "posts/Cross-Lagged Panel Models/index.html#proposed-solutions-1",
    "title": "Cross-Lagged Panel Models",
    "section": "Proposed Solutions",
    "text": "Proposed Solutions\n\nhttps://journals.sagepub.com/doi/full/10.1177/1094428119847280\nFrom Data to Causes II: Comparing Approaches to Panel Data Analysis, Zyphur et al., 2019"
  },
  {
    "objectID": "posts/Cross-Lagged Panel Models/index.html#let-us-return-to-a-previously-discussed-model",
    "href": "posts/Cross-Lagged Panel Models/index.html#let-us-return-to-a-previously-discussed-model",
    "title": "Cross-Lagged Panel Models",
    "section": "Let us return to a previously discussed model",
    "text": "Let us return to a previously discussed model\n\nFollowing the conceptual model above, can you code a similar model for the dataset latent.csv in your Posit Project?"
  },
  {
    "objectID": "posts/Cross-Lagged Panel Models/index.html#task-for-next-week",
    "href": "posts/Cross-Lagged Panel Models/index.html#task-for-next-week",
    "title": "Cross-Lagged Panel Models",
    "section": "Task for next week",
    "text": "Task for next week\nRead A Critique of the Cross-Lagged Panel Model, Hamaker et al., 2015 and From Data to Causes II: Comparing Approaches to Panel Data Analysis, Zyphur et al., 2019. Next week we will discuss strengths and limitations of these proposed models."
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#achievements-and-badgeshopefully",
    "href": "posts/Introduction to Advanced Statistics/index.html#achievements-and-badgeshopefully",
    "title": "Introduction to Advanced Statistics",
    "section": "Achievements and Badges(hopefully)",
    "text": "Achievements and Badges(hopefully)\nEvery week there will be a Canvas practice quiz provided so you can practice and enhance your understanding. These are absolutely non-compulsory but do spend some time trying to complete them. You will also be collecting points while you complete these (and other Canvas activities) and at the end of the module the two students with the highest scores will receive a surprise present."
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#essential-and-supplementary-materials",
    "href": "posts/Introduction to Advanced Statistics/index.html#essential-and-supplementary-materials",
    "title": "Introduction to Advanced Statistics",
    "section": "Essential and Supplementary Materials",
    "text": "Essential and Supplementary Materials\n\n\n\n\n\n\nPrinciples and Practice of Structural Equation Modeling\nLongitudinal Structural Equation Modeling: A comprehensive Introduction\nOfficial lavaan tutorial from lavaan.org\nPsychometrics in Exercises using R and Rstudio by Prof Anna Brown\nIntroduction to Structural Equation Modeling (SEM) in R with lavaan by Dr Johnny Lin\n** And many other online resources that will be revealed in due time"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#module-content",
    "href": "posts/Introduction to Advanced Statistics/index.html#module-content",
    "title": "Introduction to Advanced Statistics",
    "section": "Module Content",
    "text": "Module Content\n\nIntroduction to S.E.M. (Week 2)\nExploratory Factor Analysis (Week 3)\nConfirmatory Factor Analysis (Week 4)\nMediation/Moderation Analysis (Week 5)\nAdvanced topics in S.E.M, Mean Structures and Latent Growth Models (Week 6)\nAdvanced topics in S.E.M, Multiple Sample Analysis and Measurement Invariance (Week 7)\nBest Practices in S.E.M (Week 8)\nIntroduction to Meta-Analysis (Week 9)\nMeta-Analysis II (Week 10)\nMeta-Analysis III (Week 11)"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#quick-break-task",
    "href": "posts/Introduction to Advanced Statistics/index.html#quick-break-task",
    "title": "Introduction to Advanced Statistics",
    "section": "Quick Break Task",
    "text": "Quick Break Task\nDo a quick internet search and find two working definitions for maximum likelihood and ordinary least squares.\nWhat are the key similarities and differences?"
  },
  {
    "objectID": "posts/Introduction to Advanced Statistics/index.html#a-prelude-to-working-on-posit-cloud",
    "href": "posts/Introduction to Advanced Statistics/index.html#a-prelude-to-working-on-posit-cloud",
    "title": "Introduction to Advanced Statistics",
    "section": "A prelude to working on Posit Cloud",
    "text": "A prelude to working on Posit Cloud\nI would like everyone now to access our Posit Cloud Workspace and open the Week 1 Project"
  }
]