{
  "hash": "576c3563d202279b6377768654702bc8",
  "result": {
    "markdown": "---\ntitle: \"Introduction to Advanced Statistics\"\nformat: \n  revealjs:\n    theme: league\n    transition: slide\n    background-transition: zoom\n    slide-number: c/t\n    show-slide-number: all\n    chalkboard: true\n    background-size: cover\n    smaller: true\n    echo: true\n    code-fold: true\n    code-summary: \"Show the code\"\nauthor: \"Dr Lazaros Gonidis\"\ndate: \"2024-01-27\"\nimage: \"image1.jpg\"\n---\n\n\n## Who I am\n\n::: columns\n::: {.column width=\"20%\"}\n![](images/me.jpg){width=\"156\"}\n\n![](images/photo2.jpg){width=\"152\"}\n\n![](images/photo3.jpg){width=\"151\"}\n:::\n\n::: {.column width=\"80%\"}\n1.  This is me (used to be)\n2.  I love my two boys\n3.  I also love computer games, motorbikes, music, food\n4.  Funnily enough I love maths and statistics, and even more teaching these\n5.  I am also dyslexic so please do tell me when you spot typing mistakes, usually whole words missing.\n:::\n:::\n\n## Who you are\n\nI have no idea who you are but I hope by the end of this module you will love statistics a bit more and you will appreciate even more why they are paramount for Psychological research.\n\n## Today's Aims in Terms of the Module\n\n1.  Canvas\n2.  Rstudio/Posit\n3.  discord\n4.  Assessment\n5.  Quizzes (Badges to be confirmed)\n6.  Github\n\n## Canvas\n\nHopefully you should know where to find our Canvas website, but just in case:\n\n<https://canvas.sussex.ac.uk/courses/26315/pages/module-home-page>\n\n## RStudio/Posit Cloud\n\nFor our analysis we will be using exclusively R and mainly Posit Cloud. However, I strongly believe that as future scientists you should also have locally installed RStudio/Posit, have it up to date and use that for your everyday work. I would also advise you to attend our workshops bringing your own laptops/tablets/mobile phones.\n\nYou can join our Posit Cloud Workspace by following this link:\n\n<https://posit.cloud/spaces/469376/join?access_code=4YBUfhrNPGU2oWWw5Zym6bepaVV6izQ8O1KE6THK>\n\n![](/qr-code.png){fig-align=\"center\" width=\"336\"}\n\n## Discord as our mean of day to day communication\n\nWe will be using discord as our main communication channel, please use it for any stats related questions. If you have any more sensitive questions please do not hesitate to email me at: L.Gonidis\\@Sussex.ac.uk\n\nYou can join us by following this link: <https://discord.gg/9TV3xzMZ>\n\n![](/wr-code2.png){fig-align=\"center\" width=\"345\"}\n\n## Assessment\n\nThe module will be assessed exclusively by coursework. This will be in the form of three 1,000 words results sections. The deadlines are:\n\n1.  Report 1, February 13th 16:00\n2.  Report 2, March 14th 16:00\n3.  Report 3, April 23rd 16:00\n\nAll submissions will be via Canvas submission points in the Assignments, Contributory E-Submission subsection. Each report topic will be released in our workshop the week prior to the submission deadline. Specifically, for Report 1 the topic will be Mediation/Moderation analysis using lavaan. You will receive detailed instructions on that next Monday.\n\n## Quizzes and Badges(hopefully)\n\nEvery week there will be a Canvas quiz provided so you can practice and enhance your understanding. These are absolutely non-compulsory but do spend some time trying to complete them. You will also be collecting points while you complete these (and other Canvas activities) and at the end of the module the two students with the highest scores will receive a surprise present.\n\n## Github\n\nGithub has been increasingly becoming a standard in the world of coding and statistical analysis as it can serve both as a repository and a host of websites related to projects. We will have a very brief demonstration today and we will return next week and start using it as a standard in our module.\n\n## Essential and Supplamentary Materials\n\n::: columns\n::: {.column width=\"20%\"}\n![](/kline.JPG){width=\"130\"}\n\n![](/Newsom.JPG){width=\"129\"}\n:::\n\n::: {.column width=\"80%\"}\n1.  [Principles and Practice of Structural Equation Modeling](https://ebookcentral.proquest.com/lib/suss/reader.action?docID=4000663)\n\n2.  [Longitudinal Structural Equation Modeling: A comprehensive Introduction](Longitudinal%20Structural%20Equation%20Modeling:%20A%20comprehensive%20Introduction)\n\n3.  [Official lavaan tutorial from lavaan.org](https://lavaan.ugent.be/tutorial/)\n\n4.  [Psychometrics in Exercises using R and Rstudio](https://bookdown.org/annabrown/psychometricsR/) by Prof Anna Brown\n\n5.  [Introduction to Structural Equation Modeling (SEM) in R with lavaan](https://stats.oarc.ucla.edu/r/seminars/rsem/#s1a) by Dr Johnny Lin\n\n    \\*\\* And many other online resources that will be revealed in due time\n:::\n:::\n\n## Today's Aims in Terms of Structural Equation Modeling (SEM)\n\n1.  Introduce key terminology that we will be using this term\n2.  Discuss examples of **simple and multiple regression**\n3.  Revisit the same examples using **lavaan**\n4.  Discuss **baximum likelihood** vs. **least squares**\n\n## Key Terminology\n\nWe will be using quite a few terms in our module so it is important to define them in advance and try to use them consistently. Most of the them are used as in the field but you could come across some slight variations in terminology. This should not put you off or scare you, as long as you understand the substance of each term. It is also a good idea to try to learn the visual equivalents of these terms\n\n## Key Terminology 2\n\n::: columns\n::: {.column width=\"20%\"}\n![](/path.JPG){width=\"164\" height=\"438\"}\n:::\n\n::: {.column width=\"80%\"}\n1.  **latent variable:** a variable that is constructed/inferred indirectly by the data and does not exist in the data.\n2.  **observed variable:** a variable that has been measured and exists in our data.\n3.  **exogenous variable:** an independent variable that explains an endogenous variable. Can be either observed $(x)$ or latent $(ξ)$.\n4.  **endogenous variable:** a dependent variable that has a causal path leading to it. Can be either observed $(y)$ or latent $(η)$.\n5.  **measurement model:** a model that links observed variables with latent variables\n:::\n:::\n\n## Key Terminology 3\n\n::: columns\n::: {.column width=\"20%\"}\n![](/path.JPG){width=\"164\" height=\"438\"}\n:::\n\n::: {.column width=\"80%\"}\n6.  **indicator:** an observed variable in a measurement model (can be exogenous or endogenous).\n7.  **factor**: a latent variable defined by its indicators (can be exogenous or endogenous).\n8.  **loading:** a path between an indicator and a factor.\n9.  **structural model:** a model that specifies causal relationships among exogenous variables to endogenous variables (can be observed or latent).\n10. **regression path:** a path between exogenous and endogenous variables (can be observed or latent).\n:::\n:::\n\n## Are you ready for the first headache question?\n\nSo far, in linear regression we have learnt that $x$ is an **independent** variable and $y$ the **dependent** variable or outcome. However, in measurement models, the use of $x$ or $y$ depends on the type of factor we are referring to. If an indicator depends on an **exogenous** factor, the we refer to it as $x$-side. If an indicator depends on an **endogenous** factor then we refer to it as $y$-side.\n\n## Let us expand on this headache a bit more\n\n![](/path2.JPG)\n\n## Simple Regression\n\nSo far we have learnt that a simple regression is the linear relation between a predictor (or an independent variable, or an observed exogenous variable) and an outcome (or observed endogenous variable).\n\n$$\ny_1 = b_0 +b_1x_1 + ε_1\n$$\n\nwhere $b_0$ is the intercept and $b_1$ is the coefficient of $x_1$ (observed predictor) and $ε_1$ is the residual with $y_1$ being the outcome.\n\n## Simple Regression cont.\n\n**I strongly recommend reading Kline chapter 2, pages 25-30 at minimum, Regression Fundamentals.**\n\n$$\n\\hat{Y} = B_XX + A_X\n$$\n\nThe above equation represents **predicting Y from X**\n\nAlso referred to as **regressing Y on X**, with $\\hat{Y}$ representing **predicted scores**, $B_X$ unstandardised regression coefficient for predictor $X$, also known as slope, and $A_X$ is the intercept.\n\nGenerally, with linear models we would use **ordinary least square** (OLS) so that the **least squares criterion** is satisfied. In practice, we are trying to minimise the sum of squared residuals, $\\sum(Y-\\hat{Y})$\n\n## Let us see an example with data\n\nWe will work with the randomly generated data included in the **random.csv**\n\nThe datafile includes three variables:\n\n**reading: reading ability as assessed in school**\n\n**income: weekly family income in £**\n\n**books: number of books read in a month (on average)**\n\nWe will create a simple regression model of **reading**  regressing on **income** using **lm()**\n\n## Loading the datafile\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(lavaan)\ndf <- read_csv(\"random.csv\")\n```\n:::\n\n\n## Creating the model and getting model parameters\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreading_lm <- lm(reading ~ income, data = df)\n# the option below instructs R to give us the output in non-exponential notation\noptions(scipen=999)\n\nsummary(reading_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = reading ~ income, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-50.590 -16.562  -0.178  13.429  48.207 \n\nCoefficients:\n             Estimate Std. Error t value             Pr(>|t|)    \n(Intercept) 188.17787    6.68278  28.159 < 0.0000000000000002 ***\nincome        0.11800    0.01662   7.098       0.000000000201 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.44 on 98 degrees of freedom\nMultiple R-squared:  0.3395,\tAdjusted R-squared:  0.3328 \nF-statistic: 50.38 on 1 and 98 DF,  p-value: 0.0000000002013\n```\n:::\n:::\n\n\n## Looking at the output\n\nOur intercept is 188.18 and our income coefficient is 0.118 (0.12). This means that the reading ability of a student with a family income of £0 will be 188.18 and for every £1 of family income increase the reading ability will increase by 0.12.\n\nWe also see **residual standard error of 21.44**\n\nThe **square of that value is 459.67**\n\n## Let us recreate the same model using Lavaan\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# lavaan uses a simple language when specifying the model\n#simple regression using lavaan \nreading_lav <-   '\n  # regressions\n    reading ~ 1 + income\n  # variance (optional)\n    income ~~ income\n'\nreading_lav_sem <- sem(reading_lav, data=df)\nsummary(reading_lav_sem)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlavaan 0.6.17 ended normally after 11 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         5\n\n  Number of observations                           100\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate   Std.Err  z-value  P(>|z|)\n  reading ~                                            \n    income             0.118    0.016    7.170    0.000\n\nIntercepts:\n                   Estimate   Std.Err  z-value  P(>|z|)\n   .reading          188.178    6.616   28.445    0.000\n    income           380.752   12.896   29.525    0.000\n\nVariances:\n                   Estimate   Std.Err  z-value  P(>|z|)\n    income         16630.573 2351.918    7.071    0.000\n   .reading          450.401   63.696    7.071    0.000\n```\n:::\n:::\n\n\n## Comparing the two outputs\n\nWe observe that the estimates of the regression coefficients are the same despite **lm()** using **least squares (LS)** and **lavaan** using **maximum likelihood (ML)**. However the variances are different with **459.67** for **lm()** and **450.40** for **lavaan.**\n\nIn we want to convert from **LS variance to ML variance** we can use the following formula\n\n$$\n\\hat{σ}_{ML}^2 = \\frac{(N-k)}{n}\\hat{σ}_{LS}^2\n$$\n\nWhere $N$ and $n$ are the sample sizes and $k$ is the number or parameters to estimate, in this case $k$=2, one intercept and one regression coefficient\n\n$$\n\\hat{σ}_{ML}^2 = \\frac{(100-2)}{100}21.44^2=450.48\n$$\n\n## Visualising with semPaths() from semPlot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(semPlot)\n\nsemPaths(reading_lav_sem,\n         whatLabels = \"est\",\n         sizeMan = 10,\n         style = \"ram\",\n         layout = \"circle\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n## OLS vs ML\n\nGenerally in SEM, we use the **maximum likelihood estimator (MLE)**. In this module we will be using the acronyms **ML** and **MLE** to denote the maximum likelihood estimator method. This method estimates model parameters by maximising the likelihood function. In other words, maximising the probability of observing our existing data points given specific parameter values. We will be discussing in details what these parameters are in SEM (e.g., coefficients, latent variable variances, etc.). It should also be noted that **MLE** is not the only estimation methods, other methods can also be successfully implemented, such as **generalised least squares**. We decided on the appropriate estimator based on our data characteristics and assumptions.\n\n## Multiple regression, lm()\n\nWe can expand the previous example to now include a second predictor, **books** which represents the number of books read in a month (on average)\n\n$$\ny_1 = b_0 +b_1x_1 + b_2x_2 + ε_1\n$$\n\nYou may also come across the following notation:\n\n$$\n\\hat{Y} = B_XX + +B_WW + A_{X,W}\n$$\n\nImportant to note here that $B_X$ and $B_W$ are the **unstandardized partial regression coefficients**, $A_{X,W}$ is the intercept. For more information please read Kline page 30.\n\n## \n\nMultiple regression, lm()\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreading2_lm <- lm(reading ~ income + books, data = df)\n\n\nsummary(reading2_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = reading ~ income + books, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-50.397 -13.633   0.547  13.698  51.585 \n\nCoefficients:\n             Estimate Std. Error t value             Pr(>|t|)    \n(Intercept) 187.28030    6.14063  30.499 < 0.0000000000000002 ***\nincome        0.06160    0.01997   3.085              0.00265 ** \nbooks         6.14601    1.40272   4.382            0.0000298 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.69 on 97 degrees of freedom\nMultiple R-squared:  0.4487,\tAdjusted R-squared:  0.4373 \nF-statistic: 39.47 on 2 and 97 DF,  p-value: 0.0000000000002875\n```\n:::\n:::\n\n\n## Multiple regression, lavaan\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreading2_lav <-   '\n  # regressions\n    reading ~ 1 + income + books\n  # covariance\n    income ~~ books\n'\nreading2_lav_sem <- sem(reading2_lav, data=df)\nsummary(reading2_lav_sem)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlavaan 0.6.17 ended normally after 38 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n\n  Number of observations                           100\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate   Std.Err  z-value  P(>|z|)\n  reading ~                                            \n    income             0.062    0.020    3.132    0.002\n    books              6.146    1.382    4.449    0.000\n\nCovariances:\n                   Estimate   Std.Err  z-value  P(>|z|)\n  income ~~                                            \n    books            152.610   28.168    5.418    0.000\n\nIntercepts:\n                   Estimate   Std.Err  z-value  P(>|z|)\n   .reading          187.280    6.048   30.967    0.000\n    income           380.752   12.896   29.525    0.000\n    books              3.640    0.184   19.827    0.000\n\nVariances:\n                   Estimate   Std.Err  z-value  P(>|z|)\n   .reading          375.988   53.173    7.071    0.000\n    income         16630.574 2351.918    7.071    0.000\n    books              3.370    0.477    7.071    0.000\n```\n:::\n:::\n\n\n## Visualising with semPaths() from semPlot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsemPaths(reading2_lav_sem,\n         whatLabels = \"est\",\n         sizeMan = 10,\n         style = \"ram\",\n         layout = \"spring\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n## Visualising with semPaths() from semPlot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsemPaths(reading2_lav_sem,\n         whatLabels = \"est\",\n         sizeMan = 10,          \n         style = \"ram\",          \n         layout = \"tree\",\n         intercepts = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n## A prelude to next week. Mediation with Lavaan\n\nWe will explore whether the number of **books** read actually **mediate** the effect of **income** on **reading** ability. We will not go in depth today as we will discuss this topic next week in detail. We will just demonstrate how the code would change in lavaan.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreading_med_model <- ' # direct effect\n             reading ~ c*income\n           # mediator\n             books ~ a*income\n             reading ~ b*books\n           # indirect effect (a*b)\n             ab := a*b\n           # total effect\n             total := c + (a*b)\n         '\nreading_med <- sem(reading_med_model, data = df)\nsummary(reading_med )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlavaan 0.6.17 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         5\n\n  Number of observations                           100\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate   Std.Err  z-value  P(>|z|)\n  reading ~                                            \n    income     (c)     0.062    0.020    3.132    0.002\n  books ~                                              \n    income     (a)     0.009    0.001    8.431    0.000\n  reading ~                                            \n    books      (b)     6.146    1.382    4.449    0.000\n\nVariances:\n                   Estimate   Std.Err  z-value  P(>|z|)\n   .reading          375.988   53.173    7.071    0.000\n   .books              1.970    0.279    7.071    0.000\n\nDefined Parameters:\n                   Estimate   Std.Err  z-value  P(>|z|)\n    ab                 0.056    0.014    3.935    0.000\n    total              0.118    0.016    7.170    0.000\n```\n:::\n:::\n\n\n## Let us visualise the above model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsemPaths(reading_med,\n         whatLabels = \"est\",\n         sizeMan = 10,          \n         style = \"ram\",          \n         layout = \"tree\",\n         intercepts = FALSE,\n         rotation = 2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n## To conclude\n\nHopefully, this was a gentle introduction to the module and lavaan. We will be following a more formal approach in the coming weeks. In the meantime do spend some time this week to do the suggested reading and practice with R and lavaan in our Posit Cloud Workspace.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    // dispatch for htmlwidgets\r\n    function fireSlideEnter() {\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n    }\r\n\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n      fireSlideEnter();\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}