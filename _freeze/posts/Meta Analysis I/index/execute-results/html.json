{
  "hash": "77b16d45552c1706ef1497a708f81531",
  "result": {
    "markdown": "---\ntitle: \"Meta-Analysis I\"\nformat: \n  revealjs:\n    theme: league\n    transition: slide\n    background-transition: zoom\n    slide-number: c/t\n    show-slide-number: all\n    chalkboard: true\n    background-size: cover\n    smaller: true\n    echo: true\n    code-fold: true\n    code-summary: \"Show the code\"\nauthor: \"Dr Lazaros Gonidis\"\ndate: \"2024-03-12\"\nimage: \"image1.jpg\"\n---\n\n\n## Today's Aims\n\nToday we will discuss meta-analysis as a statistical technique to synthesize results from different studies.\n\nThe main aim of a meta-analysis is to assess the evidence as presented across a number of similar studies. This assessment can help us establish whether there is a true effect, especially in the case of conflicting findings.\n\n## What can meta-analysis tell us?\n\nAs mentioned in, Field, A. P., & Gillett, R. (2010). How to do a meta‐analysis. *British Journal of Mathematical and Statistical Psychology*, *63*(3), 665-694.\n\n1.  Mean and variance of underlying population effects\n2.  Variability in effects across studies\n3.  Moderator variables\n\n## Strengths of meta-analysis\n\n1.  Meta-analysis can go beyond the answers provided in individual studies\n2.  It can also improve precision of findings as it includes more information\n3.  Statistically, we have increased power in a meta-analysis compared to individual studies\n4.  As mentioned above, we can shed more light in the case of conflicting results\n5.  Makes is possible to compare subgroups\n\nActually plenty of other strenghts ...\n\n## Weaknesses of meta-analysis\n\n1.  Publication bias, meta-analysis is primarily relying on published results\n2.  Small studies vs large studies\n3.  Heterogeneity (we will discuss this in greater detail)\n4.  Fixed vs Random Effect (we will discuss this in greater detail)\n\nActually plenty of other limitations ...\n\n## Carrying out meta-analysis is simple, right?\n\n![](anakin.jpg)\n\n## Well let us see a published example\n\n![](abstract.JPG){width=\"534\"}\n\n## Well let us see a published example\n\n![](inclusion.jpg){width=\"441\"}\n\n## Well let us see a published example\n\n![](forest.jpg){width=\"554\"}\n\n## Technically it is simple, in practice it is not\n\n1.  We should carry out a literature review\n2.  We should have a research question/hypothesis (hmm not before the literature review?)\n3.  We should have a list of inclusion/exclusion criteria\n4.  We should then carry out a literature review (again?)\n5.  Collate statistical findings, calculate effect sizes\n6.  \n    1.  Carry out the statistical analysis (wait, is the calculation of effect sizes not part of this?). Also referred to as basic meta-analysis\n    2.  You might also want to carry out more advanced statistical analysis e.g., *moderator analysis*\n7.  Report your findings\n\n## In practice it is more complex\n\nIn practice, all of the above steps require extensive reading and clear decision making based on sound understanding of your research field and the statistics behind effect sizes and meta-analysis.\n\n## First thing first, well not really!\n\nToday we will start from step 5. Collate statistical findings, calculate **effect sizes**.\n\n***Effect Size***\n\nQuite often, the effect size is reported and can be directly extracted, not always though. In order to perform a meta-analysis we need **effect sizes** and we have to choose wisely as they can affect our results.\n\n1.  **Computable**\n2.  **Comparable**\n3.  **Reliable**\n4.  **Interpretable**\n\nFor more details you can see Higgins, Julian, James Thomas, Jacqueline Chandler, Miranda Cumpston, Tianjing Li, Matthew J Page, and Vivian A Welch. 2019. *Cochrane Handbook for Systematic Reviews of Interventions*. John Wiley & Sons.\n\n## So what is effect size?\n\nYou most probably have come across the term **effect size** in the past, in general terms it is a metric of the **magnitude** of the difference or relationship between variables. A larger effect size denotes a more meaningful difference/relationship between variables. If we consider two arithmetic means $\\bar{x}_1$ and $\\bar{x}_2$ then we can use Cohen's d effect size.\n\n**Cohen’s *d***\n\n$d = \\frac{{\\bar{x}_1 - \\bar{x}_2}}{{s_p}}$\n\nwhere\n\n$\\bar{x}_1$ and $\\bar{x}_2$ are the mean of the two groups, and $s_p$ is the pooled standard deviation defined as:\n\n$s_p = \\sqrt{\\frac{{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}}{{n_1 + n_2 - 2}}}$\n\n## A cautionary tale\n\nDo not be misguided by the word **effect**. There is no suggestion of **causality** hence you should not interpret it as such. Instead, you should treat **effect size** with caution.\n\nAlso, keep in mind that we will be working with measures of central tendency, so we need to be cautious and not automatically label everything as **effect size**. Furthermore, our observed effect size $\\hat{\\theta}_1$ from a study 1 is only an estimate of the true effect size $\\theta$. Therefore, it is subdue to sampling error:\n\n$\\hat{\\theta}_i= {\\theta}_i + {\\epsilon}_i$\n\n## We can see a very basic example here\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(23234)\nsample1 <- rnorm(n=250, mean = 100, sd = 15)\n\nmean(sample1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 100.4061\n```\n:::\n:::\n\n\n## We can see a very basic example here\n\nWe can now repeat this process 1,000 times and calculate the standard deviation of this sampling distribution. This is our **standard error of the mean.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamplemean <- data.frame(m=1:1000,s=1:1000)\nfor (i in 1:1000) {\n  sample1 <- rnorm(n=250, mean = 100, sd = 15)\n  samplemean$m[i] <- mean(sample1)\n  samplemean$s[i] <- sd(sample1)\n}\nhist(samplemean$m, breaks = 20)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n## Standard error of the mean\n\n$SE=\\frac{s}{\\sqrt{n}}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsd(samplemean$m)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9708343\n```\n:::\n\n```{.r .cell-code}\nsd(sample1)/sqrt(250)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.041905\n```\n:::\n:::\n\n\nWe can observe that the two values are similar but not quite the same.\n\n## Standard error of the mean\n\n$SE=\\frac{s}{\\sqrt{n}}$ We can also see that our value will depend on the sample size $n$. The larger our sample size, the larger our denominator will be, hence the smaller our **standard error**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamplemean2 <- data.frame(n=1:999, m=1:999,se=1:999)\nfor (i in 2:1000) {\n  sample1 <- rnorm(n=i, mean = 100, sd = 15)\n  samplemean2$n[i-1] <- i\n  samplemean2$m[i-1] <- mean(sample1)\n  samplemean2$se[i-1] <- sd(sample1)/sqrt(i)\n}\nlibrary(ggplot2)\nggplot(samplemean2, aes(x=n, y=m))+\n  geom_line()+\n  labs(x = \"Sample Size\")+\n  labs(y = \"Mean\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n## Standard error of the mean\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(samplemean2, aes(x=n, y=se))+\n  geom_line()+\n  labs(x = \"Sample Size\")+\n  labs(y = \"Standard Error\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\nWhat are the implications of these two plots on the **Cohen’s *d?***\n\n$d = \\frac{{\\bar{x}_1 - \\bar{x}_2}}{{s_p}}$\n\n$s_p = \\sqrt{\\frac{{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}}{{n_1 + n_2 - 2}}}$\n\n**Required figures: sample size, mean, and standard deviation of each study**\n\n## Proportions and effect size\n\nBesides arithmetic means as measures of central tendency we may also work with paradigms using **proportions**. In order to calculate a proportion we need to divide the number of observation $j$ that have a characteristic by the total sample size $n$\n\n$p=\\frac{j}{n}$\n\n$SE_p=\\sqrt{\\frac{p(1-p)}{n}}$\n\n## Proportions and effect size\n\nLet us see a basic example. We have sample of 250 observations with 100 of them exhibiting a specific behaviour.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 250\nj <- 100\np <- j/n\nse <- sqrt((p*(1-p))/n)\np\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4\n```\n:::\n\n```{.r .cell-code}\nse\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.03098387\n```\n:::\n:::\n\n\nProportions can be problematic in terms of **standard error** as they can only vary between **0** and **1**. Can you think of the reason why?\n\n## Transforming proportions\n\nIn order to avoid this limitation we **logit-transform** proportions after we calculate the **odds**. The formulae for this process are:\n\n$p_{logit}=log_e(\\frac{p}{1-p})$\n\n$SE_{plogit}=\\sqrt{{\\frac{1}{np}}+\\frac{1}{n(1-p)}}$\n\n**Required figures: Sample size and the number of observations with a specific characteristic**\n\n## Correlations\n\nThe correlation expresses the amount of covariance between two variables, the most commonly used is the Pearson correlation (for continuous variables).\n\n$r_{xy}=\\frac{\\sigma^2_{xy}}{\\sigma_x\\sigma_y}$\n\n$SE_{r_{xy}}=\\frac{1-r^2_{xy}}{\\sqrt{n-2}}$\n\nReminder 1: this is a standardized score which means we can immediately compare correlations between variables that are not necessarily on the same scale.\n\nReminder 2: correlations are scores that naturally vary between **-1** and **1**. Does this bring in mind any potential problems?\n\n## Fisher's $z$ transformation of correlations\n\n$z=0.5log_e(\\frac{1+r}{1-r})$\n\n$SE_z=\\frac{1}{\\sqrt{n-3}}$\n\n**Required figures: correlation coefficient and sample size**\n\n## More experimental paradigms and Effect size corrections\n\nNext week we will present more cases experimental designs and how we can calculate **SE** and effect sizes. Furthermore, we will discuss how we can correct our **effect sizes** to account for systematic biases such as small sizes.\n\n## Fixed-Effect vs Random-Effects Model\n\nTypically in statistical models we work with either **fixed-effect** or **random-effect** models.\n\n**Fixed-effect models** assume that all the effect sizes in our studies are derived from a homogeneous population with a fixed effect size. This means that our sample effect sizes are also homogeneous.\n\n**Random-effect models** assume our population effect size varies randomly between our studies, therefore we should not assume that our sample sizes are homogeneous.\n\nIn practice, **fixed-effect models** have one error term and **random-effect models** have two error terms.\n\n## Fixed-Effect vs Random-Effects Model\n\nIn our fixed-effect model our observed effect size in all studies deviates from the **true effect size** due to sampling error.\n\n$\\hat{\\theta}_i= \\theta + {\\epsilon}_i$\n\nIn our random-effect model **our study's observed effect size** deviates from **our study's true effect size** due to sampling error.\n\n$\\hat{\\theta}_i= \\theta_i + {\\epsilon}_i$\n\nThere is however a second error, that of our study's true effect size deviating from the overarching distribution of effect sizes with a mean score of $\\mu$\n\n$\\theta_i= \\mu + {\\zeta}_i$\n\nTherefore, our study's effect size deviates from the overall $\\mu$ by two error terms.\n\n$\\hat{\\theta}_i= \\mu + {\\zeta}_i+ {\\epsilon}_i$\n\n## Exchangeability assumption of random-effects model\n\nThe exchangeability assumption in random-effects model dictates that our ${\\zeta}_i$ error from the superpopulation mean $\\mu$ should be independent of $i$. In other words, we cannot know in advance if ${\\zeta}_i$ in one study is going to be larger or smaller that the ${\\zeta}_j$ in another study. What does this mean for the **true effect sizes** for each of the study?\n\n## Thanks for the headache but which model should I use?\n\nWell as usual I have the perfect answer for you, can you guess it?\n\nYou can get more information on the debate on which method is more suitable for different cases here:\n\n<https://journals.sagepub.com/doi/10.1177/25152459221120427>\n\nOther authors also suggest that random-effects should be the go-to approach:\n\n<https://jamiefield.github.io/files/Field%20and%20Gillett%20(2010).%20How%20to%20do%20a%20meta-analysis..pdf>\n\n## Between-Study Heterogeneity\n\nAssuming we decided to go for the random-effects model approach it instantly becomes apparent that we are facing the challenge of accounting for the ${\\zeta}_i$ error.\n\nWe will address this by estimating the variance of the distribution of true effect sizes, also known as $\\tau^2$ (**tau-squared**). This also a measure of heterogeneity among the true effect sizes (between studies variance).\n\nThere are multiple estimators that you can use, **DerSimonian-Laird (`\"DL\"`)**, Restricted Maximum Likelihood (`\"REML\"`), Maximum Likelihood (`\"ML\"`), Empirical Bayes (`\"EB\"`), The **Sidik-Jonkman (`\"SJ\"`)**  are only some examples.\n\nFor more information you can read:\n\n<https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4950030/>\n\n## Which estimator should we use?\n\nAs a rule of thumb, if you have continuous outcomes you can use maximum likelihood.\n\nIf you are concerned that you have very high heterogeneity you can use Sidik-Jonkman\n\nIt has also been suggested that DerSimonian-Laird allows for more reproducible findings. This is because this estimator uses a formula contrary to ML that uses an iterative algorithm.\n\n## Which package should I use?\n\nWe will be mainly using **metafor** although you can also use **meta** if you want.\n\nWe will also use **metadat** package that has datasets for education purposes specifically for meta-analysis.\n\nThere are also other packages which I have not used so I will be quite limited in supporting you with other packages.\n\n## Lets work through an example\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(metafor)\nlibrary(metadat)\n\ndf <- get(data(dat.molloy2014))\n\nhead(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              authors year  ni    ri controls          design   a_measure\n1     Axelsson et al. 2009 109 0.187     none cross-sectional self-report\n2     Axelsson et al. 2011 749 0.162     none cross-sectional self-report\n3        Bruce et al. 2010  55 0.340     none     prospective       other\n4  Christensen et al. 1999 107 0.320     none cross-sectional self-report\n5 Christensen & Smith 1995  72 0.270     none     prospective       other\n6        Cohen et al. 2004  65 0.000     none     prospective       other\n  c_measure meanage quality\n1     other   22.00       1\n2       NEO   53.59       1\n3       NEO   43.36       2\n4     other   41.70       1\n5       NEO   46.39       2\n6       NEO   41.20       2\n```\n:::\n:::\n\n\n## Keep only the relevant variables and add id\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- df |> \n  select(1:6)\ndf <- df |> \n  mutate(id=row_number())\nhead(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              authors year  ni    ri controls          design id\n1     Axelsson et al. 2009 109 0.187     none cross-sectional  1\n2     Axelsson et al. 2011 749 0.162     none cross-sectional  2\n3        Bruce et al. 2010  55 0.340     none     prospective  3\n4  Christensen et al. 1999 107 0.320     none cross-sectional  4\n5 Christensen & Smith 1995  72 0.270     none     prospective  5\n6        Cohen et al. 2004  65 0.000     none     prospective  6\n```\n:::\n:::\n\n\n## Remember Pearson's correlation coefficient limitation?\n\nWe will transform to FIscher's $z$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- escalc(measure = \"ZCOR\", ri=ri, ni=ni, data=df, slab = paste(authors, year, sep=\",\"))\n\nhead(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n              authors year  ni    ri controls          design id     yi     vi \n1     Axelsson et al. 2009 109 0.187     none cross-sectional  1 0.1892 0.0094 \n2     Axelsson et al. 2011 749 0.162     none cross-sectional  2 0.1634 0.0013 \n3        Bruce et al. 2010  55 0.340     none     prospective  3 0.3541 0.0192 \n4  Christensen et al. 1999 107 0.320     none cross-sectional  4 0.3316 0.0096 \n5 Christensen & Smith 1995  72 0.270     none     prospective  5 0.2769 0.0145 \n6        Cohen et al. 2004  65 0.000     none     prospective  6 0.0000 0.0161 \n```\n:::\n:::\n\n\n## Random-Effects Size\n\n\n::: {.cell}\n\n```{.r .cell-code}\nma.res <- rma(yi, vi, data=df)\nma.res\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nRandom-Effects Model (k = 16; tau^2 estimator: REML)\n\ntau^2 (estimated amount of total heterogeneity): 0.0081 (SE = 0.0055)\ntau (square root of estimated tau^2 value):      0.0901\nI^2 (total heterogeneity / total variability):   61.73%\nH^2 (total variability / sampling variability):  2.61\n\nTest for Heterogeneity:\nQ(df = 15) = 38.1595, p-val = 0.0009\n\nModel Results:\n\nestimate      se    zval    pval   ci.lb   ci.ub      \n  0.1499  0.0316  4.7501  <.0001  0.0881  0.2118  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n## Heterogeneity Variance $\\tau^2$\n\nThis gives us an estimate of the variance of the true effect sizes. Similarly to the variance when we square root it we get the standard deviation. Therefore, $\\tau$ is the standard deviation of the true effect sizes.\n\nCan you think of an advantage of the $\\tau$ that $\\tau^2$ does not offer?\n\n## Q-test\n\n**Q** approximates the chi-squared distribution with **k-1** degrees of freedom. Significant **Q-test** denotes significant between-study heterogeneity.\n\nHowever, be very careful and do not rely exclusively on the **Q-test**. Its values increases as the number of studies increases as well as when the sample size of study increases.\n\nDo not decide between fixed and random effects model based on this result.\n\n## The $I^2$ Higgins and Thompson Statistic\n\n$I^2=\\frac{Q-(k-1)}{Q}$\n\nWe can use the following rule of thumb:\n\n1.  \\~25%, low heterogeneity\n2.  \\~50% moderate heterogeneity\n3.  \\~75% substantial heterogeneity\n4.  What about negative values?\n\n## The $H^2$ Higgins and Thompson Statistic\n\n$H^2=\\frac{Q}{K-1}$\n\nValues greater than 1 indicate the presence of between-study heterogeneity\n\n## So which measure is more important?\n\n**Q**'s significance depends on the size of your pool. $I^2$ does not suffer from this limitation but it is still affected by the precision of the included studies, larger sample sizes will result in sampling error that will be approaching zero. $H^2$ shares similarities with $I^2$. Finally, $\\tau^2$ does not have any of these limitations, but it is hard to interpret it as there are no rules of thumb. The best approach would be to report at least $\\tau^2$, $I^2$, and $Q$ and examine them in conjunction. You should also report confidence and prediction intervals.\n\n## Prediction and confidence intervals\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(ma.res)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n   pred     se  ci.lb  ci.ub   pi.lb  pi.ub \n 0.1499 0.0316 0.0881 0.2118 -0.0371 0.3370 \n```\n:::\n\n```{.r .cell-code}\nconfint(ma.res)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n       estimate   ci.lb   ci.ub \ntau^2    0.0081  0.0017  0.0378 \ntau      0.0901  0.0412  0.1944 \nI^2(%)  61.7324 25.2799 88.2545 \nH^2      2.6132  1.3383  8.5139 \n```\n:::\n:::\n\n\n## Exploring for influencial studies\n\n\n::: {.cell}\n\n```{.r .cell-code}\nma.inf <- influence(ma.res)\nprint(ma.inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n                         rstudent  dffits cook.d  cov.r tau2.del  QE.del    hat \nAxelsson et al.,2009       0.2918  0.0485 0.0025 1.1331   0.0091 37.7109 0.0568 \nAxelsson et al.,2011       0.1196 -0.0031 0.0000 1.2595   0.0100 36.7672 0.1054 \nBruce et al.,2010          1.2740  0.2595 0.0660 0.9942   0.0075 35.3930 0.0364 \nChristensen et al.,1999    1.4711  0.3946 0.1439 0.9544   0.0068 33.5886 0.0562 \nChristensen & Smith,1995   0.8622  0.1838 0.0339 1.0505   0.0082 36.5396 0.0441 \nCohen et al.,2004         -0.9795 -0.2121 0.0455 1.0639   0.0084 37.1703 0.0411 \nDobbels et al.,2005        0.2177  0.0296 0.0010 1.1740   0.0094 37.6797 0.0714 \nEdiger et al.,2007        -0.9774 -0.3120 0.1001 1.1215   0.0084 36.1484 0.0889 \nInsel et al.,2006          0.7264  0.1392 0.0195 1.0561   0.0083 37.0495 0.0379 \nJerant et al.,2011        -1.8667 -0.5861 0.2198 0.8502   0.0047 25.0661 0.1058 \nMoran et al.,1997         -1.4985 -0.2771 0.0756 1.0073   0.0077 35.6617 0.0369 \nO'Cleirigh et al.,2007     1.8776  0.4918 0.2148 0.8819   0.0059 31.9021 0.0511 \nPenedo et al.,2003        -1.1892 -0.2939 0.0859 1.0550   0.0080 36.3291 0.0587 \nQuine et al.,2012         -0.0020 -0.0423 0.0021 1.2524   0.0100 37.7339 0.0998 \nStilley et al.,2004        0.8066  0.2126 0.0459 1.0907   0.0083 35.8385 0.0684 \nWiebe & Christensen,1997  -0.7160 -0.1656 0.0280 1.0853   0.0087 37.7017 0.0411 \n                          weight    dfbs inf \nAxelsson et al.,2009      5.6776  0.0481     \nAxelsson et al.,2011     10.5396 -0.0032     \nBruce et al.,2010         3.6432  0.2623     \nChristensen et al.,1999   5.6195  0.3994     \nChristensen & Smith,1995  4.4069  0.1837     \nCohen et al.,2004         4.1094 -0.2112     \nDobbels et al.,2005       7.1362  0.0296     \nEdiger et al.,2007        8.8886 -0.3128     \nInsel et al.,2006         3.7886  0.1387     \nJerant et al.,2011       10.5826 -0.5430     \nMoran et al.,1997         3.6922 -0.2791     \nO'Cleirigh et al.,2007    5.1150  0.5059     \nPenedo et al.,2003        5.8732 -0.2941     \nQuine et al.,2012         9.9778 -0.0434     \nStilley et al.,2004       6.8403  0.2125     \nWiebe & Christensen,1997  4.1094 -0.1642     \n```\n:::\n:::\n\n\n## Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(ma.inf)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n## Forest Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforest(ma.res)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n## Funnel Plot\n\nPlease be aware that these are not publication bias tests!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfunnel(ma.res)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n## Small Study Bias\n\n\n::: {.cell}\n\n```{.r .cell-code}\nregtest(ma.res)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nRegression Test for Funnel Plot Asymmetry\n\nModel:     mixed-effects meta-regression model\nPredictor: standard error\n\nTest for Funnel Plot Asymmetry: z = 1.0216, p = 0.3070\nLimit Estimate (as sei -> 0):   b = 0.0790 (CI: -0.0686, 0.2266)\n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    // dispatch for htmlwidgets\r\n    function fireSlideEnter() {\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n    }\r\n\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n      fireSlideEnter();\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}