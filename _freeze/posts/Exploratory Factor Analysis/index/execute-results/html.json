{
  "hash": "cb1fed40312ee340f98b11f8fbc3fd77",
  "result": {
    "markdown": "---\ntitle: \"Exploratory Factor Analysis\"\nformat: \n  revealjs:\n    theme: league\n    transition: slide\n    background-transition: zoom\n    slide-number: c/t\n    show-slide-number: all\n    chalkboard: true\n    background-size: cover\n    smaller: true\n    echo: true\n    code-fold: true\n    code-summary: \"Show the code\"\nauthor: \"Dr Lazaros Gonidis\"\ndate: \"2025-02-11\"\nimage: \"image1.jpg\"\n---\n\n\n## Today's Aims\n\nToday we will define what **exploratory factor analysis (EFA)** is, we focus on the steps required to carry out **EFA** using **R**.\n\n## What is Factor Analysis\n\nFactor Analysis is the statistical technique that seeks to identify underlying relationships between observed variables.\n\nSpecifically, grouping these variables into groups where in-group variables **correlate highly**.\n\nIdeally, we want variables to correlate highly only with their in-group variables, and **correlate weakly or not at all with variables belonging to other groups**.\n\nWe will be referring to the term **group** as **factor**. These factors are **unobserved variables**...\n\n## What is Exploratory Factor Analysis (EFA)?\n\nIt is a statistical exploratory process that seeks to identify underlying relationships between observed variables.\n\nFurthermore, with **EFA** we aim to identify **latent variables** that might be responsible for the shared variances between the observed variables. As mentioned in our introduction lectures latent variables are variables that are not directly measured. Instead, they are **inferred by the existing relationships between our observed variables**.\n\n## Specification of models in EFA\n\nAccording to Kline (pages 190-191):\n\n1.  **EFA** does not require a priori specification, with number of possible factors varying from **one** up to **as many as the indicators**. (Highly not advised, but theoretically possible)\n2.  In **EFA** we have **unrestricted measurement models** where indicators are allowed to depend on all factors\n3.  Multiple factors models in **EFA** are not actually identified. Headache question: Why would that be the case?\n4.  In **EFA** we assume that specific variance of each indicator is not shared with that of any other indicator\n\n**Note:** Next week we will contrast all the above points with **Confirmatory Factor Analysis (CFA)**\n\n## Visualisation\n\n![](model.JPG)\n\n## Do we analyze correlated Factors?\n\nTypically this is not required. However, we can specify a **rotation** that will allow us to analyze correlated factors!\n\n**Rotation?**\n\nRotation allows us to simplify our model further, thus enhance its interpretation. This is an option that is **applied after our initial solution** and its aim is to achieve a solution where an indicator has high **loading** to one factor (or to as few as possible) and low loading on all other factors.\n\n## Rotations\n\n1.  **Orthogonal rotation**, usually the default setting for most **EFA** functions, treats all factors as non-correlated. The most commonly used is **Varimax**, however there are others. Be cautious when using **orthogonal rotations**, refer back to your theoretical background in order to make sure that your possible factors are indeed expected to be **uncorrelated**.\n2.  **Oblique rotation**, **allows** for correlated factors. The most commonly used is **Promax**, however there are others.\n\nSo which one should we use? Outside of the **correlated** or **uncorrelated allowance** it is difficult to decide. Many different rotation methods may give similarly valid results. I advise you to look into the **factor score indeterminacy** for more details.\n\n## Software Considerations\n\n**Can we use lavaan to carry our EFA?**\n\nYes, however ...... **psych** package might make your life easier.\n\n**EFAtools**, will definitely make your life easier.\n\nWe will use all three in combination in the coming examples and you can decide on your own.\n\n## Can we always carry out EFA?\n\nSome researchers argue that you could. You can definitely try, in terms of coding and running the relevant software.\n\nHowever, you shouldn't if you do not meet the following two criteria (at minimum).\n\n**KMO: Kaiser-Meyer-Olkin measure of sampling adequacy:** Evaluates whether our sample is suitable for factor analysis.It does so by evaluating the proportion of variance among variable that could be attributed to underlying factors. Ranges from **0 - 1**, and values closer to **1** indicate higher suitability.\n\n**Bartlett's test of Sphericity:** Assesses whether our variables/indicators have significant correlations. If our correlations are non-significant then we should not proceed. Here we are looking for evidence (***p \\< .05***) in order to reject the null hypothesis that our inter-variable correlations are zero.\n\n## Other Useful Terminology\n\n1.  **Communality:** The proportion of variance explained by the common factor. This will be used as a decision criterion to include or exclude indicators to a factor.\n2.  **Percentage of Variance:** The percentage of variance that is due to one factor in relation to the total variance in all factors.\n3.  **Eigenvalue:** The total variance explained by each factor, we are ideally looking for eigenvalues above **1.**\n\n## Let us work through an example together\n\nFirst, let's create the random data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1212)\n### normally distributed factors\n### these are just to help me set the indicators\n### the f1 and f2 will not be included in the data.frame\nf1 <- rnorm(250)\nf2 <- rnorm(250)\n\n### f1 indicators x1 to x3\nx1 <- f1 + rnorm(250, sd=0.15)\nx2 <- f1 + rnorm(250, sd=0.15)\nx3 <- f1 + rnorm(250, sd=0.15)\n\n### f2 indicators x4 to x6\nx4 <- f2 + rnorm(250, sd=0.15)\nx5 <- f2 + rnorm(250, sd=0.15)\nx6 <- f2 + rnorm(250, sd=0.15)\n\n### creating the dataframe\ndf <- data.frame(x1=x1, x2=x2, x3=x3, x4=x4, x5=x5, x6=x6)\n```\n:::\n\n\n## Assess KMO and Bartlett's test of Sphericity\n\n**EFAtools**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(psych)\nlibrary(EFAtools)\n\nKMO(df) ### THIS IS NOW MASKED BY EFAtools\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n── Kaiser-Meyer-Olkin criterion (KMO) ──────────────────────────────────────────\n\n✔ The overall KMO value for your data is middling.\n  These data are probably suitable for factor analysis.\n\n  Overall: 0.795\n\n  For each variable:\n   x1    x2    x3    x4    x5    x6 \n0.790 0.775 0.819 0.789 0.793 0.807 \n```\n:::\n\n```{.r .cell-code}\nBARTLETT(df) ### THIS IS NOW MASKED BY EFAtools\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n✔ The Bartlett's test of sphericity was significant at an alpha level of .05.\n  These data are probably suitable for factor analysis.\n\n  𝜒²(15) = 3272.23, p < .001\n```\n:::\n:::\n\n\n## Assess KMO and Bartlett's test of Sphericity\n\n**psych**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npsych::KMO(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKaiser-Meyer-Olkin factor adequacy\nCall: psych::KMO(r = df)\nOverall MSA =  0.8\nMSA for each item = \n  x1   x2   x3   x4   x5   x6 \n0.79 0.77 0.82 0.79 0.79 0.81 \n```\n:::\n\n```{.r .cell-code}\nr <- cor(df)\npsych::cortest.bartlett(r)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$chisq\n[1] 1278.319\n\n$p.value\n[1] 2.428264e-263\n\n$df\n[1] 15\n```\n:::\n:::\n\n\n## Determining number of factors\n\n**EFAtools**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nPARALLEL(df, eigen_type = \"PCA\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nParallel Analysis performed using 1000 simulated random data sets\nEigenvalues were found using PCA\n\nDecision rule used: means\n\n── Number of factors to retain according to ────────────────────────────────────\n\n◌ PCA-determined eigenvalues:  2\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n## Determining number of factors\n\n**psych**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfa.parallel(df, fa=\"pc\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nParallel analysis suggests that the number of factors =  NA  and the number of components =  2 \n```\n:::\n:::\n\n\n## In EFAtools you can also run multiple retention methods\n\n\n::: {.cell}\n\n```{.r .cell-code}\nN_FACTORS(df, criteria = c(\"PARALLEL\", \"EKC\", \"SMT\"),\n          eigen_type_other = c(\"SMC\", \"PCA\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n                                                                                                                                                                \n  🏃 ◯ ◯ Running EKC\n                                                                                                                                                                \n ◉ 🏃 ◯ Running PARALLEL\n                                                                                                                                                                \n ◉ ◉ 🏃  Running SMT\n                                                                                                                                                                \n ◉ ◉ ◉ Done!\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n── Tests for the suitability of the data for factor analysis ───────────────────\n\nBartlett's test of sphericity\n\n✔ The Bartlett's test of sphericity was significant at an alpha level of .05.\n  These data are probably suitable for factor analysis.\n\n  𝜒²(15) = 3272.23, p < .001\n\nKaiser-Meyer-Olkin criterion (KMO)\n\n✔ The overall KMO value for your data is middling with 0.795.\n  These data are probably suitable for factor analysis.\n\n── Number of factors suggested by the different factor retention criteria ──────\n\n◌ Empirical Kaiser criterion: 2\n◌ Parallel analysis with PCA: 2\n◌ Parallel analysis with SMC: 2\n◌ Sequential 𝜒² model tests: 2\n◌ Lower bound of RMSEA 90% confidence interval: 2\n◌ Akaike Information Criterion: 2\n```\n:::\n:::\n\n\n## Multiple Scree-plots\n\nTry the following code at home by removing the \\#\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# N_FACTORS(df, method = \"ULS\")\n```\n:::\n\n\n## Factor Extraction\n\n**EFAtools**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nEFA(df, n_factors = 2, method = \"ML\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nEFA performed with type = 'EFAtools', method = 'ML', and rotation = 'none'.\n\n── Unrotated Loadings ──────────────────────────────────────────────────────────\n\n  \t F1  \t F2  \nx1\t-.561\t .815\nx2\t-.562\t .816\nx3\t-.552\t .818\nx4\t .846\t .515\nx5\t .855\t .500\nx6\t .844\t .515\n\n── Variances Accounted for ─────────────────────────────────────────────────────\n\n                 \t F1  \t F2  \nSS loadings      \t 3.093\t 2.781\nProp Tot Var     \t 0.515\t 0.463\nCum Prop Tot Var \t 0.515\t 0.979\nProp Comm Var    \t 0.527\t 0.473\nCum Prop Comm Var\t 0.527\t 1.000\n\n── Model Fit ───────────────────────────────────────────────────────────────────\n\n𝜒²(4) =  1.47, p = .832\nCFI = 1.00\nRMSEA [90% CI] = .00 [.00; .06]\nAIC = -6.53\nBIC = -20.61\nCAF = .50\n```\n:::\n:::\n\n\n## Rotating solution\n\n**EFAtools**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nEFA(df, n_factors = 2, rotation = \"promax\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nEFA performed with type = 'EFAtools', method = 'PAF', and rotation = 'promax'.\n\n── Rotated Loadings ────────────────────────────────────────────────────────────\n\n  \t F1  \t F2  \nx1\t-.004\t .989\nx2\t-.003\t .991\nx3\t .006\t .987\nx4\t .991\t .005\nx5\t .989\t-.012\nx6\t .989\t .007\n\n── Factor Intercorrelations ────────────────────────────────────────────────────\n\n  \t F1  \t F2  \nF1\t 1.000\t-0.058\nF2\t-0.058\t 1.000\n\n── Variances Accounted for ─────────────────────────────────────────────────────\n\n                 \t F1  \t F2  \nSS loadings      \t 3.107\t 2.766\nProp Tot Var     \t 0.518\t 0.461\nCum Prop Tot Var \t 0.518\t 0.979\nProp Comm Var    \t 0.529\t 0.471\nCum Prop Comm Var\t 0.529\t 1.000\n\n── Model Fit ───────────────────────────────────────────────────────────────────\n\nCAF: .50\ndf:   4\n```\n:::\n:::\n\n\n## Rotating solution 2, oblimin\n\n**EFAtools**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nEFA(df, n_factors = 2, rotation = \"oblimin\", method = \"ULS\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nEFA performed with type = 'EFAtools', method = 'ULS', and rotation = 'oblimin'.\n\n── Rotated Loadings ────────────────────────────────────────────────────────────\n\n  \t F1  \t F2  \nx1\t-.004\t .989\nx2\t-.003\t .991\nx3\t .006\t .987\nx4\t .991\t .005\nx5\t .989\t-.012\nx6\t .989\t .007\n\n── Factor Intercorrelations ────────────────────────────────────────────────────\n\n  \t F1  \t F2  \nF1\t 1.000\t-0.058\nF2\t-0.058\t 1.000\n\n── Variances Accounted for ─────────────────────────────────────────────────────\n\n                 \t F1  \t F2  \nSS loadings      \t 3.107\t 2.766\nProp Tot Var     \t 0.518\t 0.461\nCum Prop Tot Var \t 0.518\t 0.979\nProp Comm Var    \t 0.529\t 0.471\nCum Prop Comm Var\t 0.529\t 1.000\n\n── Model Fit ───────────────────────────────────────────────────────────────────\n\n𝜒²(4) =  0.00, p =1.000\nCFI = 1.00\nRMSEA [90% CI] = .00 [.00; .00]\nAIC = -8.00\nBIC = -22.09\nCAF = .50\n```\n:::\n:::\n\n\n## Factor Extraction\n\n**psych**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nFA_df<- fa(df, nfactors=2, fm=\"ml\")\nsummary.psych(FA_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFactor analysis with Call: fa(r = df, nfactors = 2, fm = \"ml\")\n\nTest of the hypothesis that 2 factors are sufficient.\nThe degrees of freedom for the model is 4  and the objective function was  0.01 \nThe number of observations was  250  with Chi Square =  1.45  with prob <  0.84 \n\nThe root mean square of the residuals (RMSA) is  0 \nThe df corrected root mean square of the residuals is  0 \n\nTucker Lewis Index of factoring reliability =  1.003\nRMSEA index =  0  and the 10 % confidence intervals are  0 0.055\nBIC =  -20.64\n With factor correlations of \n      ML1   ML2\nML1  1.00 -0.06\nML2 -0.06  1.00\n```\n:::\n:::\n\n\n## Factor Extraction, psych continued\n\n**Examine the residuals**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresiduals.psych(FA_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   x1   x2   x3   x4   x5   x6  \nx1 0.02                         \nx2 0.00 0.02                    \nx3 0.00 0.00 0.03               \nx4 0.00 0.00 0.00 0.02          \nx5 0.00 0.00 0.00 0.00 0.02     \nx6 0.00 0.00 0.00 0.00 0.00 0.02\n```\n:::\n:::\n\n\n## Interpretation, psych\n\n\n::: {.cell}\n\n```{.r .cell-code}\nFA_df$loadings\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nLoadings:\n   ML1    ML2   \nx1         0.989\nx2         0.991\nx3         0.987\nx4  0.991       \nx5  0.989       \nx6  0.989       \n\n                 ML1   ML2\nSS loadings    2.939 2.935\nProportion Var 0.490 0.489\nCumulative Var 0.490 0.979\n```\n:::\n:::\n\n\n## Mental Break (down)\n\nProbably a lot to process in one go. We will take a mental break here and then we will work together through the next example.\n\n## But, but, what about lavaan???\n\nYes we have left lavaan out so far. Now is the time to have a look of how we could go through the above process using lavaan (partially).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lavaan)\n\nefa.model.fit <- lavaan::efa(data = df, nfactors = 2, rotation = \"promax\")\nsummary(efa.model.fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThis is lavaan 0.6.17 -- running exploratory factor analysis\n\n  Estimator                                         ML\n  Rotation method                       PROMAX OBLIQUE\n  Promax kappa                                       4\n  Rotation algorithm (rstarts)              PROMAX (0)\n  Standardized metric                             TRUE\n  Row weights                                   Kaiser\n\n  Number of observations                           250\n\nFit measures:\n                   aic      bic    sabic chisq df pvalue cfi rmsea\n  nfactors = 2 1025.02 1084.885 1030.994 1.478  4  0.831   1     0\n\nEigenvalues correlation matrix:\n\n     ev1      ev2      ev3      ev4      ev5      ev6 \n  3.1285   2.7871   0.0247   0.0214   0.0193   0.0189 \n\nStandardized loadings:\n\n       f1     f2      unique.var   communalities\nx1  0.989                  0.021           0.979\nx2  0.991                  0.018           0.982\nx3  0.987                  0.026           0.974\nx4         0.991           0.019           0.981\nx5         0.989           0.020           0.980\nx6         0.989           0.022           0.978\n\n                              f2    f1 total\nSum of sq (obliq) loadings 2.939 2.935 5.873\nProportion of total        0.500 0.500 1.000\nProportion var             0.490 0.489 0.979\nCumulative var             0.490 0.979 0.979\n\nFactor correlations:\n\n       f1     f2\nf1  1.000       \nf2 -0.058  1.000\n```\n:::\n:::\n\n\n## OK, time for our collaborative example\n\nWe will use a lavaan built-in dataset called **HolzingerSwineford1939**\n\nThe data consists of mental ability test scores of seventh- and eighth-grade children from two different schools (Pasteur and Grant-White). In our version of the dataset, only 9 out of the original 26 tests are included. A CFA model that is often proposed for these 9 variables consists of three latent variables (or factors), each with three indicators:\n\n-   a *visual* factor measured by 3 variables: `x1`, `x2` and `x3`\n\n-   a *textual* factor measured by 3 variables: `x4`, `x5` and `x6`\n\n-   a *speed* factor measured by 3 variables: `x7`, `x8` and `x9`\n\n## Visual Model\n\n![](model2.JPG){width=\"408\"}\n\n## Let us assign the data to a dataframe\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(HolzingerSwineford1939)\ndf2 <- HolzingerSwineford1939\nhead(df2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  id sex ageyr agemo  school grade       x1   x2    x3       x4   x5        x6\n1  1   1    13     1 Pasteur     7 3.333333 7.75 0.375 2.333333 5.75 1.2857143\n2  2   2    13     7 Pasteur     7 5.333333 5.25 2.125 1.666667 3.00 1.2857143\n3  3   2    13     1 Pasteur     7 4.500000 5.25 1.875 1.000000 1.75 0.4285714\n4  4   1    13     2 Pasteur     7 5.333333 7.75 3.000 2.666667 4.50 2.4285714\n5  5   2    12     2 Pasteur     7 4.833333 4.75 0.875 2.666667 4.00 2.5714286\n6  6   2    14     1 Pasteur     7 5.333333 5.00 2.250 1.000000 3.00 0.8571429\n        x7   x8       x9\n1 3.391304 5.75 6.361111\n2 3.782609 6.25 7.916667\n3 3.260870 3.90 4.416667\n4 3.000000 5.30 4.861111\n5 3.695652 6.30 5.916667\n6 4.347826 6.65 7.500000\n```\n:::\n:::\n\n\n## Keep only the indicators columns\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ndf2 <- df2 |> \n  dplyr::select(7:15)\n```\n:::\n\n\n## What should be our first step?\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}