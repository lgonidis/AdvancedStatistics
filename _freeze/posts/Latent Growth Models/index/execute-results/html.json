{
  "hash": "345f2d0af8f9a25a7738ec8d8881a84c",
  "result": {
    "markdown": "---\ntitle: \"Latent Growth Models\"\nformat: \n  revealjs:\n    theme: league\n    transition: slide\n    background-transition: zoom\n    slide-number: c/t\n    show-slide-number: all\n    chalkboard: true\n    background-size: cover\n    smaller: true\n    echo: true\n    code-fold: true\n    code-summary: \"Show the code\"\nauthor: \"Dr Lazaros Gonidis\"\ndate: \"2024-02-28\"\nimage: \"image1.jpg\"\n---\n\n\n## Today's Aims\n\nToday we will discuss Latent Growth Modeling (LGM) also known as Latent Growth Curve Analysis (LGCA), how it help us measure change over time (repeated measurements) and what advantages they have in comparison to other repeated measures analysis such as repeated measures ANOVA or hierarchical linear models.\n\nSpecifically we will cover:\n\n1.  Assumptions for LGM\n2.  Specifying LGM using lavaan\n3.  Interpreting output\n4.  Improving a model\n5.  Adding a covariate (predictor)\n\n## Notations in lavaan (refresher)\n\n-   **`~`** **predict**, used for regression of observed outcome to observed predictors\n\n-   **`=~`** **indicator**, used for latent variable to observed indicators\n\n-   **`~~`** **covariance**\n\n-   **`1*`** **fixes** **parameter** or **loading** to **1**\n\n-   **`NA*`** **frees** **parameter** or **loading**\n\n-   **`~1`** **intercept** or mean (e.g., **`x1 ~ 1`** estimates the mean of variable **`x1`**)\n\n-   **`a*`** **defines** the **parameter** ‘a’,\n\n## lavaan package\n\nToday we will also expand our lavaan usage beyond the analysis. We will also use the function **simulateData** to create random data for our **LGM**. If you like challenges try to randomly create data that will be suitable for **LGM**.\n\n## What is LGM?\n\nGenerally speaking, **LGM** is a special case of **CFA** where we incorporate a longitudinal element. This implies that we have a set of repeated measurements, at least **three**, and we want investigate change over these repeated time measurements.\n\n1.  LGM allows us to estimate means and covariances\n2.  LGM allows us to estimate observed and latent values\n3.  We will only focus on continuous measurements\n4.  All of our participants needs to be measured with the same time measurement information\n5.  We will also primarily focus on linear relationship (not exclusively though)\n\n## Let us describe a longitudinal/repeated measurements paradigm\n\nWe measure a psychological construct over **4** time points **T1, T2, T3, T4**. We are interested in investigating whether the measurements will **\"grow\"** (technically not grow) across these time points.\n\nWe will specify **two latent variables**, the **intercept** and the **slope**. Each latent variable will pass through each of the measurement points.\n\nWe will set the **intercept** factor loadings to 1 as we do not want to estimate them.\n\nWe will set the **slope** factor loadings to increasing **integer** values, usually starting from **0**.\n\n## Conceptual/Statistical Model\n\n![](model1.JPG)\n\n## Conceptual/Statistical Model continued\n\n![](model1matrix.JPG)\n\n## Let us generate some simulated data\n\nHere we just demonstrate the code, we will return to this code once we have explained our growth model in greater detail.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lavaan)\nlibrary(tidyverse)\nset.seed(25256)\n\n# Define the latent growth model\nlgm_model1 <- '\ni =~ 1*T0 + 1*T1 +1*T2 +1*T3\ns =~ 0*T0 + 1*T1 +2*T2 +3*T3\ngender|0*t1\n'\n# Generate random data\nsimulated_data1 <- simulateData(lgm_model1, model.type = \"growth\")\nsimulated_data1 <- simulated_data1 |> \n  mutate(id = row_number())\n# Check the generated data\nhead(simulated_data1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            T0          T1         T2         T3 gender id\n1  0.979298116  0.01583834 -0.8306769 -0.6729512      1  1\n2  1.567274129  0.34300527 -0.4162988  0.6335290      1  2\n3 -0.937327749 -0.83246257 -1.3628852 -0.2684692      2  3\n4  3.293394471  4.92009237  4.0263852  5.3132807      1  4\n5 -0.007578629  1.09421820  1.7625738  0.2041869      2  5\n6 -0.550506563  3.25869067  4.6268938  6.4532276      1  6\n```\n:::\n:::\n\n\n## Analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the latent growth model\nlgm_model1 <- '\ni =~ 1*T0 + 1*T1 +1*T2 +1*T3\ns =~ 0*T0 + 1*T1 +2*T2 +3*T3\n'\nfit1 <- growth(lgm_model1, data = simulated_data1)\nsummary(fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlavaan 0.6.17 ended normally after 26 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                                 2.409\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.790\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  i =~                                                \n    T0                1.000                           \n    T1                1.000                           \n    T2                1.000                           \n    T3                1.000                           \n  s =~                                                \n    T0                0.000                           \n    T1                1.000                           \n    T2                2.000                           \n    T3                3.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  i ~~                                                \n    s                -0.099    0.073   -1.362    0.173\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    i                -0.071    0.059   -1.195    0.232\n    s                 0.073    0.049    1.490    0.136\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .T0                0.696    0.109    6.385    0.000\n   .T1                1.014    0.081   12.469    0.000\n   .T2                1.138    0.115    9.907    0.000\n   .T3                1.092    0.215    5.083    0.000\n    i                 1.206    0.128    9.406    0.000\n    s                 1.025    0.079   13.010    0.000\n```\n:::\n:::\n\n\n## Reading through the output\n\n![](loadings.JPG)\n\n## Reading through the output\n\n![](intercepts.JPG)\n\n**i represents the mean of intercepts** for all our participants at **T0.**\n\n**s represents the mean of the slope,** as we move from each measurement time point to the next we should observe an increase of **0.051**\n\nFor example, we would expect the slope at the last time point to be:\n\n**s = -0.038 + 3x0.051**\n\n## Reading through the output\n\n![](variances.JPG)\n\nRemember dots denote residuals. **i** and **s** do not have a dot in from of them so these values denote **variances**.\n\n## Technically, this is the full model\n\n![](model1icpt.JPG)\n\n## Getting an even better look\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit1, standardized=TRUE, fit.measures=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlavaan 0.6.17 ended normally after 26 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                                 2.409\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.790\n\nModel Test Baseline Model:\n\n  Test statistic                              1115.899\n  Degrees of freedom                                 6\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.003\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3767.597\n  Loglikelihood unrestricted model (H1)      -3766.393\n                                                      \n  Akaike (AIC)                                7553.195\n  Bayesian (BIC)                              7591.126\n  Sample-size adjusted Bayesian (SABIC)       7562.560\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.040\n  P-value H_0: RMSEA <= 0.050                    0.977\n  P-value H_0: RMSEA >= 0.080                    0.001\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.008\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  i =~                                                                  \n    T0                1.000                               1.098    0.796\n    T1                1.000                               1.098    0.629\n    T2                1.000                               1.098    0.447\n    T3                1.000                               1.098    0.332\n  s =~                                                                  \n    T0                0.000                               0.000    0.000\n    T1                1.000                               1.012    0.580\n    T2                2.000                               2.025    0.823\n    T3                3.000                               3.037    0.919\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  i ~~                                                                  \n    s                -0.099    0.073   -1.362    0.173   -0.089   -0.089\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n    i                -0.071    0.059   -1.195    0.232   -0.064   -0.064\n    s                 0.073    0.049    1.490    0.136    0.072    0.072\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .T0                0.696    0.109    6.385    0.000    0.696    0.366\n   .T1                1.014    0.081   12.469    0.000    1.014    0.333\n   .T2                1.138    0.115    9.907    0.000    1.138    0.188\n   .T3                1.092    0.215    5.083    0.000    1.092    0.100\n    i                 1.206    0.128    9.406    0.000    1.000    1.000\n    s                 1.025    0.079   13.010    0.000    1.000    1.000\n```\n:::\n:::\n\n\n## Looking at residuals\n\nWe can look at residuals to acquire more information about specific predictors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresiduals(fit1, type=\"cor\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$type\n[1] \"cor.bollen\"\n\n$cov\n       T0     T1     T2     T3\nT0  0.000                     \nT1  0.005  0.000              \nT2 -0.005 -0.003  0.000       \nT3  0.004 -0.001  0.002  0.000\n\n$mean\n    T0     T1     T2     T3 \n 0.009 -0.001 -0.024  0.011 \n```\n:::\n:::\n\n\n## Looking at modification indices\n\nEven though you can acquire modification indices be very mindful on what these might mean about your model. In **LGM** our predictors are more than just items. Our main goal is to investigate how our measurements change across time points.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodificationindices(fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   lhs op rhs    mi    epc sepc.lv sepc.all sepc.nox\n1    i =~  T0 0.000  0.001   0.001    0.001    0.001\n2    i =~  T1 0.057  0.024   0.026    0.015    0.015\n3    i =~  T2 0.094 -0.019  -0.021   -0.009   -0.009\n4    i =~  T3 0.074  0.030   0.033    0.010    0.010\n5    s =~  T0 0.043  0.020   0.021    0.015    0.015\n6    s =~  T1 0.025 -0.009  -0.009   -0.005   -0.005\n7    s =~  T2 0.005 -0.005  -0.006   -0.002   -0.002\n8    s =~  T3 0.040  0.027   0.027    0.008    0.008\n16  T0 ~1     0.487  0.055   0.055    0.040    0.040\n17  T1 ~1     0.001 -0.002  -0.002   -0.001   -0.001\n18  T2 ~1     2.044 -0.081  -0.081   -0.033   -0.033\n19  T3 ~1     2.053  0.120   0.120    0.036    0.036\n22  T0 ~~  T1 0.196  0.070   0.070    0.083    0.083\n23  T0 ~~  T2 0.134 -0.026  -0.026   -0.029   -0.029\n24  T0 ~~  T3 0.155  0.052   0.052    0.060    0.060\n25  T1 ~~  T2 0.005 -0.006  -0.006   -0.006   -0.006\n26  T1 ~~  T3 0.001 -0.004  -0.004   -0.004   -0.004\n27  T2 ~~  T3 0.020  0.048   0.048    0.043    0.043\n```\n:::\n:::\n\n\n## Constraining our Time Point Measurements \n\nWe can leave our observed residual variances free to be estimated but constrain all of them to be the same.\n\nDoes this ring a bell in terms of **hierarchical linear models?**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlgm_model2 <- '\ni =~ 1*T0 + 1*T1 +1*T2 +1*T3\ns =~ 0*T0 + 1*T1 +2*T2 +3*T3\nT0 ~~ a*T0\nT1 ~~ a*T1\nT2 ~~ a*T2\nT3 ~~ a*T3\n'\nfit2 <- growth(lgm_model2, data = simulated_data1)\nsummary(fit2, standardized=TRUE, fit.measures=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlavaan 0.6.17 ended normally after 21 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n  Number of equality constraints                     3\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                                12.248\n  Degrees of freedom                                 8\n  P-value (Chi-square)                           0.140\n\nModel Test Baseline Model:\n\n  Test statistic                              1115.899\n  Degrees of freedom                                 6\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.996\n  Tucker-Lewis Index (TLI)                       0.997\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3772.517\n  Loglikelihood unrestricted model (H1)      -3766.393\n                                                      \n  Akaike (AIC)                                7557.034\n  Bayesian (BIC)                              7582.321\n  Sample-size adjusted Bayesian (SABIC)       7563.277\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.033\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.067\n  P-value H_0: RMSEA <= 0.050                    0.766\n  P-value H_0: RMSEA >= 0.080                    0.008\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.028\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  i =~                                                                  \n    T0                1.000                               1.024    0.712\n    T1                1.000                               1.024    0.590\n    T2                1.000                               1.024    0.420\n    T3                1.000                               1.024    0.310\n  s =~                                                                  \n    T0                0.000                               0.000    0.000\n    T1                1.000                               1.002    0.578\n    T2                2.000                               2.004    0.822\n    T3                3.000                               3.006    0.910\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  i ~~                                                                  \n    s                -0.034    0.068   -0.499    0.618   -0.033   -0.033\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n    i                -0.076    0.059   -1.275    0.202   -0.074   -0.074\n    s                 0.075    0.049    1.525    0.127    0.075    0.075\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .T0         (a)    1.022    0.046   22.361    0.000    1.022    0.494\n   .T1         (a)    1.022    0.046   22.361    0.000    1.022    0.340\n   .T2         (a)    1.022    0.046   22.361    0.000    1.022    0.172\n   .T3         (a)    1.022    0.046   22.361    0.000    1.022    0.094\n    i                 1.048    0.116    9.033    0.000    1.000    1.000\n    s                 1.004    0.077   13.045    0.000    1.000    1.000\n```\n:::\n:::\n\n\n## Is this a better model? Or the same?\n\nBased on the output above is **model2** better than **model1**?\n\n## Visualization\n\nTypically you could acquire this at the start so you can get a feel of your data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lcsm)\n\nplot_trajectories(data = simulated_data1,\n                  id_var = \"id\", \n                  var_list = c(\"T0\", \"T1\", \"T2\", \"T3\"),\n                  xlab = \"Time points T0-T3\",\n                  ylab = \"Measurement\",\n                  line_colour = \"black\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n## Can we also add a predictor to your model?\n\nLet us consider the research question that **gender** can predict differences in the **slope** and **intercept** latent variables. In other words, I want to explore whether there are **gender** differences in our longitudinal measurement.\n\nBefore we move on I want you to think now in terms of **endogenous** and **exogenous** variables.\n\n![](predictor.JPG){width=\"340\"}\n\n## Coding a predictor in our model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlgm_model3 <- '\ni =~ 1*T0 + 1*T1 +1*T2 +1*T3\ns =~ 0*T0 + 1*T1 +2*T2 +3*T3\ni ~ gender\ns ~ gender\nT0 ~~ a*T0\nT1 ~~ a*T1\nT2 ~~ a*T2\nT3 ~~ a*T3\n'\nfit3 <- growth(lgm_model3, data = simulated_data1)\nsummary(fit3, standardized=TRUE, fit.measures=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlavaan 0.6.17 ended normally after 33 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        11\n  Number of equality constraints                     3\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                                14.742\n  Degrees of freedom                                10\n  P-value (Chi-square)                           0.142\n\nModel Test Baseline Model:\n\n  Test statistic                              1122.538\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.996\n  Tucker-Lewis Index (TLI)                       0.996\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3770.444\n  Loglikelihood unrestricted model (H1)      -3763.073\n                                                      \n  Akaike (AIC)                                7556.888\n  Bayesian (BIC)                              7590.605\n  Sample-size adjusted Bayesian (SABIC)       7565.213\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.031\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.062\n  P-value H_0: RMSEA <= 0.050                    0.823\n  P-value H_0: RMSEA >= 0.080                    0.003\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.024\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  i =~                                                                  \n    T0                1.000                               1.024    0.712\n    T1                1.000                               1.024    0.590\n    T2                1.000                               1.024    0.420\n    T3                1.000                               1.024    0.310\n  s =~                                                                  \n    T0                0.000                               0.000    0.000\n    T1                1.000                               1.002    0.578\n    T2                2.000                               2.004    0.822\n    T3                3.000                               3.006    0.910\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  i ~                                                                   \n    gender           -0.061    0.119   -0.512    0.609   -0.059   -0.030\n  s ~                                                                   \n    gender           -0.176    0.098   -1.798    0.072   -0.176   -0.088\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n .i ~~                                                                  \n   .s                -0.037    0.068   -0.540    0.589   -0.036   -0.036\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .i                 0.017    0.191    0.090    0.928    0.017    0.017\n   .s                 0.344    0.158    2.185    0.029    0.344    0.344\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .T0         (a)    1.022    0.046   22.361    0.000    1.022    0.494\n   .T1         (a)    1.022    0.046   22.361    0.000    1.022    0.340\n   .T2         (a)    1.022    0.046   22.361    0.000    1.022    0.172\n   .T3         (a)    1.022    0.046   22.361    0.000    1.022    0.094\n   .i                 1.047    0.116    9.029    0.000    0.999    0.999\n   .s                 0.996    0.076   13.026    0.000    0.992    0.992\n```\n:::\n:::\n\n\n## This week's exercises/challenges\n\n**Exercise 1:**\n\nI want you to specify the mode3 in lavaan but write all the necessary components in order to run it using the **cfa()** instead of **growth()**.\n\nIn other words think in terms of specifying, fixing, and freeing parameters as we would have done in a **CFAt** in order to acquire the exact same values in your output. You can find the relevant project in our Posit Cloud under the name **Week 6 exercise**.\n\n**Exercise 2:**\n\nI want you to modify **model1** in order to test for **a quadratic projection instead of a linear one**. Think in terms of how a quadratic relation works and amend the slope accordingly.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    // dispatch for htmlwidgets\r\n    function fireSlideEnter() {\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n    }\r\n\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n      fireSlideEnter();\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}